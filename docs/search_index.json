[["index.html", "User’s guide for LTabundR Overview", " User’s guide for LTabundR Overview The R package LTabundR offers tools that facilitate and standardize design-based line-transect abundance estimation of cetaceans, based on typical workflow used following NOAA Fisheries ship surveys in the central and eastern Pacific (e.g., Barlow 2006, Barlow and Forney 2007, Bradford et al. 2017, Bradford et al. 2021). That workflow typically involves four stages: (1) Data processing This step involves reading in &amp; processing raw DAS files (the files produced by the software Wincruz commonly used during NOAA Fisheries line-transect surveys in the Pacific), breaking effort into discrete segments for variance estimation, correcting group size estimates according to calibration models, and then averaging together group size estimates for each sighting. Most importantly, this step standardizes the data structure in a way that all downstream analyses depend upon. The name we will use for this standardized data object is a cruz object. The key LTabundR functions you will use in this stage are load_settings() and process_surveys(). (2) Data exploration This step involves summarizing effort and sightings totals, determining the appropriate truncation distances for each species – or pool of species – based on sample sizes, and producing maps. The key LTabundR functions in this stage are cruz_explorer() (a Shiny dashboard for data exploration) and the summarize... functions, such as summarize_species() and summarize_effort(). (3) Data analysis This step involves estimating Beaufort-specific “relative” trackline detection probabilities – i.e., g(0) estimates; estimating density/abundance with detection functions and determining the CV of that estimate; handling stratified analyses; and evaluating if random variation in the encounter rate of a species may be driving differences in abundance estimates over time. The key LTabundR functions in this stage are g0_table() and lta(). Most analyses are group-based analyses, but false killer whales (Pseudorca crassidens) are analyzed differently using a subgroup-based approach. For this exception, the function lta_subgroup() will be used. (4) Reports &amp; plots This step produces summary tables of the processed data and line-transect estimates; plots the best-fitting detection function model(s); and plots species-specific abundance estimates (and their CV). They key LTabundR functions in this stage are lta_report(), df_plot(), and lta_plot(). This user’s guide is structured around these four workflow stages. Those pages are followed by a case study with full-fledged example code. The guide concludes with appendices that offer further details and minutiae on certain aspects of the package. Throughout this user’s guide, we will primarily be using example data from the winter Hawaiian Islands Cetacean Ecosystem and Assessment Survey (WHICEAS) of 2020, along with the summer-fall HICEAS 2017 data collected within the WHICEAS study area. Installation Note: BETA testing only. This package is currently in beta testing and is not yet ready for widespread use. The LTabundR package is available on GitHub here. To install directly within R, use the following code: # Install support packages, if needed if (!require(&#39;devtools&#39;)) install.packages(&#39;devtools&#39;) # Increase timeout for download, since there are datasets options(timeout=9999999) # Install LTabundR remotely from GitHub devtools::install_github(&#39;PIFSC-Protected-Species-Division/LTabundR&#39;) You may need to download “Rtools” if it is not already installed on your computer. You can do so here. # Load into session library(LTabundR) # Check package version utils::packageVersion(&#39;LTabundR&#39;) [1] &#39;0.7&#39; Note that this package contains large built-in datasets and may take several minutes to install. Credits This R package is a consolidation of code developed over many years by many NOAA Fisheries scientists, primarily Jay Barlow, Tim Gerrodette, Jeff Laake, Karin Forney, Amanda Bradford, and Jeff Moore. This package was developed by Eric Keen and Amanda Bradford with support from the NOAA Fisheries National Protected Species Toolbox Initiative. "],["settings.html", "1 Settings Survey strata Survey-wide settings Cohort-specific settings Example code", " 1 Settings To customize the way data are processed and included in your analysis, use the load_settings() function. This function emulates and expands upon the settings file, ABUND.INP, that was used to run ABUND7/9 in FORTRAN. ABUND7/9 was a program written by Jay Barlow (NOAA Fisheries) to process DAS files. This function allows you to use ‘factory defaults’ if you don’t wish to specify anything special such as strata or study area polygons: settings &lt;- load_settings() If you do not want to use all the defaults, you can provide load_settings() with custom inputs. The function accepts three arguments: strata: dataframe(s) of coordinates survey: settings that will apply universally to the analysis cohorts: settings that are specific to groups of species. By providing cohort-specific settings, the code for a single analysis becomes simpler and more easily reproduced, since the code only needs to be run once without modification. The output of load_settings() is a named list with a slot for each of these arguments: settings %&gt;% names [1] &quot;strata&quot; &quot;survey&quot; &quot;cohorts&quot; Survey strata Stratum polygons can be provided as a named list of data.frame objects. Each data.frame must have Lon and Lat as the first two columns, providing coordinates in decimal degrees in which West and South coordinates are negative. Other columns are allowed, but the first two need to be Lon and Lat. The name of the slot holding the data.frame will be used as a reference name for the stratum. It is acceptable if vertices in the eastern hemisphere are described using negative longitudes below -180, e.g., -185. (LTabundR will correct these to proper decimal degrees, e.g., -185 will become 175.) If strata is NULL, abundance will not be estimated; only density within the searched area (i.e., the total segment length x effective strip width). While users are welcome to upload polygons of their own, the package comes with built-in polygons for strata that are commonly used in three NOAA Fisheries study regions in the central and eastern Pacific: the Central North Pacific (CNP, including Hawaii) … data(strata_cnp) names(strata_cnp) [1] &quot;HI_EEZ&quot; &quot;OtherCNP&quot; &quot;MHI&quot; &quot;WHICEAS&quot; [5] &quot;Spotted_OU&quot; &quot;Spotted_FI&quot; &quot;Spotted_BI&quot; &quot;Bottlenose_KaNi&quot; [9] &quot;Bottlenose_OUFI&quot; &quot;Bottlenose_BI&quot; &quot;nwhi_fkw&quot; &quot;mhi_fkw&quot; # HI-EEZ: U.S. Exclusive Economic Zone around the Hawaiian Islands. # OtherCNP: Extended Central North Pacific study area. # MHI: Main Hawaiian Islands survey stratum during HICEAS 2002. # WHICEAS: Study area for winter HICEAS of 2020. # Spotted_OU: Stock boundary for the Oahu population of spotted dolphins. # Spotted_FI: Stock boundary for the 4-Islands population of spotted dolphins. # Spotted_BI: Stock boundary for the Hawaii Island population of spotted dolphins. # Bottlenose_KaNi: Stock boundary for the Kauai/Niihau population of bottlenose dolphins. # Bottlenose_OUFI: Collective stock boundaries for the adjacent Oahu and 4-Islands populations of bottlenose dolphins. # Bottlenose_BI: Stock boundary for the Hawaii Island population of bottlenose dolphins. # NWHI: Stock boundary for the Northwestern Hawaiian Islands population of false killer whales. …the California Current System (CCS) … data(strata_ccs) names(strata_ccs) [1] &quot;CCS&quot; &quot;Southern_CA&quot; &quot;Central_CA&quot; &quot;Nothern_CA&quot; &quot;OR_WA&quot; #For more information on these strata, see Barlow (2010). … and the Eastern Tropical Pacific (ETP): data(strata_etp) names(strata_etp) [1] &quot;MOPS_AreaCoreM&quot; &quot;MOPS_AreaIn&quot; &quot;MOPS_AreaIn1&quot; [4] &quot;MOPS_AreaIn2&quot; &quot;MOPS_AREAINS&quot; &quot;MOPS_AREAMID&quot; [7] &quot;MOPS_AreaMid1&quot; &quot;MOPS_AreaMid2&quot; &quot;MOPS_AreaMOPS&quot; [10] &quot;MOPS_AREANORS&quot; &quot;MOPS_AreaOuterM&quot; &quot;MOPS_AreaSou&quot; [13] &quot;MOPS_AREASOUS&quot; &quot;MOPS_AreaSpin&quot; &quot;MOPS_AreaSpinS&quot; [16] &quot;MOPS_AREAWES&quot; &quot;PODS_93STRAT1&quot; &quot;PODS_93STRAT2&quot; [19] &quot;PODS_Area92&quot; &quot;PODS_AREA92RS&quot; &quot;PODS_Area92s&quot; [22] &quot;PODS_AREA93&quot; &quot;PODS_AREA93A&quot; &quot;PODS_AREA93AR&quot; [25] &quot;PODS_AREA93AS&quot; &quot;PODS_AREA93BR&quot; &quot;PODS_AREA93M&quot; [28] &quot;PODS_AREA93MS&quot; &quot;PODS_AREA93R&quot; &quot;PODS_AREA93R1&quot; [31] &quot;PODS_AREA93R2&quot; &quot;PODS_AREA93RS&quot; &quot;PODS_AREA93S&quot; [34] &quot;PODS_AREANCOR&quot; &quot;PODS_GOCpoly&quot; &quot;Pre1986_Area79ES1&quot; [37] &quot;Pre1986_Area79ES1s&quot; &quot;Pre1986_Area79ES2&quot; &quot;Pre1986_Area79ES2s&quot; [40] &quot;Pre1986_Area79NE1&quot; &quot;Pre1986_Area79NE1s&quot; &quot;Pre1986_Area79NE2&quot; [43] &quot;Pre1986_Area79NE2s&quot; &quot;Pre1986_Area79NE3&quot; &quot;Pre1986_Area79NE3s&quot; [46] &quot;Pre1986_AreaCal&quot; &quot;Pre1986_AreaCals&quot; &quot;Pre1986_AreaMid&quot; [49] &quot;Pre1986_AreaMidS&quot; &quot;Pre1986_AreaNorth&quot; &quot;Pre1986_AreaNorthS&quot; [52] &quot;Pre1986_AreaSouth&quot; &quot;Pre1986_AreaSouthS&quot; &quot;STAR_Area98a&quot; [55] &quot;STAR_Area98b&quot; &quot;STAR_AreaCore&quot; &quot;STAR_AreaCore2&quot; [58] &quot;STAR_AreaCoreS&quot; &quot;STAR_AreaNCoast&quot; &quot;STAR_AreaNCstS&quot; [61] &quot;STAR_AreaOuter&quot; &quot;STAR_AreaOuter00&quot; &quot;STAR_AreaSCoast&quot; [64] &quot;STAR_AreaSCstS&quot; &quot;STAR_AreaSPn&quot; &quot;STAR_AreaSPs&quot; [67] &quot;STAR_AreaSTAR&quot; &quot;STAR_AreaSTAR2&quot; &quot;STAR_AreaSTARlite&quot; [70] &quot;STAR_Dcaparea&quot; #For more information on these strata, contact &lt;swfsc.info@noaa.gov&gt; The package includes functions for visualizing and selecting from these strata. See the Strata Gallery appendix. To create your own geostratum, you would use code like this: # Create a dataframe of coordinates # (this example is a closed rectangle) mine1 &lt;- data.frame(Lon = c(-120, -125, -125, -120, 120), Lat = c(20, 20, 40, 40, 20)) # Make into a list my_strata &lt;- list(mine1 = mine1) # Check it out my_strata $mine1 Lon Lat 1 -120 20 2 -125 20 3 -125 40 4 -120 40 5 120 20 # Supply to load_settings() my_settings &lt;- load_settings(strata = my_strata) To keep some of the strata from strata_cnp then add one of your own design: # Subset `strata_cnp`: my_cnp &lt;- strata_cnp[c(2,3)] # Check it out my_cnp %&gt;% names [1] &quot;OtherCNP&quot; &quot;MHI&quot; # Create your new geostratum mine1 &lt;- data.frame(Lon = c(-120, -125, -125, -120, 120), Lat = c(20, 20, 40, 40, 20)) # Assemble your list: my_strata &lt;- c(my_cnp, list(mine1 = mine1)) # Check it out my_strata $OtherCNP Lon Lat 1 -131 40.00 2 -126 32.05 3 -120 25.00 4 -120 -5.00 5 -185 -5.00 6 -185 40.00 7 -131 40.00 $MHI Lon Lat 1 -156.00 22.00 2 -154.40 20.60 3 -153.50 19.20 4 -154.35 18.55 5 -155.20 17.75 6 -157.00 18.25 7 -157.50 19.20 8 -161.00 21.84 9 -160.00 23.00 10 -157.00 22.50 11 -156.00 22.00 $mine1 Lon Lat 1 -120 20 2 -125 20 3 -125 40 4 -120 40 5 120 20 # Supply to load_settings() my_settings &lt;- load_settings(strata = my_strata) Survey-wide settings Survey-wide settings apply universally to all species in the analysis. Defaults settings$survey $out_handling [1] &quot;remove&quot; $interpolate NULL $min_row_interval [1] 2 $max_row_interval [1] 3600 $max_row_km [1] 100 $km_filler [1] 1 $speed_filler [1] 18.52 $segment_method [1] &quot;day&quot; $segment_target_km [1] 150 $segment_max_interval [1] 48 $segment_remainder_handling [1] &quot;segment&quot; $ship_list NULL $species_codes NULL $group_size_coefficients NULL $smear_angles [1] FALSE Defaults for the survey argument list are built up efficiently using the function load_survey_settings() (see example code at bottom). Details The survey_settings input accepts a list with any of the following named slots: out_handling: the first slot allows you to specify how data occurring outside of geo-strata should be handled. If this is set to \"remove\", those rows will be filtered out of the data early in the process. This reduces memory usage, speeds up processing, and gives you geographic control of how effort and sightings will be summarize. If this is set to \"stratum\", those data will be assigned to a fake geo-stratum, named \"out\". Effort in the \"out\" stratum will not be segmentized, but \"out\" sightings will be processed and retained in the final datasets. This setting might be useful if you want to use \"out\" data for survey summaries and/or detection function estimation. The default is \"remove\", since that saves the most time and memory. interpolate: This argument allows you to interpolate the DAS data at the onset of processing if your position updates are separated by large time intervals, which would make spatial effort and stratum assignments less exact. If this argument is NULL, then no interpolation will occur. If it is a number, e.g., 30, LTabundR will interpolate the data using simple-linear methods (i.e., no great-sphere calculations), such that position updates occur every 30 seconds or less. If adjacent DAS rows are from different dates or cruises, the interpolation routine will skip to the next pair of related rows. Interpolation will only occur for On-Effort rows (i.e., column OnEffort is TRUE). min_row_interval: The minimum time interval, in seconds, between rows in order for the Great Circle distance between rows to be calculated. Intervals less than this number will be assigned a distance of 0 km. max_row_interval: The maximum alloweable time interval, in seconds, between rows before LTabundR assumes that there has been a break in survey data logging. max_row_km: The maximum alloweable distance interval, in km, between rows before the function assumes that there has been a break in survey data logging. km_filler: When valid speed and position information is not available (e.g., the given distance exceeds max_km_gap), this value (in km) will be used as an estimate of the distance in between consecutive rows of data. speed_filler: When speed is not available in the data, this value (in kph) will be used as a filler in order to estimate the distance between consecutive rows of data based on timestamp differences (when lat/long coordinates are not available). segment_method: This and the next few slots are devoted to controlling how effort will be “segmentized”, or chopped into discrete sections for the purposes of estimating the variance of the density/abundance estimates. The two method options are \"day\" – all effort within the same Cruise-StudyArea-Stratum-Year-Effort scenario will be binned into segments by calendar date – and \"equallength\" – effort within each unique effort scenario (Cruise-StudyArea-etc.) will be divided into segments of approximately equal length. See the Appendix on segmentizing for details. segment_target_km: if segmentizing by \"equallength\", this field allows you to specify what that target length is, in km. The default is 150 km, the distance generally surveyed in one day on NOAA Fisheries surveys. segment_max_interval: if segmentizing by \"equallength\", this setting allows you to specify the time gaps in effort that are allowed to be contained within a single segment. For example, if your goal is a few large segments of equal length (e.g., 150-km segments, for bootstrap estimation of density variance), you are probably willing for discrete periods of effort to be concatenated into a single segment, even if the gaps between effort are as large as 1 or 2 days, in which case you would set segment_max_interval to 24 or 48 (hours), respectively. However, if your goal is many smaller segments (e.g., 5-km segments, for habitat modeling), you want to ensure that effort is contiguous so that segment locations can be accurately related to environmental variables, in which case you would set segment_max_interval to be very small (e.g., .2 hours, or 12 minutes). Setting this interval to a small number, such as 0.2, also allows the segmentizing function to overlook momentary breaks in effort. segment_remainder_handling: if segmentizing by \"equallength\", periods of effectively-contiguous effort (as specified by segment_max_interval) are unlikely to be perfectly divisible by your segment_target_km; there is going to be a remainder. You can handle this remainder in three ways: (1) \"disperse\" allows the function to adjust segment_target_km so that there is in fact no remainder, effectively dispersing the remainder evenly across all segments within that period of contiguous effort; (2) \"append\" asks the function to append the remainder to a randomly selected segment, such that most segments are the target length with the exception of one longer one; or (3) \"segment\" asks the function to simply place the remainder in its own segment, placed randomly within the period of contiguous effort. This setting also has a second layer of versatility because it can accept a one- or two-element character vector. If a two-element vector is provided (e.g., c(\"append\",\"segment\")), the first element will be used in the event that the remainder is less than or equal to half your segment_target_km; if the remainder is more than half that target length, the second element will be used. This feature allows for replication of the segmentizing methods in Becker et al. (2010). The remaining slots in survey_settings pertain to various datasets and settings used in data processing: ship_list: A data.frame containing a list of survey numbers and ship names. If not provided the default version, which was current as of the release of ABUND9 in 2020, will be used (data(ships)), although note that surveys not included there will not be associated with a ship. Supplied data.frames must match the column naming structure of data(ships). species_codes: A data.frame containing species codes. This is an optional input, chiefly used to format species names in the reporting stage of the workflow (lta_report() especially). If missing, neither data processing nor line transect analysis will be obstructed. If the user supplies a data.frame, it must match the column naming structure of data(species_codes). group_size_coefficients: A data.frame of calibration factors. Find details in the subsection on processing sightings and estimating group size. smear_angles: If TRUE (the default is FALSE), bearing angles to a group of animals will be “smeared” by adding a uniformly distributed random number between -5 and +5 degrees. This has not been used in any recent analyses because observers have not been rounding angles as much as they used to. It was suggested by Buckland et al. (2001) as a method for dealing with rounding which is especially influential when rounding to zero places many sightings at zero perpendicular distance. Cohort-specific settings Cohort-specific settings apply only to a group of species. Since you can add as many cohorts to a settings object as you need, this allows you to stage your entire analysis and run your code once without modifying code or creating multiple versions of your code for each analysis of each cohort. Defaults The default is to use a single cohort for all species: settings$cohorts %&gt;% names [1] &quot;default&quot; Default values for the default cohort: settings$cohorts$default $id [1] &quot;default&quot; $species NULL $strata NULL $probable_species [1] FALSE $sighting_method [1] 0 $cue_range [1] 0 1 2 3 4 5 6 7 $school_size_range [1] 0 10000 $school_size_calibrate [1] TRUE $calibration_floor [1] 0 $use_low_if_na [1] FALSE $io_sightings [1] 0 $geometric_mean_group [1] TRUE $truncation_km [1] 5.5 $beaufort_range [1] 0 1 2 3 4 5 6 $abeam_sightings [1] FALSE $strata_overlap_handling [1] &quot;smallest&quot; $distance_types [1] &quot;S&quot; &quot;F&quot; &quot;N&quot; $distance_modes [1] &quot;P&quot; &quot;C&quot; $distance_on_off [1] TRUE Defaults for the cohorts argument list are built up efficiently using the function load_cohort_settings() (see example code at bottom). Details The cohort_settings input accepts a list of any length. Each slot in that list can contain settings for a different cohort. Each cohort list can have any of the following named slots: id: An informal identifier for this cohort, to help you keep track of which cohort is which. For example, settings for a cohort of large whales species could be named \"big whales\"; settings for small delphinids and phocoenids could be named \"small_odontocetes\"; settings for beaked whales could be named \"beakers\". species: A character vector of species codes to include in this cohort. If NULL (the default), all species within the survey data will be included. Note that if you specify a vector, all species to be used in modeling a detection function for this cohort must be included here. For example, in Hawaii the bottlenose dolphin is analyzed as part of a multi-species pool along with the rough-toothed dolphin, Risso’s dolphin, and pygmy killer whale. However, the bottlenose dolphin has insular populations that need to be differentiated from their pelagic counterpart, which requires some special geostratum handling that behooves the preparation of a dedicated cohort for bottlenose dolphin. Even so, in the cohort_settings object for the bottlenose dolphin cohort, the species codes for the rough-toothed, Risso’s, and pygmy-killer-whale dolphins need to be provided in this species argument. Conversely, in the cohort_settings object that holds most other species, including the rough-toothed, Risso’s, and pygmy-killer-whale dolphins, the bottlenose dolphin’s code still needs to be included in this species argument. strata: A character vector of geostratum names. These must match the names listed in the strata slot of your survey settings (see documentation for load_survey_settings()). If NULL (the default), all geostrata in your survey settings will be used. This argument is an opportunity to subset the geostrata used for a cohort. For example, as discussed above, certain dolphin species in Hawaiian waters have unique geostrata that apply only to their insular/pelagic populations, and should only have a role in breaking effort segments in the bootstrap variance analysis for these specific species. Those dolphins should be given their own cohort, and those insular/pelagic geostrata should be included in this strata argument. Conversely, all other species should be placed in a separate cohort and only the generic geostrata should be included in this strata argument. See the WHICEAS example below for a demonstration. probable_species: If TRUE (default is FALSE), the “probable” species identifications will be used in place of the “unidentified” categories. sighting_method: A coded integer which determines which sightings will be included based on how they were first seen. Allowable codes are 0=any method, 1=with 25X only, 2=neither with 25x binoculars nor from the helicopter (i.e., naked eyes and 7x binoculars only). These codes match those used in ABUND7/9. cue_range: Numeric vector of acceptable “observation cues” for sightings used in estimates of abundance. (0=this detail is missing in the data, 1=associated birds, 2=splashes, 3=body of the marine mammal, 4=associated vessel, 5=?, 6=blow / spout, 7=associated helicopter). These codes match those used in ABUND7/9. school_size_range: Minimum and maximum group sizes to be included in estimates of abundance. This is the overall group size, not the number of the given species that are present in a mixed-species group. school_size_calibrate: A logical (TRUE or FALSE) specifying whether or not to carry out group size adjustments according to the calibration table provided in survey$group_size_coefficients (if that table is provided). This setting allows you to toggle the survey-wide setting for certain cohorts. For example, perhaps you want to carry out calibration for a cohort of dolphin species, but not for a cohort of large whales whose group sizes tend to be smaller and easier to estimate accurately. calibration_floor: A numeric indicating the minimum school size estimate for which group size calibration will be attempted. This pertains only to observers who do no have an entry in the group_size_coefficients table provided in load_survey_settings() (that table has a calibration floor for each observer). The default is 0, meaning that calibration will be attempted for all group size estimates, regardless of the raw estimate. use_low_if_na: If this setting is TRUE, and an observer(s) does not make a best estimate of group size, mean group size will be calculated from “low” estimates. This will be done only if no observer has a “best” estimate. io_sightings: A coded integer which specifies how sightings by the independent observer will be handled. Allowable codes, which are inherited from those used in ABUND7/9, are \"_1\"=include independent observer sightings wih all other sightings, \"0\"=ignore sightings by independent observer, \"1\"=use only sightings made by regular observer team WHEN an independent observer was present, \"2\"=include only sightings made by the independent observer. IO sightings are typically used only for making g(0) estimates, otherwise IO sightings are usually ignored (code = \"0\"). geometric_mean_group: This logical variable specifies whether to use a weighted geometric mean when calculating mean group size. Barlow et al. (1998) found that this gave slightly better performance than an arithmetic mean group size for calibrated estimates. Default is TRUE, but a geometric mean will only be calculated if group_size_coefficients is not NULL. If group_size_coefficients is NULL, then an arithmetic mean will be calculated. (This setting does not apply to subgroup analyses.) truncation_km: Specifies the maximum perpendicular distance for groups that could potentially be included for abundance estimation. This is not the stage at which you set the truncation distance for detection function modeling; it is simply a preliminary cutoff for sightings made at unrealiable detection distances. The default is set at 5.5 km, which is the maximum distance sightings are typically approached (“closing mode”) during NOAA Fisheries surveys. beaufort_range: Vector of Beaufort sea states (integers) that are acceptable in estimating the detection function and density. Beaufort data with a decimal place will be rounded to the nearest integer to evaluate for inclusion. abeam_sightings: = If TRUE, sightings that occur aft of beam (i.e., 90 degrees) are included in estimating the detection function and densities. Default is FALSE: all abeam sightings will be ignored. strata_overlap_handling: This setting informs how effort is split into segments when surveys cross stratum boundaries, and also which stratum name is assigned to each row of data. Note that the main impact of this setting is on how effort is broken into segments; the assigned stratum name is for display only and will not constrain options for including/excluding strata in analyses farther along in the LTabundR workflow. The default option is \"smallest\", which means that effort will always be assigned to the smallest stratum when multiple strata overlap spatially. This is a safe option for surveys with “nested” strata (such as the Central North Pacific strata used by NOAA Fisheries; see below). Another option is \"each\"in which each time a stratum boundary is crossed the current segment will end and a new segment will begin. Also, stratum assignments for each row of effort will be shown as a concatenation of all the stratum layers overlapping at its position (e.g., “OtherCNP&amp;HI_EEZ”). Note that the \"each\" option segmentizes effort in the exact same was as \"smallest\" when strata are fully nested; its main advantage is in dealing with partially overlapping strata (such as strata used in the Marianas Archipelago; see below). The third option is \"largest\", in which the largest of overlapping strata is used to assign a stratum name to each row. (We are not sure what use case this would serve, but we offer it as an option for niche analyses.) distance_types: A character vector of the full-range of effort types that will be included in detection function estimation and density estimation, and therefore considered in effort segmentizing. Accepted values are \"S\" (systematic/standard effort), \"F\" (fine-scale effort), and \"N\" (non-systematic/non-standard effort, in which systematic protocols are being used but effort is not occurring along design-based transect routes). The default values are c(\"S\",\"F\",\"N\"). Note that it is possible to further subset the effort types specifically for density estimation in the line transect analysis function (see later chapter). distance_modes: The effort modes that will be included in detection function estimation and density estimation, and therefore considered in effort segmentizing. Accepted values are \"P\" (passing) and \"C\" (closing), and the default values are c(\"P\",\"C\"). distance_on_off: The value(s) of OnEffort (On Effort is TRUE, Off Effort is FALSE) that will be included in detection function estimation and density estimation, and therefore considered in effort segmentizing. Default is TRUE only. (We don’t expect FALSE or c(TRUE,FALSE) to be used much, if at all, but we make this option available). Example code Use settings defaults No strata or group size calibration. settings &lt;- load_settings() Use settings defaults, but with strata # Load strata dataframes data(strata_cnp) settings &lt;- load_settings(strata = strata_cnp) Customize survey, but not cohorts This code will process survey data such that effort segments are 5 km in length, with any effort falling outside of the provided geostrata relegated to a virtual geostratum named \"out\". Since a cohort is not specified, the default values will be used. # Load built-in datasets data(strata_cnp) data(group_size_coefficients) data(ships) data(species_codes) # Survey settings survey &lt;- load_survey_settings(out_handling = &#39;stratum&#39;, min_row_interval = 5, max_row_interval = 1800, max_row_km = 10, km_filler = 1, speed_filler = 10 * 1.852, segment_method = &#39;equallength&#39;, segment_target_km = 5, segment_max_interval = .3, segment_remainder_handling = c(&#39;append&#39;,&#39;segment&#39;), ship_list = ships, species_codes = species_codes, group_size_coefficients = group_size_coefficients, smear_angles = FALSE) # Load settings settings &lt;- load_settings(strata = strata_cnp, survey) Fully custom: WHICEAS case study These are the settings we will use in the remainder of the tutorial. Survey-wide settings To emulate the analysis done in Bradford et al. (2021), we want to process effort with 150-km segments, relegating any remainder to its own segments. We also want to make sure to remove any survey data that falls outside of the geostrata, to ensure that detection functions are regionally specific. We will use the built-in tables for species codes, ship codes, and group size calibration coefficients. data(species_codes) data(ships) data(group_size_coefficients) survey &lt;- load_survey_settings( out_handling = &#39;remove&#39;, min_row_interval = 2, max_row_interval = 3600, max_row_km = 100, km_filler = 1, speed_filler = 10 * 1.852, segment_method = &quot;equallength&quot;, segment_target_km = 150, segment_max_interval = 24, segment_remainder_handling = c(&quot;segment&quot;), ship_list = ships, species_codes = species_codes, group_size_coefficients = group_size_coefficients, smear_angles = FALSE ) Geostrata We will use the built-in dataset of Central North Pacific geostrata: data(strata_cnp) strata_cnp %&gt;% names [1] &quot;HI_EEZ&quot; &quot;OtherCNP&quot; &quot;MHI&quot; &quot;WHICEAS&quot; [5] &quot;Spotted_OU&quot; &quot;Spotted_FI&quot; &quot;Spotted_BI&quot; &quot;Bottlenose_KaNi&quot; [9] &quot;Bottlenose_OUFI&quot; &quot;Bottlenose_BI&quot; &quot;nwhi_fkw&quot; &quot;mhi_fkw&quot; Species cohorts Cohort 1: All species At least one cohort needs to be specified in order to run process survey data, so this first cohort will serve as a ‘catch-all’ for species who do not need special handling. It does not hurt to include all species in this catch-all cohort – except perhaps by increasing the file-size of your processed data by a few kilobytes – even if you will be creating a separate, dedicated cohort for one of these species downstream. Having a catch-all cohort like this serves two purposes: (1) it avoids unforeseen complications if you will be modeling detection functions with multi-species pools, as mentioned above; and (2) it will simplify the code you will use to produce summary statistics of effort and sightings totals, since you will not need to pool together statistics from multiple cohorts. To build this catch-all cohort, we will not specify any species so that all species in the data are included, and we will specify that only the generic geostrata should be used, so that species specific insular stock boundaries are ignored. Here we show all possible inputs. Most of these match the built-in defaults. Those that do not (there are just 4) are noted with a commented asterisk. all_species &lt;- load_cohort_settings( id = &quot;all&quot;, # * species = NULL, strata = c(&#39;WHICEAS&#39;, &#39;HI_EEZ&#39;, &#39;OtherCNP&#39;), # * probable_species = FALSE, sighting_method = 0, cue_range = 0:7, school_size_range = c(0, 10000), school_size_calibrate = TRUE, calibration_floor = 0, use_low_if_na = TRUE, # * io_sightings = 0, geometric_mean_group = TRUE, truncation_km = 7.5, # * beaufort_range = 0:6, abeam_sightings = FALSE, strata_overlap_handling = c(&quot;smallest&quot;), distance_types = c(&#39;S&#39;,&#39;F&#39;,&#39;N&#39;), distance_modes = c(&#39;P&#39;,&#39;C&#39;), distance_on_off = TRUE ) Cohort 2: Bottlenose dolphin As mentioned above, bottlenose dolphins are going to be analyzed as part of a multi-species pool that includes the rough-toothed dolphin, Risso’s dolphin, and pygmy killer whale. Because those species’ codes will be needed to model the detection function used in bottlenose dolphin density/abundance estimation, those species codes will be included in this cohort’s settings. Also mentioned above: Hawaii has a pelagic population of bottlenose dolphins as well as several distinct insular stocks. In this case study, we are interested in estimating only the abundance of the pelagic population, which means we will need to include geostrata of the insular stock boundaries in order to make sure the effort and sightings within those insular areas are ignored. That means we will specify the generic geostrata (\"WHICEAS\", \"HI_EEZ\", and \"Other_CNP\"), as well as the geostrata for the insular stocks. We can spell out as many any inputs that we wish, but here will only show the inputs that are non-default and/or different from Cohort 1 above. bottlenose &lt;- load_cohort_settings( id = &quot;bottlenose&quot;, species = c(&#39;015&#39;, &#39;018&#39;, &#39;021&#39;, &#39;032&#39;), strata = c(&#39;WHICEAS&#39;, &#39;HI_EEZ&#39;, &#39;OtherCNP&#39;, &#39;Bottlenose_BI&#39;, &#39;Bottlenose_OUFI&#39;, &#39;Bottlenose_KaNi&#39;), use_low_if_na = TRUE, truncation_km = 7.5) Cohort 3: Pantropical spotted dolphin Similar to the bottlenose dolphin above, spotted dolphins in Hawaiian waters belong to pelagic stocks as well as insular stocks. We will be estimating density/abundance for the only pelagic stocks here, but we need to include the geostrata for the insular stocks in order to ignore their effort and sightings. spotted &lt;- load_cohort_settings( id = &quot;spotted&quot;, species = &#39;002&#39;, strata = c(&#39;WHICEAS&#39;, &#39;HI_EEZ&#39;, &#39;OtherCNP&#39;, &#39;Spotted_OU&#39;,&#39;Spotted_FI&#39;,&#39;Spotted_BI&#39;), use_low_if_na = TRUE, truncation_km = 7.5) Compile settings Finally, we create our settings object, providing cohorts as a list of lists: settings &lt;- load_settings(strata = strata_cnp, survey = survey, cohorts = list(all_species, bottlenose, spotted)) Save this settings object locally, to use in downstream scripts: save(settings, file=&#39;whiceas_settings.RData&#39;) "],["das.html", "2 DAS editing Reviewing a DAS file Staging edits Applying edits", " 2 DAS editing The LTabundR package includes several functions that facilitate the exploration of DAS files of WinCruz survey data, as well as a function that allows you to apply “edits” to the survey data in a reproducible way (i.e., using code, without modifying the actual data file). Reviewing a DAS file das_readtext() The swfscDAS package has functions for reading in a DAS file and parsing it into columns of fixed-width text. To complement those functions, LTabundR includes the function das_readtext(), which reads in a DAS file without applying any column parsing, so that the data can be read in its true raw format. # Local path to DAS file das_file &lt;- &#39;data/surveys/CenPac1986-2020_Final_alb.das&#39; das &lt;- das_readtext(das_file) das$das %&gt;% head(20) [1] &quot; 1B 1805 073086 N31:57. W116:57. 989&quot; [2] &quot; 1S 1805 073086 N31:57. W116:57. 01 004 6 4 300 1.77 1.5&quot; [3] &quot; 2A 1805 073086 N31:57. W116:57. 01 15.0 N 099&quot; [4] &quot; 1 004 0001 0001 0001 100 000 000 000&quot; [5] &quot; 1S 0610 073186 N30:05. W116:04. 01 004 3 4 340 5.00 0.1&quot; [6] &quot; 2A 0610 073186 N30:05. W116:04. 01 18.3 N 005&quot; [7] &quot; 1 004 0150 0175 0125 100 000 000 000&quot; [8] &quot; 3B.1311 073186 N29:46. W115:52. 989 N&quot; [9] &quot; 4R.1311 073186 N29:46. W115:52. N&quot; [10] &quot; 5P.1311 073186 N29:46. W115:52. 022 031 056&quot; [11] &quot; 6V.1311 073186 N29:46. W115:52. 4 16.1&quot; [12] &quot; 7N.1311 073186 N29:46. W115:52. 167 10.5&quot; [13] &quot; 8W.1311 073186 N29:46. W115:52. 2&quot; [14] &quot; 9V.1329 073186 N29:43. W115:51. 5 16.1&quot; [15] &quot; 10P.1404 073186 N29:37. W115:50. 004 056 062&quot; [16] &quot; 11V.1404 073186 N29:37. W115:50. 5 18.3&quot; [17] &quot; 12S.1406 073186 N29:37. W115:50. 02 004 3 4 355 1.4 1.8&quot; [18] &quot; 13A.1406 073186 N29:37. W115:50. 02 18.3 Y 005&quot; [19] &quot; 1 022 0030 0100 0020 100 000 000 000&quot; [20] &quot; 2 004 0250 0300 0200 100 000 000 000&quot; To follow along, this data file can be downloaded here. das_inspector() We also provide a function, das_inspector(), that allows you to explore a DAS file within an interactive Shiny app. This app can also be used to find, prepare, and preview coded edits to the DAS data. dasi &lt;- das_inspector(das_file) In this app, you specify the rows and ‘columns’ (i.e., character indices) that will be affected by your edit, the edit you want to apply, and the type of edit it will be (see next subsection). A screenshot of the das_inspector() app: . Staging edits The interactive app launched by the das_inspector() function, introduced above, allows you to prepare and preview edits to your DAS data. Once you have an edit prepared and previewed within the app, you can then “save” that edit with the click of a button, and when you close the app your log of staged edits will be returned as a list. Here is the list produced from the edit staged in the screenshot above: . For this reason, it can be useful to type the above command such that the output is saved into an object; in the example above we saved the output into an object named dasi. You can then pass this list of edits to das_editor() (next subsection on this page), or as an argument within process_surveys() (next page). However, for the sake of full code reproducibility, it may be most useful to use this output to manually write coded edits into an R script, as demonstrated in the remainder of this section, which you can then use for the next stages of data processing. Below we step through various types of edits that can be applied to a DAS file. We show you how to format the staged edit, then we use the LTabundR function das_editor() to show the result of applying the edit. We discuss das_editor() in detail in the next section of this chapter. The das_editor() function can currently handle 6 types of edits, which we shall demonstrate below. Types of editing To demonstrate the types of editing actions that can be achieved through das_editor(), we will use the DAS file from the 2020 WHICEAS survey (you can download here). # Local path to das_file das_file &lt;- &quot;data/surveys/HICEASwinter2020.das&quot; das &lt;- das_readtext(das_file) Verbatim text replacement The edit will be interpreted verbatim as text that will replace the specified data. edits &lt;- list(list(das_file = das_file, type = &#39;text&#39;, rows = 10:15, chars = 20:39, edit = &#39;lat, lon&#39;)) Here is what this edit log object will look like: edits [[1]] [[1]]$das_file [1] &quot;data/surveys/HICEASwinter2020.das&quot; [[1]]$type [1] &quot;text&quot; [[1]]$rows [1] 10 11 12 13 14 15 [[1]]$chars [1] 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 [[1]]$edit [1] &quot;lat, lon&quot; And here is the effect it will have: # Before das$das[9:16] [1] &quot;009* 072752 011920 N21:50.58 W159:46.26&quot; [2] &quot;010* 072952 011920 N21:50.91 W159:46.33&quot; [3] &quot;011B.073111 011920 N21:51.12 W159:46.36 2001 C -10 N&quot; [4] &quot;012R.073111 011920 N21:51.12 W159:46.36 F&quot; [5] &quot;013P.073111 011920 N21:51.12 W159:46.36 126 307 238 &quot; [6] &quot;014V.073111 011920 N21:51.12 W159:46.36 4 06 120 15.0&quot; [7] &quot;015N.073111 011920 N21:51.12 W159:46.36 350 09.9&quot; [8] &quot;016W.073111 011920 N21:51.12 W159:46.36 5 120 6.0&quot; # After dase &lt;- das_editor(edits) --- applying edits to DAS file: data/surveys/HICEASwinter2020.das --- --- verbatim/function text replacements ... --- --- --- working on edit 1 ... dase$das[[1]]$das$das[9:16] [1] &quot;009* 072752 011920 N21:50.58 W159:46.26&quot; [2] &quot;010* 072952 011920 lat, lon&quot; [3] &quot;011B.073111 011920 lat, lon 2001 C -10 N&quot; [4] &quot;012R.073111 011920 lat, lon F&quot; [5] &quot;013P.073111 011920 lat, lon 126 307 238 &quot; [6] &quot;014V.073111 011920 lat, lon 4 06 120 15.0&quot; [7] &quot;015N.073111 011920 lat, lon 350 09.9&quot; [8] &quot;016W.073111 011920 N21:51.12 W159:46.36 5 120 6.0&quot; Function-based text replacement The edit will be evaluated as a function that is applied to the specified characters in each of rows. edits &lt;- list(list(das_file = das_file, type = &#39;function&#39;, rows = 10:15, chars = 20:39, edit = &#39;tolower&#39;)) # Before das$das[9:16] [1] &quot;009* 072752 011920 N21:50.58 W159:46.26&quot; [2] &quot;010* 072952 011920 N21:50.91 W159:46.33&quot; [3] &quot;011B.073111 011920 N21:51.12 W159:46.36 2001 C -10 N&quot; [4] &quot;012R.073111 011920 N21:51.12 W159:46.36 F&quot; [5] &quot;013P.073111 011920 N21:51.12 W159:46.36 126 307 238 &quot; [6] &quot;014V.073111 011920 N21:51.12 W159:46.36 4 06 120 15.0&quot; [7] &quot;015N.073111 011920 N21:51.12 W159:46.36 350 09.9&quot; [8] &quot;016W.073111 011920 N21:51.12 W159:46.36 5 120 6.0&quot; # After dase &lt;- das_editor(edits) --- applying edits to DAS file: data/surveys/HICEASwinter2020.das --- --- verbatim/function text replacements ... --- --- --- working on edit 1 ... dase$das[[1]]$das$das[9:16] [1] &quot;009* 072752 011920 N21:50.58 W159:46.26&quot; [2] &quot;010* 072952 011920 n21:50.91 w159:46.33&quot; [3] &quot;011B.073111 011920 n21:51.12 w159:46.36 2001 C -10 N&quot; [4] &quot;012R.073111 011920 n21:51.12 w159:46.36 F&quot; [5] &quot;013P.073111 011920 n21:51.12 w159:46.36 126 307 238 &quot; [6] &quot;014V.073111 011920 n21:51.12 w159:46.36 4 06 120 15.0&quot; [7] &quot;015N.073111 011920 n21:51.12 w159:46.36 350 09.9&quot; [8] &quot;016W.073111 011920 N21:51.12 W159:46.36 5 120 6.0&quot; A special application of this form of editing is adjusting timestamps using the LTabundR function das_time(). In this next example, we subtract an hour from the first 5 rows of timestamps: edits &lt;- list(list(das_file = das_file, type = &#39;function&#39;, rows = 1:5, chars = 6:40, edit = &#39;function(x){das_time(x, tz_adjust = -1)$dt}&#39;)) # Before das$das[1:6] [1] &quot;001* 071152 011920 N21:47.99 W159:45.91&quot; [2] &quot;002* 071352 011920 N21:48.31 W159:45.94&quot; [3] &quot;003* 071552 011920 N21:48.63 W159:45.97&quot; [4] &quot;004* 071752 011920 N21:48.95 W159:46.01&quot; [5] &quot;005* 071952 011920 N21:49.28 W159:46.04&quot; [6] &quot;006* 072152 011920 N21:49.60 W159:46.08&quot; # After dase &lt;- das_editor(edits) --- applying edits to DAS file: data/surveys/HICEASwinter2020.das --- --- verbatim/function text replacements ... --- --- --- working on edit 1 ... dase$das[[1]]$das$das[1:6] [1] &quot;001* 061152 011920 N21:47.99 W159:45.91&quot; [2] &quot;002* 061352 011920 N21:48.31 W159:45.94&quot; [3] &quot;003* 061552 011920 N21:48.63 W159:45.97&quot; [4] &quot;004* 061752 011920 N21:48.95 W159:46.01&quot; [5] &quot;005* 061952 011920 N21:49.28 W159:46.04&quot; [6] &quot;006* 072152 011920 N21:49.60 W159:46.08&quot; In the event that a survey was conducted using UTC timestamps instead of local time, you can adjust each timestamp according to its actual timezone as determined from its corresponding lat/long coordinates. Let’s say the first 5 timestamps were collected in UTC by accident. The following code could correct that mistake: edits &lt;- list(list(das_file = das_file, type = &#39;function&#39;, rows = 1:5, chars = 6:40, edit = &#39;function(x){das_time(x, tz_adjust = &quot;from utc&quot;)$dt}&#39;)) # Before das$das[1:6] [1] &quot;001* 071152 011920 N21:47.99 W159:45.91&quot; [2] &quot;002* 071352 011920 N21:48.31 W159:45.94&quot; [3] &quot;003* 071552 011920 N21:48.63 W159:45.97&quot; [4] &quot;004* 071752 011920 N21:48.95 W159:46.01&quot; [5] &quot;005* 071952 011920 N21:49.28 W159:46.04&quot; [6] &quot;006* 072152 011920 N21:49.60 W159:46.08&quot; # After dase &lt;- das_editor(edits) --- applying edits to DAS file: data/surveys/HICEASwinter2020.das --- --- verbatim/function text replacements ... --- --- --- working on edit 1 ... dase$das[[1]]$das$das[1:6] [1] &quot;001* 201152 011820 N21:47.99 W159:45.91&quot; [2] &quot;002* 201352 011820 N21:48.31 W159:45.94&quot; [3] &quot;003* 201552 011820 N21:48.63 W159:45.97&quot; [4] &quot;004* 201752 011820 N21:48.95 W159:46.01&quot; [5] &quot;005* 201952 011820 N21:49.28 W159:46.04&quot; [6] &quot;006* 072152 011920 N21:49.60 W159:46.08&quot; Moving data The rows will be deleted from their current location and pasted immediately below the row number specified by edit. The moved rows will be given the same date, time, latitude, and longitude, as the edit row. edits &lt;- list(list(das_file = das_file, type = &#39;move&#39;, rows = 10, chars = NULL, edit = 15)) # Before das$das[9:16] [1] &quot;009* 072752 011920 N21:50.58 W159:46.26&quot; [2] &quot;010* 072952 011920 N21:50.91 W159:46.33&quot; [3] &quot;011B.073111 011920 N21:51.12 W159:46.36 2001 C -10 N&quot; [4] &quot;012R.073111 011920 N21:51.12 W159:46.36 F&quot; [5] &quot;013P.073111 011920 N21:51.12 W159:46.36 126 307 238 &quot; [6] &quot;014V.073111 011920 N21:51.12 W159:46.36 4 06 120 15.0&quot; [7] &quot;015N.073111 011920 N21:51.12 W159:46.36 350 09.9&quot; [8] &quot;016W.073111 011920 N21:51.12 W159:46.36 5 120 6.0&quot; # After dase &lt;- das_editor(edits) --- applying edits to DAS file: data/surveys/HICEASwinter2020.das --- --- move, copy/paste, insertion, and deletion events ... --- --- --- working on edit 1 ... dase$das[[1]]$das$das[9:16] [1] &quot;009* 072752 011920 N21:50.58 W159:46.26&quot; [2] &quot;011B.073111 011920 N21:51.12 W159:46.36 2001 C -10 N&quot; [3] &quot;012R.073111 011920 N21:51.12 W159:46.36 F&quot; [4] &quot;013P.073111 011920 N21:51.12 W159:46.36 126 307 238 &quot; [5] &quot;014V.073111 011920 N21:51.12 W159:46.36 4 06 120 15.0&quot; [6] &quot;015N.073111 011920 N21:51.12 W159:46.36 350 09.9&quot; [7] &quot;010* 073111 011920 N21:51.12 W159:46.36&quot; [8] &quot;016W.073111 011920 N21:51.12 W159:46.36 5 120 6.0&quot; Copying &amp; pasting data The rows will be copied from their current location and pasted immediately below the row number specified by edit. The pasted rows will be given the same date, time, latitude, and longitude, as the edit row. edits &lt;- list(list(das_file = das_file, type = &#39;copy&#39;, rows = 10, edit = 15)) # Before das$das[9:17] [1] &quot;009* 072752 011920 N21:50.58 W159:46.26&quot; [2] &quot;010* 072952 011920 N21:50.91 W159:46.33&quot; [3] &quot;011B.073111 011920 N21:51.12 W159:46.36 2001 C -10 N&quot; [4] &quot;012R.073111 011920 N21:51.12 W159:46.36 F&quot; [5] &quot;013P.073111 011920 N21:51.12 W159:46.36 126 307 238 &quot; [6] &quot;014V.073111 011920 N21:51.12 W159:46.36 4 06 120 15.0&quot; [7] &quot;015N.073111 011920 N21:51.12 W159:46.36 350 09.9&quot; [8] &quot;016W.073111 011920 N21:51.12 W159:46.36 5 120 6.0&quot; [9] &quot;017*.073152 011920 N21:51.24 W159:46.39&quot; # After dase &lt;- das_editor(edits) --- applying edits to DAS file: data/surveys/HICEASwinter2020.das --- --- move, copy/paste, insertion, and deletion events ... --- --- --- working on edit 1 ... dase$das[[1]]$das$das[9:17] [1] &quot;009* 072752 011920 N21:50.58 W159:46.26&quot; [2] &quot;010* 072952 011920 N21:50.91 W159:46.33&quot; [3] &quot;011B.073111 011920 N21:51.12 W159:46.36 2001 C -10 N&quot; [4] &quot;012R.073111 011920 N21:51.12 W159:46.36 F&quot; [5] &quot;013P.073111 011920 N21:51.12 W159:46.36 126 307 238 &quot; [6] &quot;014V.073111 011920 N21:51.12 W159:46.36 4 06 120 15.0&quot; [7] &quot;015N.073111 011920 N21:51.12 W159:46.36 350 09.9&quot; [8] &quot;010* 073111 011920 N21:51.12 W159:46.36&quot; [9] &quot;016W.073111 011920 N21:51.12 W159:46.36 5 120 6.0&quot; Inserting data The text provided in edit will be inserted verbatim immediately below the first of the rows provided. edits &lt;- list(list(das_file = das_file, type = &#39;insert&#39;, rows = 10, edit = &quot;SEQ* HHMMSS MMDDYY NDG:MI.NT WDEG:MI.NT&quot;)) # Before das$das[9:16] [1] &quot;009* 072752 011920 N21:50.58 W159:46.26&quot; [2] &quot;010* 072952 011920 N21:50.91 W159:46.33&quot; [3] &quot;011B.073111 011920 N21:51.12 W159:46.36 2001 C -10 N&quot; [4] &quot;012R.073111 011920 N21:51.12 W159:46.36 F&quot; [5] &quot;013P.073111 011920 N21:51.12 W159:46.36 126 307 238 &quot; [6] &quot;014V.073111 011920 N21:51.12 W159:46.36 4 06 120 15.0&quot; [7] &quot;015N.073111 011920 N21:51.12 W159:46.36 350 09.9&quot; [8] &quot;016W.073111 011920 N21:51.12 W159:46.36 5 120 6.0&quot; # After dase &lt;- das_editor(edits) --- applying edits to DAS file: data/surveys/HICEASwinter2020.das --- --- move, copy/paste, insertion, and deletion events ... --- --- --- working on edit 1 ... dase$das[[1]]$das$das[9:16] [1] &quot;009* 072752 011920 N21:50.58 W159:46.26&quot; [2] &quot;010* 072952 011920 N21:50.91 W159:46.33&quot; [3] &quot;SEQ* HHMMSS MMDDYY NDG:MI.NT WDEG:MI.NT&quot; [4] &quot;011B.073111 011920 N21:51.12 W159:46.36 2001 C -10 N&quot; [5] &quot;012R.073111 011920 N21:51.12 W159:46.36 F&quot; [6] &quot;013P.073111 011920 N21:51.12 W159:46.36 126 307 238 &quot; [7] &quot;014V.073111 011920 N21:51.12 W159:46.36 4 06 120 15.0&quot; [8] &quot;015N.073111 011920 N21:51.12 W159:46.36 350 09.9&quot; Deleting data The specified rows will be deleted. edits &lt;- list(list(das_file = das_file, type = &#39;delete&#39;, rows = 10)) # Before das$das[9:16] [1] &quot;009* 072752 011920 N21:50.58 W159:46.26&quot; [2] &quot;010* 072952 011920 N21:50.91 W159:46.33&quot; [3] &quot;011B.073111 011920 N21:51.12 W159:46.36 2001 C -10 N&quot; [4] &quot;012R.073111 011920 N21:51.12 W159:46.36 F&quot; [5] &quot;013P.073111 011920 N21:51.12 W159:46.36 126 307 238 &quot; [6] &quot;014V.073111 011920 N21:51.12 W159:46.36 4 06 120 15.0&quot; [7] &quot;015N.073111 011920 N21:51.12 W159:46.36 350 09.9&quot; [8] &quot;016W.073111 011920 N21:51.12 W159:46.36 5 120 6.0&quot; # After dase &lt;- das_editor(edits) --- applying edits to DAS file: data/surveys/HICEASwinter2020.das --- --- move, copy/paste, insertion, and deletion events ... --- --- --- working on edit 1 ... dase$das[[1]]$das$das[9:16] [1] &quot;009* 072752 011920 N21:50.58 W159:46.26&quot; [2] &quot;011B.073111 011920 N21:51.12 W159:46.36 2001 C -10 N&quot; [3] &quot;012R.073111 011920 N21:51.12 W159:46.36 F&quot; [4] &quot;013P.073111 011920 N21:51.12 W159:46.36 126 307 238 &quot; [5] &quot;014V.073111 011920 N21:51.12 W159:46.36 4 06 120 15.0&quot; [6] &quot;015N.073111 011920 N21:51.12 W159:46.36 350 09.9&quot; [7] &quot;016W.073111 011920 N21:51.12 W159:46.36 5 120 6.0&quot; [8] &quot;017*.073152 011920 N21:51.24 W159:46.39&quot; Actual edits The above examples were silly demonstrations of the types of edits that can be handled by das_editor(). Here we show the preparation of four edits that we will actually use when we process surveys on the next page. These edits will be applied to the following DAS file of survey data from 1986-2020: # Local path to das_file das_file &lt;- &#39;data/surveys/CenPac1986-2020_Final_alb.das&#39; das &lt;- das_readtext(das_file) Acoustic update to a 2020 beaked whale sighting During the 2020 WHICEAS study, a sighting was assigned the species code, \"051” or Unidentified Mesoplodon in the field: das[606393,] [1] &quot;304A.142408 012020 N21:48.31 W160:42.18 044 N N 051 &quot; (Note that we used das_inspector() to get the row numbers for this region of the data.) Later the acoustics team used click detections to re-classify this species to Blainsville’s beaked whale (species code \"059\"). To update the DAS file accordingly, we can edit the species code directly within the appropriate line: edit_acoustic &lt;- list(das_file = das_file, type = &#39;text&#39;, rows = 606393, chars = 62:64, edit = &#39;059&#39;) Here is what this change will look like: dase &lt;- das_editor(list(edit_acoustic)) --- applying edits to DAS file: data/surveys/CenPac1986-2020_Final_alb.das --- --- verbatim/function text replacements ... --- --- --- working on edit 1 ... dase$das[[1]]$das$das[606393] [1] &quot;304A.142408 012020 N21:48.31 W160:42.18 044 N N 059 &quot; Cruise 1607 sighting 55 This sighting, at sequence ID 032 below, currently triggers errors in swfscDAS due to a manually entered R event a few lines above that does not have the P (observer positions) event that typically follows it. Without tht P entry, the observer positions of the sighting are unknown. das[128111:128125,] [1] &quot;022P 120643 041597 N37:00.08 W151:55.47 143 091 005&quot; [2] &quot;023C 120643 041597 N37:00.08 W151:55.47 both right and left on 7x and naked eye&quot; [3] &quot;024N 120643 041597 N37:00.08 W151:55.47 075 08.0&quot; [4] &quot;025W 120643 041597 N37:00.08 W151:55.47 1 310 6.0&quot; [5] &quot;026C 120916 041597 N37:00.33 W151:55.18 both right and left on 7x and naked eye&quot; [6] &quot;027* 121120 041597 N37:00.45 W151:54.85&quot; [7] &quot;028C 121933 041597 N37:00.75 W151:53.50 both right and left back on 25x&quot; [8] &quot; R.121933 041597 N37:00.75 W151:53.50 S&quot; [9] &quot;029*.122120 041597 N37:00.81 W151:53.22&quot; [10] &quot;030V.122120 041597 N37:00.81 W151:53.22 4 07 320 21.0&quot; [11] &quot;031C.122946 041597 N37:01.12 W151:51.77 wind speed appears lower than bridge speed, 10-15&quot; [12] &quot;032S.123023 041597 N37:01.14 W151:51.70 055 005 3 4 058 4.0 0.8&quot; [13] &quot;033A.123023 041597 N37:01.14 W151:51.70 055 N N 022&quot; [14] &quot; 1 005 0002 0002 0002 100&quot; [15] &quot;034C.123120 041597 N37:01.17 W151:51.51 remained on effort after sighting&quot; (Note that we used das_inspector() to get the row numbers for this region of the data.) To fix this, we can stage an edit that copies the P line that occurs minutes earlier and pastes that line just below the rogue R line. edit_1607_55 &lt;- list(das_file = das_file, type = &#39;copy&#39;, rows = 128111, chars = NULL, edit = 128118) Here is what this change will look like: dase &lt;- das_editor(list(edit_1607_55)) --- applying edits to DAS file: data/surveys/CenPac1986-2020_Final_alb.das --- --- move, copy/paste, insertion, and deletion events ... --- --- --- working on edit 1 ... dase$das[[1]]$das$das[128111:128123] [1] &quot;022P 120643 041597 N37:00.08 W151:55.47 143 091 005&quot; [2] &quot;023C 120643 041597 N37:00.08 W151:55.47 both right and left on 7x and naked eye&quot; [3] &quot;024N 120643 041597 N37:00.08 W151:55.47 075 08.0&quot; [4] &quot;025W 120643 041597 N37:00.08 W151:55.47 1 310 6.0&quot; [5] &quot;026C 120916 041597 N37:00.33 W151:55.18 both right and left on 7x and naked eye&quot; [6] &quot;027* 121120 041597 N37:00.45 W151:54.85&quot; [7] &quot;028C 121933 041597 N37:00.75 W151:53.50 both right and left back on 25x&quot; [8] &quot; R.121933 041597 N37:00.75 W151:53.50 S&quot; [9] &quot;022P 121933 041597 N37:00.75 W151:53.50 143 091 005&quot; [10] &quot;029*.122120 041597 N37:00.81 W151:53.22&quot; [11] &quot;030V.122120 041597 N37:00.81 W151:53.22 4 07 320 21.0&quot; [12] &quot;031C.122946 041597 N37:01.12 W151:51.77 wind speed appears lower than bridge speed, 10-15&quot; [13] &quot;032S.123023 041597 N37:01.14 W151:51.70 055 005 3 4 058 4.0 0.8&quot; Cruise 1607 sighting 68 This sighting faces a similar issue: a rogue R event without the follow-up P event. This case is also missing the follow-up V event (viewing conditions). das[129980:129993,] [1] &quot;012* 065738 042797 N31:03.98 W136:03.60&quot; [2] &quot;013C 065944 042797 N31:04.25 W136:03.55 Both observers on 7X since going ON EFFORT this morning. JP.&quot; [3] &quot;014P 070118 042797 N31:04.43 W136:03.51 091 005 148&quot; [4] &quot;015V 070118 042797 N31:04.43 W136:03.51 5 06 320 20.0&quot; [5] &quot;016N 070118 042797 N31:04.43 W136:03.51 010 08.0&quot; [6] &quot;017W 070118 042797 N31:04.43 W136:03.51 1 02 03 035 6.0&quot; [7] &quot;018C 070257 042797 N31:04.65 W136:03.47 This rotation is now on the 25X&quot; [8] &quot; R.070257 042797 N31:04.65 W136:03.47 S&quot; [9] &quot;019*.070738 042797 N31:05.25 W136:03.36&quot; [10] &quot;020S.071601 042797 N31:06.34 W136:03.16 068 148 3 4 028 1.8 1.5&quot; [11] &quot;021A.071601 042797 N31:06.34 W136:03.16 068 N N 037&quot; [12] &quot; 1 091 0009 0011 0008 100&quot; [13] &quot; 2 005 0014 0015 0012 100&quot; [14] &quot; 3 148 0018 0023 0015 100&quot; To fix this we will stage a similar edit, this time copying and pasting two rows (P and V events) below the rogue R event: edit_1607_68 &lt;- list(das_file = das_file, type = &#39;copy&#39;, rows = c(129982, 129983 , 129985), chars = NULL, edit = 129987) Preview of change: dase &lt;- das_editor(list(edit_1607_68)) --- applying edits to DAS file: data/surveys/CenPac1986-2020_Final_alb.das --- --- move, copy/paste, insertion, and deletion events ... --- --- --- working on edit 1 ... dase$das[[1]]$das$das[129982:129995] [1] &quot;014P 070118 042797 N31:04.43 W136:03.51 091 005 148&quot; [2] &quot;015V 070118 042797 N31:04.43 W136:03.51 5 06 320 20.0&quot; [3] &quot;016N 070118 042797 N31:04.43 W136:03.51 010 08.0&quot; [4] &quot;017W 070118 042797 N31:04.43 W136:03.51 1 02 03 035 6.0&quot; [5] &quot;018C 070257 042797 N31:04.65 W136:03.47 This rotation is now on the 25X&quot; [6] &quot; R.070257 042797 N31:04.65 W136:03.47 S&quot; [7] &quot;014P 070257 042797 N31:04.65 W136:03.47 091 005 148&quot; [8] &quot;015V 070257 042797 N31:04.65 W136:03.47 5 06 320 20.0&quot; [9] &quot;017W 070257 042797 N31:04.65 W136:03.47 1 02 03 035 6.0&quot; [10] &quot;019*.070738 042797 N31:05.25 W136:03.36&quot; [11] &quot;020S.071601 042797 N31:06.34 W136:03.16 068 148 3 4 028 1.8 1.5&quot; [12] &quot;021A.071601 042797 N31:06.34 W136:03.16 068 N N 037&quot; [13] &quot; 1 091 0009 0011 0008 100&quot; [14] &quot; 2 005 0014 0015 0012 100&quot; Cruise 1621 sighting 245 This is another case of a rogue R event, again missing both the requisite P and the V post-R events. das[271930:271939,] [1] &quot;036E 085345 103002 N20:22.56 W160:02.21 U&quot; [2] &quot;037R.085346 103002 N20:22.56 W160:02.21 S&quot; [3] &quot;038P.085346 103002 N20:22.56 W160:02.21 126 224 200&quot; [4] &quot;039V.085346 103002 N20:22.56 W160:02.21 5 07 000 21.0&quot; [5] &quot;040N.085346 103002 N20:22.56 W160:02.21 285 09.7&quot; [6] &quot;041W.085346 103002 N20:22.56 W160:02.21 1 06 02 045 6.0&quot; [7] &quot;042E 085352 103002 N20:22.56 W160:02.22 U&quot; [8] &quot;043R.085354 103002 N20:22.57 W160:02.23 S&quot; [9] &quot;048S.085359 103002 N20:22.57 W160:02.24 245 200 3 4 057 11.0 0.35 270 2.0&quot; [10] &quot;049A.085359 103002 N20:22.57 W160:02.24 245 N N 015&quot; Staged edit: edit_1621_245 &lt;- list(das_file = das_file, type = &#39;copy&#39;, rows = 271932:271933, chars = NULL, edit = 271937) Preview of change: dase &lt;- das_editor(list(edit_1621_245)) --- applying edits to DAS file: data/surveys/CenPac1986-2020_Final_alb.das --- --- move, copy/paste, insertion, and deletion events ... --- --- --- working on edit 1 ... dase$das[[1]]$das$das[271930:271941] [1] &quot;036E 085345 103002 N20:22.56 W160:02.21 U&quot; [2] &quot;037R.085346 103002 N20:22.56 W160:02.21 S&quot; [3] &quot;038P.085346 103002 N20:22.56 W160:02.21 126 224 200&quot; [4] &quot;039V.085346 103002 N20:22.56 W160:02.21 5 07 000 21.0&quot; [5] &quot;040N.085346 103002 N20:22.56 W160:02.21 285 09.7&quot; [6] &quot;041W.085346 103002 N20:22.56 W160:02.21 1 06 02 045 6.0&quot; [7] &quot;042E 085352 103002 N20:22.56 W160:02.22 U&quot; [8] &quot;043R.085354 103002 N20:22.57 W160:02.23 S&quot; [9] &quot;038P.085354 103002 N20:22.57 W160:02.23 126 224 200&quot; [10] &quot;039V.085354 103002 N20:22.57 W160:02.23 5 07 000 21.0&quot; [11] &quot;048S.085359 103002 N20:22.57 W160:02.24 245 200 3 4 057 11.0 0.35 270 2.0&quot; [12] &quot;049A.085359 103002 N20:22.57 W160:02.24 245 N N 015&quot; Timestamp issues with Cruise 1004 This edit will correct for the fact that all of Cruise 1004 was conducted using UTC timestamps instead of local timestamps. edit_1004_utc &lt;- list(das_file = das_file, type = &#39;function&#39;, rows = 433327:437665, chars = 6:39, edit = &#39;function(x){das_time(x, tz_adjust = &quot;from utc&quot;)$dt}&#39;) # Before das$das[433326:433330] # beginning of cruise [1] &quot;229* 181728 020510 N13:14.49 E145:00.34&quot; [2] &quot;001* 201109 041910 N13:35.01 E145:59.35&quot; [3] &quot;002* 201309 041910 N13:35.03 E145:59.42&quot; [4] &quot;003* 201509 041910 N13:35.08 E145:59.54&quot; [5] &quot;004* 201709 041910 N13:35.13 E145:59.70&quot; das$das[437664:437667] # end of cruise [1] &quot;431* 044625 050410 N21:15.78 W158:53.32&quot; [2] &quot;432* 044825 050410 N21:16.05 W158:53.31&quot; [3] &quot;001* 144327 080410 N32:39.39 W117:13.60&quot; [4] &quot;002* 144827 080410 N32:38.58 W117:13.49&quot; dase &lt;- das_editor(list(edit_1004_utc)) # After dase$das[[1]]$das$das[433326:433330] [1] &quot;229* 181728 020510 N13:14.49 E145:00.34&quot; [2] &quot;001* 061109 042010 N13:35.01 E145:59.35&quot; [3] &quot;002* 061309 042010 N13:35.03 E145:59.42&quot; [4] &quot;003* 061509 042010 N13:35.08 E145:59.54&quot; [5] &quot;004* 061709 042010 N13:35.13 E145:59.70&quot; dase$das[[1]]$das$das[437664:437667] [1] &quot;431* 184625 050310 N21:15.78 W158:53.32&quot; [2] &quot;432* 184825 050310 N21:16.05 W158:53.31&quot; [3] &quot;001* 074327 080410 N32:39.39 W117:13.60&quot; [4] &quot;002* 144827 080410 N32:38.58 W117:13.49&quot; Note that this type of edit can be dangerous, however, since ships can cross time zone boundaries mid-day, potentially repeating timestamps and giving the appearance that the DAS data is out of chronological order, which may bring about consequences for data processing that are difficult to predict. This type of edit is also time-consuming; since the time zone needs to be calculated in each DAS row individually, this edit could take &gt;20 minutes to process. An expedited (and safer) approximation of this edit would be to simply adjust the timezone by the GMT offset for Guam (UTC + 10 hours). edit_1004_gmt10 &lt;- list(das_file = das_file, type = &#39;function&#39;, rows = 433327:437665, chars = 6:39, edit = &#39;function(x){das_time(x, tz_adjust = 10)$dt}&#39;) dase &lt;- das_editor(list(edit_1004_gmt10)) --- applying edits to DAS file: data/surveys/CenPac1986-2020_Final_alb.das --- --- verbatim/function text replacements ... --- --- --- working on edit 1 ... dase$das[[1]]$das$das[433326:433330] [1] &quot;229* 181728 020510 N13:14.49 E145:00.34&quot; [2] &quot;001* 061109 042010 N13:35.01 E145:59.35&quot; [3] &quot;002* 061309 042010 N13:35.03 E145:59.42&quot; [4] &quot;003* 061509 042010 N13:35.08 E145:59.54&quot; [5] &quot;004* 061709 042010 N13:35.13 E145:59.70&quot; dase$das[[1]]$das$das[437664:437667] [1] &quot;431* 144625 050410 N21:15.78 W158:53.32&quot; [2] &quot;432* 144825 050410 N21:16.05 W158:53.31&quot; [3] &quot;001* 144327 080410 N32:39.39 W117:13.60&quot; [4] &quot;002* 144827 080410 N32:38.58 W117:13.49&quot; Combining and saving edits Finally, we will collect these edits into a single list and save them for use during survey processing (next page). # Combine edits &lt;- list(edit_acoustic, edit_1607_55, edit_1607_68, edit_1621_245, #edit_1004_utc, edit_1004_gmt10) # Save saveRDS(edits,file=&#39;cnp_1986_2020_edits.RData&#39;) Applying edits The das_editor() function allows you to apply edits to a DAS file without modifiyng the original data. You supply edits to this function as a set of instructions saved within a list object. As explained in the previous section, you can prepare these instructions manually or use the das_inspector() function above to get help. For the sake of reproducibility we recommend documenting your edits as code in a script, then saving all the edits into a single list (see the previous R chunk above). Note that there is no limit to the number of edits that can be provided in a single list, and they can be provided in any order. When the edits are applied, the LTabundR function will sort the edits by DAS file and by edit type, and then apply edits in increasing order of “disruption”, i.e., text replacements first, then moving rows of data (no net change in number of rows), then copying-pasting, inserting, and deleting. LTabundR provides two functions for applying these staged edits: das_editor() and das_editor_tofile(). The former is a simple function that applies edits then returns the modified DAS data directly to the R console. It is a handy function for testing that a specific edit makes the intended changes successfully. The latter function, das_editor_tofile(), applies the edits then saves new versions of the DAS file. Those new files can then be passed to the data processing functions described in the next chapter. (See the next chapter for examples of how to use das_editor_tofile(). In summary, we recommend das_editor() for preparing each individual edit, and das_editor_tofile() when you are ready to apply all edits and move on to data processing. Note that the original DAS file is never modified or replaced. The edited DAS filename will have a suffix to indicate that it is an edited version of the data. This allows survey data to be modified reproducibly before being processed with LTabundR::process_surveys() without touching the original DAS data files or requiring analysts to duplicate files and make one-off modifications manually. "],["processing.html", "3 Data processing Behind the scenes Review", " 3 Data processing In our WHICEAS case study example, we are interested in estimating density/abundance for 2017 and 2020 only, but we want to use surveys from previous years to help model species detection functions. We will therefore be using a dataset of NOAA Fisheries surveys in the Central North Pacific from 1986 to 2020. # Local path to DAS file das_file &lt;- &#39;data/surveys/CenPac1986-2020_Final_alb.das&#39; To follow along, this data file can be downloaded here. You can process your survey data using a single function, process_surveys(), which takes two primary arguments: the filepath(s) to your DAS survey data, and your settings object. For example: cruz &lt;- process_surveys(das_file, settings) That single command will convert your raw DAS data to a “cruz” object, a list of polished datasets that are prepared to be passed to subsequent analyses. In our case we will use a third argument to apply edits to the DAS data before processing (see previous page for details on those edits): edits &lt;- readRDS(&#39;cnp_1986_2020_edits.RData&#39;) cruz &lt;- process_surveys(das_file, settings, edits) Behind the scenes The process_surveys() function is a wrapper for several discrete stages of data formatting/processing. Behind the scenes, each of those stages is carried out using a specific LTabundR function. The remainder of this page is a detailed step-by-step explanation of the data processing that occurs when you call process_surveys(). Edit cruise data If the edits input argument is supplied to process_surveys(), temporary copies of the DAS file(s) are made and edited before processing. This step is discussed on the previous page and implemented with the function das_editor_tofile(): das_file &lt;- das_editor_tofile(das_file, edits, suffix = &#39;_edited_example&#39;, verbose = TRUE) This function produces new DAS files with the edits applied, and returns an updated set of DAS filenames you can pass to the next stage of data processing. das_file [1] &quot;data/surveys/CenPac1986-2020_Final_alb_edited_example.das&quot; Bring in cruise data Read in and process your .DAS file using the functions in Sam Woodward’s swfscDAS package. To do so quickly, we built a wrapper function that makes this quick and easy: das &lt;- das_load(das_file, perform_checks = TRUE, print_glimpse = TRUE) Interpolate cruise data Run the following function if you wish to interpolate the DAS data to a more frequent time interval, which can be useful for stratum assignment and effort calculations in some circumstances. This is only done in process_surveys() if the settings object passed to it has the interpolate input for load_survey_settings() set to a number (indicating the desired interval in seconds). In this example the interval is set to 120 seconds. Interpolation will only occur for On-Effort rows. das &lt;- das_interpolate(das, new_interval = 120) Process strata Run the following function to add strata and study-area information to each row of DAS data: das_strata &lt;- process_strata(das, settings) This function loops through each stratum data.frame you have provided it in settings$strata, formats the stratum, and asks whether each DAS row occurs within it. For each stratum, a column named stratum_&lt;StratumName&gt; is added to the das object; each row in this column is TRUE (included) or FALSE. Format DAS data into a cruz object The function das_format() takes care of some final formatting and initiates the cruz object data structure. cruz &lt;- das_format(das_strata, verbose=TRUE) This function (1) removes rows with invalid Cruise numbers, times, or locations; (ii) calculates the distance, in km, between each row of data; (iii) adds a ship column to the dataset, with initials for the ship corresponding to each cruise; (iv) creates a new list, cohorts, which copies the cruise data for each cohort specified in your settings; and (v) adds a stratum column to the data in each cohort. That column specifies a single stratum assignment for each row of DAS data in the event of overlapping strata, based upon the cohort setting stratum_overlap_handling. The cruz object The function das_format() returns a list, which we have saved in an object named cruz, with several slots: cruz %&gt;% names [1] &quot;settings&quot; &quot;strata&quot; &quot;cohorts&quot; The slots strata and study_area provide the area, in square km, of each polygon being used: cruz$strata stratum area 1 HI_EEZ 2474595.762 [km^2] 2 OtherCNP 33817779.065 [km^2] 3 MHI 212033.063 [km^2] 4 WHICEAS 402948.734 [km^2] 5 Spotted_OU 5102.666 [km^2] 6 Spotted_FI 10509.869 [km^2] 7 Spotted_BI 39454.720 [km^2] 8 Bottlenose_KaNi 2755.024 [km^2] 9 Bottlenose_OUFI 14417.027 [km^2] 10 Bottlenose_BI 4668.072 [km^2] 11 NWHI 449375.569 [km^2] The slot cohorts is itself a list with one slot for each cohort. The slots are named using the id cohort setting. cruz$cohorts %&gt;% names [1] &quot;all&quot; &quot;bottlenose&quot; &quot;spotted&quot; Each cohort slot has a copy of the DAS data with a new stratum column, which contains a stratum assignment tailored to its cohort-specific settings. For instance, the all cohort, whose stratum_overlap_handling is set to \"smallest\", assigns the smallest stratum in the event of overlapping or nested strata: cruz$cohorts$all$stratum %&gt;% table(useNA=&#39;ifany&#39;) . HI_EEZ OtherCNP WHICEAS 117715 126252 85671 Since the bottlenose cohort uses a different subset of geostrata, its distribution of stratum assignments will also differ: cruz$cohorts$bottlenose$stratum %&gt;% table(useNA=&#39;ifany&#39;) . Bottlenose_BI Bottlenose_KaNi Bottlenose_OUFI HI_EEZ OtherCNP 3415 1495 6862 117715 126252 WHICEAS 73899 This list, with these three primary slots, will be referred to from hereon as a cruz object. Segmentize the data To allocate survey data into discrete ‘effort segments’, which are used in variance estimation in subsequent steps, run the function segmentize(). This process is controlled by both survey-wide and cohort-specific settings, which are now carried in a slot within the cruz object. The process is outlined in detail in the Appendix on Segmentizing. cruz &lt;- segmentize(cruz, verbose=FALSE) This function does not change the high-level structure of the cruz object … cruz %&gt;% names [1] &quot;settings&quot; &quot;strata&quot; &quot;cohorts&quot; … or the cohort names in the cohorts slot: cruz$cohorts %&gt;% names [1] &quot;all&quot; &quot;bottlenose&quot; &quot;spotted&quot; For each cohorts slot, the list structure is the same: cruz$cohorts$all %&gt;% names [1] &quot;segments&quot; &quot;das&quot; cruz$cohorts$bottlenose %&gt;% names [1] &quot;segments&quot; &quot;das&quot; cruz$cohorts$spotted %&gt;% names [1] &quot;segments&quot; &quot;das&quot; The segments slot contains summary data for each effort segment, including start/mid/end coordinates, average conditions, and segment distance: cruz$cohorts$all$segments %&gt;% glimpse Rows: 1,455 Columns: 39 $ seg_id &lt;dbl&gt; 2051, 2062, 2055, 2063, 2064, 2056, 2065, 2066, 2067, 205… $ Cruise &lt;dbl&gt; 901, 901, 901, 901, 901, 901, 901, 901, 901, 901, 901, 90… $ ship &lt;chr&gt; &quot;OES&quot;, &quot;OES&quot;, &quot;OES&quot;, &quot;OES&quot;, &quot;OES&quot;, &quot;OES&quot;, &quot;OES&quot;, &quot;OES&quot;, &quot;… $ stratum &lt;chr&gt; &quot;WHICEAS&quot;, &quot;WHICEAS&quot;, &quot;WHICEAS&quot;, &quot;WHICEAS&quot;, &quot;WHICEAS&quot;, &quot;W… $ use &lt;lgl&gt; FALSE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TR… $ Mode &lt;chr&gt; &quot;C-P&quot;, &quot;C&quot;, &quot;C&quot;, &quot;C-P&quot;, &quot;P-C&quot;, &quot;C&quot;, &quot;C&quot;, &quot;C&quot;, &quot;C&quot;, &quot;C&quot;, &quot;… $ EffType &lt;chr&gt; &quot;S&quot;, &quot;S&quot;, &quot;N&quot;, &quot;S&quot;, &quot;S&quot;, &quot;N&quot;, &quot;S&quot;, &quot;S&quot;, &quot;S&quot;, &quot;N&quot;, &quot;N&quot;, &quot;N… $ OnEffort &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRU… $ ESWsides &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, … $ dist &lt;dbl&gt; 0.1224221, 145.5464171, 38.3219637, 148.8739186, 151.6705… $ minutes &lt;dbl&gt; 0.400, 515.450, 136.800, 540.017, 558.767, 40.900, 507.20… $ n_rows &lt;int&gt; 82, 188, 75, 178, 169, 31, 197, 146, 189, 51, 54, 219, 16… $ min_line &lt;int&gt; 424496, 424498, 424734, 424892, 425505, 425680, 425789, 4… $ max_line &lt;int&gt; 427047, 424891, 424831, 425504, 425788, 425735, 426303, 4… $ year &lt;dbl&gt; 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 200… $ month &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, … $ day &lt;int&gt; 6, 6, 6, 7, 9, 10, 10, 11, 12, 12, 14, 14, 16, 16, 17, 18… $ lat1 &lt;dbl&gt; 21.998167, 21.998167, 21.669333, 20.541000, 19.628833, 21… $ lon1 &lt;dbl&gt; -159.1487, -159.1487, -158.0867, -157.5555, -156.2075, -1… $ DateTime1 &lt;dttm&gt; 2009-02-06 07:33:52, 2009-02-06 07:33:52, 2009-02-06 15:… $ timestamp1 &lt;dbl&gt; 1233905632, 1233905632, 1233932684, 1233999718, 123417162… $ yday1 &lt;dbl&gt; 37, 37, 37, 38, 40, 41, 41, 42, 43, 43, 45, 45, 47, 47, 4… $ lat2 &lt;dbl&gt; 19.193833, 20.541000, 21.591500, 19.621833, 21.143500, 21… $ lon2 &lt;dbl&gt; -156.0575, -157.5555, -157.8283, -156.1827, -158.0830, -1… $ DateTime2 &lt;dttm&gt; 2009-02-14 08:21:51, 2009-02-07 09:41:58, 2009-02-06 17:… $ timestamp2 &lt;dbl&gt; 1234599711, 1233999718, 1233942729, 1234171624, 123426333… $ yday2 &lt;dbl&gt; 45, 38, 37, 40, 41, 41, 42, 43, 45, 43, 45, 47, 47, 49, 4… $ mlat &lt;dbl&gt; 21.426500, 21.788500, 21.737833, 19.607500, 19.771167, 21… $ mlon &lt;dbl&gt; -158.9077, -158.5185, -157.9085, -156.1373, -156.8483, -1… $ mDateTime &lt;dttm&gt; 2009-02-10 17:18:39, 2009-02-06 12:30:29, 2009-02-06 16:… $ mtimestamp &lt;dbl&gt; 1234286319, 1233923429, 1233938494, 1234086957, 123418822… $ avgBft &lt;dbl&gt; NaN, 3.620949, 4.431633, 3.496502, 3.708160, 2.000000, 1.… $ avgSwellHght &lt;dbl&gt; NaN, 3.740710, 4.000000, 2.663336, 3.062949, 2.000000, 2.… $ avgHorizSun &lt;dbl&gt; NaN, 4.758324, 4.073185, 4.842962, 8.780615, 9.000000, 5.… $ avgVertSun &lt;dbl&gt; NaN, 2.062058, 1.349072, 1.577954, 1.426446, 2.688592, 1.… $ avgGlare &lt;dbl&gt; NaN, 0.54538502, 0.00000000, 0.11825495, 0.15454666, 0.00… $ avgVis &lt;dbl&gt; NaN, 6.750312, 6.176895, 6.483022, 5.771184, 6.500000, 6.… $ avgCourse &lt;dbl&gt; NaN, 106.2314, 113.4356, 208.8357, 288.5192, 205.4784, 21… $ avgSpdKt &lt;dbl&gt; NaN, 9.216194, 8.912595, 9.110399, 8.827567, 9.181559, 9.… # Number of segments cruz$cohorts$all$segments %&gt;% nrow [1] 1455 # Segment length distribution x &lt;- cruz$cohorts$all$segments$dist hist(x, breaks = seq(0,ceiling(max(x, na.rm=TRUE)),by=1), xlab=&#39;Segment lengths (km)&#39;, main=paste0(&#39;Target km: &#39;,settings$survey$segment_target_km)) And the das slot holds the original data.frame of DAS data, modified slightly: the column OnEffort has been modified according to Beaufort range conditions, and the column seg_id indicates which segment the event occurs within. cruz$cohorts$all$das %&gt;% names [1] &quot;Event&quot; &quot;DateTime&quot; &quot;Lat&quot; &quot;Lon&quot; [5] &quot;OnEffort&quot; &quot;Cruise&quot; &quot;Mode&quot; &quot;OffsetGMT&quot; [9] &quot;EffType&quot; &quot;ESWsides&quot; &quot;Course&quot; &quot;SpdKt&quot; [13] &quot;Bft&quot; &quot;SwellHght&quot; &quot;WindSpdKt&quot; &quot;RainFog&quot; [17] &quot;HorizSun&quot; &quot;VertSun&quot; &quot;Glare&quot; &quot;Vis&quot; [21] &quot;ObsL&quot; &quot;Rec&quot; &quot;ObsR&quot; &quot;ObsInd&quot; [25] &quot;Data1&quot; &quot;Data2&quot; &quot;Data3&quot; &quot;Data4&quot; [29] &quot;Data5&quot; &quot;Data6&quot; &quot;Data7&quot; &quot;Data8&quot; [33] &quot;Data9&quot; &quot;Data10&quot; &quot;Data11&quot; &quot;Data12&quot; [37] &quot;EffortDot&quot; &quot;EventNum&quot; &quot;file_das&quot; &quot;line_num&quot; [41] &quot;stratum_HI_EEZ&quot; &quot;stratum_OtherCNP&quot; &quot;stratum_WHICEAS&quot; &quot;year&quot; [45] &quot;month&quot; &quot;day&quot; &quot;yday&quot; &quot;km_valid&quot; [49] &quot;km_int&quot; &quot;km_cum&quot; &quot;ship&quot; &quot;stratum&quot; [53] &quot;use&quot; &quot;eff_bloc&quot; &quot;seg_id&quot; The segmentize() function and its associated settings were designed to give researchers full control over how data are segmented, be it for design-based density analysis (which tend to use long segments of 100 km or more and allow for non-contiguous effort to be included in the same segment) or for habitat modeling (which tend to use short segments of 5 - 10 km and disallow non-contiguous effort to be pooled into the same segment). To demonstrate that versatility, checkout the appendix on segmentizing. Process sightings To process sightings for each cohort of species, use the function process_sightings(). This function has three basic steps: for each cohort, the function (1) prepares a sightings table using the function das_sight() from swfscDAS; (2) filters those sightings to species codes specified for the cohort in your settings input; and (3) evaluates each of those sightings, asking if each should be included in the analysis according to your settings. cruz &lt;- process_sightings(cruz) The function produces a formatted dataset and adds it to a new sightings slot. cruz$cohorts$all %&gt;% names [1] &quot;segments&quot; &quot;das&quot; &quot;sightings&quot; cruz$cohorts$bottlenose %&gt;% names [1] &quot;segments&quot; &quot;das&quot; &quot;sightings&quot; cruz$cohorts$spotted %&gt;% names [1] &quot;segments&quot; &quot;das&quot; &quot;sightings&quot; Note that the sightings table has a column named included (TRUE = yes, use it in the analysis). Any sightings that do not meet the inclusion criteria as specified in your settings will be included = FALSE, but they won’t be removed from the data. The sightings table also has a new column, ss_valid, indicating whether or not the group size estimate for this sighting is valid and appropriate for use in abundance estimation and detection function fitting when group size is used as a covariate. Since the sightings in each cohort are processed slightly differently according to the cohort’s specific settings – most importantly the species that will be included – you should expect different numbers of included/excluded sightings in each cohort dataset: cruz$cohorts$all$sightings$included %&gt;% table . FALSE TRUE 806 3128 cruz$cohorts$bottlenose$sightings$included %&gt;% table . FALSE TRUE 113 410 When this function’s verbose argument is TRUE (the default), a message is printed each time a sighting does not meet the inclusion criteria. Sightings data structure The sightings table has many other variables: cruz$cohorts$all$sightings %&gt;% names [1] &quot;Event&quot; &quot;DateTime&quot; &quot;Lat&quot; &quot;Lon&quot; [5] &quot;OnEffort&quot; &quot;Cruise&quot; &quot;Mode&quot; &quot;OffsetGMT&quot; [9] &quot;EffType&quot; &quot;ESWsides&quot; &quot;Course&quot; &quot;SpdKt&quot; [13] &quot;Bft&quot; &quot;SwellHght&quot; &quot;WindSpdKt&quot; &quot;RainFog&quot; [17] &quot;HorizSun&quot; &quot;VertSun&quot; &quot;Glare&quot; &quot;Vis&quot; [21] &quot;ObsL&quot; &quot;Rec&quot; &quot;ObsR&quot; &quot;ObsInd&quot; [25] &quot;EffortDot&quot; &quot;EventNum&quot; &quot;file_das&quot; &quot;line_num&quot; [29] &quot;stratum_HI_EEZ&quot; &quot;stratum_OtherCNP&quot; &quot;stratum_WHICEAS&quot; &quot;year&quot; [33] &quot;month&quot; &quot;day&quot; &quot;yday&quot; &quot;km_valid&quot; [37] &quot;km_int&quot; &quot;km_cum&quot; &quot;ship&quot; &quot;stratum&quot; [41] &quot;use&quot; &quot;eff_bloc&quot; &quot;seg_id&quot; &quot;SightNo&quot; [45] &quot;Subgroup&quot; &quot;SightNoDaily&quot; &quot;Obs&quot; &quot;ObsStd&quot; [49] &quot;Bearing&quot; &quot;Reticle&quot; &quot;DistNm&quot; &quot;Cue&quot; [53] &quot;Method&quot; &quot;Photos&quot; &quot;Birds&quot; &quot;CalibSchool&quot; [57] &quot;PhotosAerial&quot; &quot;Biopsy&quot; &quot;CourseSchool&quot; &quot;TurtleSp&quot; [61] &quot;TurtleGs&quot; &quot;TurtleJFR&quot; &quot;TurtleAge&quot; &quot;TurtleCapt&quot; [65] &quot;PinnipedSp&quot; &quot;PinnipedGs&quot; &quot;BoatType&quot; &quot;BoatGs&quot; [69] &quot;PerpDistKm&quot; &quot;species&quot; &quot;best&quot; &quot;low&quot; [73] &quot;high&quot; &quot;prob&quot; &quot;mixed&quot; &quot;ss_tot&quot; [77] &quot;lnsstot&quot; &quot;ss_percent&quot; &quot;n_sp&quot; &quot;n_obs&quot; [81] &quot;n_best&quot; &quot;n_low&quot; &quot;n_high&quot; &quot;calibr&quot; [85] &quot;ss_valid&quot; &quot;mixed_max&quot; &quot;spp_max&quot; &quot;included&quot; Columns 42 onwards correspond to sightings information. Columns of note: species contains the species code. There is only one species-code per row (i.e, multi-species sightings have been expanded to multiple rows). best, low, and high contain the refined group size estimates, averaged across observers and calibrated according to the cohort’s settings specifications. For multi-species sightings, these numbers represent the number of individuals for the single species represented in the row (i.e., the original group size estimate has been scaled by the percentage attritbuted to this species). The columns following those group size estimates (prob through spp_max) detail how group sizes were estimated: prob indicates whether probable species codes were accepted; mixed indicates whether this species’ sighting is part of a mixed-species sighting; n_sp provides the number of species occurring in this sighitng; n_obs gives the number of observers who contributed group size estimates; n_best through n_high gives the number of valid group size estimates given; and calibr indicates whether or not calibration was attempted for this sighting based on the settings (see next section); mixed_max indicates whether this species was the most abundant in the sighting (if multi-species); spp_max indicates the species code for the most abundant species in the sighting (if multi-species). As explained above, the final column, included, indicates whether this species should be included in the analysis. Here is a glimpse of the data: cruz$cohorts$all$sightings %&gt;% glimpse Rows: 3,934 Columns: 88 $ Event &lt;chr&gt; &quot;S&quot;, &quot;S&quot;, &quot;S&quot;, &quot;S&quot;, &quot;S&quot;, &quot;S&quot;, &quot;S&quot;, &quot;S&quot;, &quot;S&quot;, &quot;S&quot;, &quot;S&quot;… $ DateTime &lt;dttm&gt; 2009-02-06 09:50:14, 2009-02-06 11:11:17, 2009-02-06… $ Lat &lt;dbl&gt; 21.93600, 21.83917, 21.78333, 21.70433, 21.69733, 21.… $ Lon &lt;dbl&gt; -158.8153, -158.6568, -158.4722, -158.2835, -158.2485… $ OnEffort &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE,… $ Cruise &lt;dbl&gt; 901, 901, 901, 901, 901, 901, 901, 901, 901, 901, 901… $ Mode &lt;chr&gt; &quot;C&quot;, &quot;C&quot;, &quot;C&quot;, &quot;C&quot;, &quot;C&quot;, &quot;C&quot;, &quot;C&quot;, &quot;C&quot;, &quot;C&quot;, &quot;C&quot;, &quot;C&quot;… $ OffsetGMT &lt;int&gt; 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 1… $ EffType &lt;chr&gt; &quot;S&quot;, &quot;S&quot;, &quot;S&quot;, &quot;S&quot;, &quot;S&quot;, &quot;S&quot;, &quot;S&quot;, &quot;N&quot;, &quot;N&quot;, &quot;N&quot;, &quot;N&quot;… $ ESWsides &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,… $ Course &lt;dbl&gt; 167, 108, 111, 106, 106, 102, 102, 39, 39, 39, 39, 94… $ SpdKt &lt;dbl&gt; 9.4, 8.6, 9.2, 10.4, 10.4, 10.2, 10.2, 9.1, 9.1, 9.1,… $ Bft &lt;dbl&gt; 4, 4, 3, 2, 2, 2, 2, 3, 3, 3, 4, 4, 5, 5, 5, 5, 4, 2,… $ SwellHght &lt;dbl&gt; 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 3, 1,… $ WindSpdKt &lt;dbl&gt; 15, 11, 6, 3, 3, 4, 4, 9, 9, 9, 15, 15, 15, 17, 17, 1… $ RainFog &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 5, 5, 5, 5, 3, 1, 5,… $ HorizSun &lt;dbl&gt; 1, 2, 3, 3, 3, 4, 4, 4, 4, 4, 4, 5, 5, 5, 3, 3, 12, N… $ VertSun &lt;dbl&gt; 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 3, NA… $ Glare &lt;lgl&gt; TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE… $ Vis &lt;dbl&gt; 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0… $ ObsL &lt;chr&gt; &quot;197&quot;, &quot;308&quot;, &quot;086&quot;, &quot;197&quot;, &quot;197&quot;, &quot;307&quot;, &quot;307&quot;, &quot;308… $ Rec &lt;chr&gt; &quot;309&quot;, &quot;307&quot;, &quot;238&quot;, &quot;309&quot;, &quot;309&quot;, &quot;197&quot;, &quot;197&quot;, &quot;307… $ ObsR &lt;chr&gt; &quot;086&quot;, &quot;197&quot;, &quot;308&quot;, &quot;086&quot;, &quot;086&quot;, &quot;309&quot;, &quot;309&quot;, &quot;197… $ ObsInd &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… $ EffortDot &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE,… $ EventNum &lt;chr&gt; &quot;027&quot;, &quot;061&quot;, &quot;129&quot;, &quot;168&quot;, &quot;176&quot;, &quot;188&quot;, &quot;191&quot;, &quot;204… $ file_das &lt;chr&gt; &quot;CenPac1986-2020_Final_alb_edited_example.das&quot;, &quot;CenP… $ line_num &lt;int&gt; 424551, 424585, 424655, 424696, 424704, 424716, 42471… $ stratum_HI_EEZ &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE,… $ stratum_OtherCNP &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE,… $ stratum_WHICEAS &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE,… $ year &lt;dbl&gt; 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009,… $ month &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,… $ day &lt;int&gt; 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 8,… $ yday &lt;dbl&gt; 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 3… $ km_valid &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE,… $ km_int &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… $ km_cum &lt;dbl&gt; 142323.0, 142344.5, 142366.8, 142388.5, 142392.2, 142… $ ship &lt;chr&gt; &quot;OES&quot;, &quot;OES&quot;, &quot;OES&quot;, &quot;OES&quot;, &quot;OES&quot;, &quot;OES&quot;, &quot;OES&quot;, &quot;OES… $ stratum &lt;chr&gt; &quot;WHICEAS&quot;, &quot;WHICEAS&quot;, &quot;WHICEAS&quot;, &quot;WHICEAS&quot;, &quot;WHICEAS&quot;… $ use &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE,… $ eff_bloc &lt;chr&gt; &quot;291-0&quot;, &quot;291-0&quot;, &quot;291-0&quot;, &quot;291-0&quot;, &quot;291-0&quot;, &quot;291-0&quot;,… $ seg_id &lt;dbl&gt; 2062, 2062, 2062, 2062, 2062, 2062, 2062, 2055, 2055,… $ SightNo &lt;chr&gt; &quot;001&quot;, &quot;002&quot;, &quot;003&quot;, &quot;004&quot;, &quot;005&quot;, &quot;006&quot;, &quot;007&quot;, &quot;008… $ Subgroup &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… $ SightNoDaily &lt;chr&gt; &quot;20090206_1&quot;, &quot;20090206_2&quot;, &quot;20090206_3&quot;, &quot;20090206_4… $ Obs &lt;chr&gt; &quot;086&quot;, &quot;197&quot;, &quot;308&quot;, &quot;086&quot;, &quot;086&quot;, &quot;309&quot;, &quot;307&quot;, &quot;308… $ ObsStd &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE,… $ Bearing &lt;dbl&gt; 78, 3, 23, 5, 57, 49, 331, 354, 358, 7, 260, 42, 12, … $ Reticle &lt;dbl&gt; 1.4, 0.3, 2.0, 1.4, 1.0, 2.0, 0.8, 8.0, 0.1, NA, NA, … $ DistNm &lt;dbl&gt; 2.17, 4.10, 1.76, 2.17, 2.58, 1.76, 2.87, 0.31, 5.27,… $ Cue &lt;dbl&gt; 2, 6, 6, 3, 6, 6, 6, 6, 6, 6, 3, 6, 3, 6, 6, 6, 6, 6,… $ Method &lt;dbl&gt; 4, 4, 4, 4, 4, 4, 4, 4, 4, 1, 1, 4, 4, 4, 2, 4, 4, 4,… $ Photos &lt;chr&gt; &quot;N&quot;, &quot;N&quot;, &quot;N&quot;, &quot;N&quot;, &quot;N&quot;, &quot;N&quot;, &quot;N&quot;, &quot;N&quot;, &quot;N&quot;, &quot;N&quot;, &quot;N&quot;… $ Birds &lt;chr&gt; &quot;N&quot;, &quot;N&quot;, &quot;N&quot;, &quot;N&quot;, &quot;N&quot;, &quot;N&quot;, &quot;N&quot;, &quot;N&quot;, &quot;N&quot;, &quot;N&quot;, &quot;N&quot;… $ CalibSchool &lt;chr&gt; &quot;N&quot;, &quot;N&quot;, &quot;N&quot;, &quot;N&quot;, &quot;N&quot;, &quot;N&quot;, &quot;N&quot;, &quot;N&quot;, &quot;N&quot;, &quot;N&quot;, &quot;N&quot;… $ PhotosAerial &lt;chr&gt; &quot;N&quot;, &quot;N&quot;, &quot;N&quot;, &quot;N&quot;, &quot;N&quot;, &quot;N&quot;, &quot;N&quot;, &quot;N&quot;, &quot;N&quot;, &quot;N&quot;, &quot;N&quot;… $ Biopsy &lt;chr&gt; &quot;Y&quot;, &quot;N&quot;, &quot;N&quot;, &quot;N&quot;, &quot;N&quot;, &quot;N&quot;, &quot;N&quot;, &quot;N&quot;, &quot;N&quot;, &quot;N&quot;, &quot;N&quot;… $ CourseSchool &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… $ TurtleSp &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… $ TurtleGs &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… $ TurtleJFR &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… $ TurtleAge &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… $ TurtleCapt &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… $ PinnipedSp &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… $ PinnipedGs &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… $ BoatType &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… $ BoatGs &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… $ PerpDistKm &lt;dbl&gt; 3.9310187037330472925589, 0.3973973829439210736503, 1… $ species &lt;chr&gt; &quot;076&quot;, &quot;076&quot;, &quot;076&quot;, &quot;076&quot;, &quot;076&quot;, &quot;076&quot;, &quot;076&quot;, &quot;076… $ best &lt;dbl&gt; 1.000000, 2.057008, 1.030011, 1.986117, 1.000000, 1.1… $ low &lt;dbl&gt; 1.000000, 2.000000, 1.000000, 2.000000, 1.000000, 1.0… $ high &lt;dbl&gt; 1.000000, 3.000000, 1.246355, 2.000000, 2.000000, 1.0… $ prob &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALS… $ mixed &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALS… $ ss_tot &lt;dbl&gt; 1.000000, 2.057008, 1.030011, 1.986117, 1.000000, 1.1… $ lnsstot &lt;dbl&gt; 0.00000000, 0.72125271, 0.02956944, 0.68618171, 0.000… $ ss_percent &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,… $ n_sp &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,… $ n_obs &lt;int&gt; 1, 3, 3, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 2, 2,… $ n_best &lt;int&gt; 1, 3, 3, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 2, 2,… $ n_low &lt;int&gt; 1, 3, 3, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 2, 2,… $ n_high &lt;int&gt; 1, 3, 3, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 2, 2,… $ calibr &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE,… $ ss_valid &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE,… $ mixed_max &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE,… $ spp_max &lt;chr&gt; &quot;076&quot;, &quot;076&quot;, &quot;076&quot;, &quot;076&quot;, &quot;076&quot;, &quot;076&quot;, &quot;076&quot;, &quot;076… $ included &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE,… Note that the process_sightings() function draws upon cruz$settings for inclusion criteria, but some of those settings can be overridden with the function’s manual inputs if you want to explore your options (see below). Group size estimates In the settings we are using in this tutorial, group size estimates are adjusted using the calibration models from Barlow et al. (1998) (their analysis is refined slightly and further explained in Gerrodette et al. (2002)). These calibration corrections are observer-specific. Most observers tend to underestimate group size and their estimates are adjusted up; others tend to overestimate and their estimates are adjusted down. Some observers do not have calibration coefficients, and for them a generic adjustment (upwards, by dividing estimates by 0.8625) is used. In LTabundR, each observer’s estimate is calibrated, then all observer estimates are averaged. To do that averaging, our settings specify that we shall use a geometric weighted mean, instead of an arithmetic mean, that weights school size estimates from multiple observers according to the variance of their calibration coefficients. Here are our current best estimates of group size: cruz$cohorts$all$sightings$best %&gt;% head(20) [1] 1.000000 2.057008 1.030011 1.986117 1.000000 1.159420 [7] 2.318841 2.318841 3.666409 7.513902 2.783748 9.275362 [13] 3.478261 1.159420 1.986117 1.159420 1.000000 3.279336 [19] 8.115942 393.320601 Let’s compare those estimates to unadjusted ones, in which calibration (and therefore weighted geometric mean) is turned off: cruz_demo &lt;- process_sightings(cruz, calibrate = FALSE, verbose = FALSE) cruz_demo$cohorts$all$sightings$best %&gt;% head(20) [1] 1.000000 2.000000 1.000000 2.000000 1.000000 1.000000 [7] 2.000000 2.000000 3.162278 6.480741 3.000000 8.000000 [13] 3.000000 1.000000 2.000000 1.000000 1.000000 2.828427 [19] 7.000000 234.627970 You can also carry out calibration corrections without using a geometric weighted mean (the arithmetic mean will be used instead): cruz_demo &lt;- process_sightings(cruz, calibrate = TRUE, geometric_mean = FALSE, verbose = FALSE) cruz_demo$cohorts$all$sightings$best %&gt;% head(20) [1] 1.000000 2.166746 1.053140 1.986117 1.000000 1.159420 [7] 2.318841 2.318841 4.057971 7.536232 2.783748 9.275362 [13] 3.478261 1.159420 1.986117 1.159420 1.000000 3.478261 [19] 8.115942 465.411665 Note that when geometric_mean = TRUE but calibration is not carried out, the simple geometric mean is calculated instead of the weighted geometric mean, since the weights are the variance estimates from the calibration routine. Also note that group size calibration is only carried out if settings$group_size_calibration is not NULL. However, even when calibration coefficients are provided, it is possible to specify that calibration should only be carried out for raw estimates above a minimum threshold (see cohort setting calibration_floor, whose default is 0), since observers may be unlikely to mis-estimate the group size of a lone whale or pair. For observers who have calibration coefficients in the settings$group_size_coefficients table, that minimum is specified for each observer individually. For observers not in that table, calibration will only be applied to raw group size estimates above settings$cohorts[[i]]$calibration_floor or above. Process subgroups After sightings data are processed, the process_surveys() function calls the subroutine process_subgroups() to find and calculate subgroup group size estimates for false killer whales (or other species that may have been recorded using the subgroup functionality in WinCruz), if any occur in the DAS data (Event code “G”). cruz &lt;- process_subgroups(cruz) If subgroups are found, a subgroups slot is added to the analysis list for a cohort. cruz$cohorts$all %&gt;% names [1] &quot;segments&quot; &quot;das&quot; &quot;sightings&quot; &quot;subgroups&quot; This subgroups slot holds a list with three dataframes: cruz$cohorts$all$subgroups %&gt;% names [1] &quot;sightings&quot; &quot;subgroups&quot; &quot;events&quot; $events (each row is a group size estimate for a single subgroup during a single phase of the false killer whale protocol (if applicable) within a single sighting; this is effectively the raw data). Internally, LTabundR uses the function subgroup_events() to produce this data.frame. $subgroups (each row is a single subgroup for a single protocol phase, with all group size estimates averaged together (both arithmetically and geometrically). Internally, LTabundR uses the function subgroup_subgroups() to produce this data.frame. $sightings (each row is a “group” size estimate for a single sighting during a single phase protocol, with all subgroup group sizes summed together). Note for false killer whales this “group” size estimate is not likely to represent actual group size because groups can be spread out over tens of kilometers, and it is not expected that every subgroup is detected during each protocol phase. Internally, LTabundR uses the function subgroup_sightings() to produce this data.frame. For a detailed example of how subgroup data are analyzed, see the vignette page on subgroup analysis using data on Central North Pacific false killer whales. Editing subgroup details The subgroup protocol involves a few components that may not be adequately included in the raw DAS data, such as the assignment of subgroup detections to certain “phases” of the protocol or certain populations (or multiple populations). These details are described in detail on the vignette page on subgroup analysis, as is the process for manually updating the processed cruz object with those details. LTabundR includes several functions that facilitate this work. Review By the end of this process, you have a single data object, cruz, with all the data you need to move forward into the next stages of mapping and analysis. The LTabundR function cruz_structure() provides a synopsis of the data structure: cruz_structure(cruz) &quot;cruz&quot; list structure ======================== $settings $strata --- with 11 polygon coordinate sets $survey --- with 15 input arguments $cohorts --- with 3 cohorts specified, each with 19 input arguments $strata ... containing a summary dataframe of 11 geostrata and their spatial areas ... geostratum names: HI_EEZ, OtherCNP, MHI, WHICEAS, Spotted_OU, Spotted_FI, Spotted_BI, Bottlenose_KaNi, Bottlenose_OUFI, Bottlenose_BI, NWHI $cohorts $all geostrata: WHICEAS, HI_EEZ, OtherCNP $segments --- with 1457 segments (median = 149.5 km) $das --- with 329638 data rows $sightings --- with 3934 detections $subgroups --- with 255 subgroups, 61 sightings, and 389 events $bottlenose geostrata: WHICEAS, HI_EEZ, OtherCNP, Bottlenose_BI, Bottlenose_OUFI, Bottlenose_KaNi $segments --- with 1538 segments (median = 149.2 km) $das --- with 329638 data rows $sightings --- with 523 detections $spotted geostrata: WHICEAS, HI_EEZ, OtherCNP, Spotted_OU, Spotted_FI, Spotted_BI $segments --- with 1540 segments (median = 149.1 km) $das --- with 329638 data rows $sightings --- with 527 detections Each species-specific cohort has its own list under cruz$cohorts, and each of these cohorts has the same list structure: segments is a summary table of segments. das is the raw DAS data, modified with seg_id to associate each row with a segment. sightings is a dataframe of sightings processed according to this cohort’s settings. subgroups (if any subgroup data exist in your survey) is a list with subgroup details. In each cohort data.frame, there are three critically important columns to keep in mind: seg_id: this column is used to indicate the segment ID that a row of data belongs to. use: this column indicates whether a row of effort should be used in the line-transect analysis. Every row of data within a single segment will have the same use value. included: this column occurs in the sightings dataframe only. It indicates whether the sightings should be included in line-transect analysis based on the specified settings. Any sighting with use == FALSE will also have included == FALSE, but it is possible for sightings to have use == TRUE with included == FALSE. For example, if the setting abeam_sightings is set to FALSE, a sighting with a bearing angle beyond the ship’s beam can be excluded from the analysis (included == FALSE) even though the effort segment it occurs within will still be used (use == TRUE). Finally, let’s save this cruz object locally, to use in downstream scripts: save(cruz, file=&#39;whiceas_cruz.RData&#39;) "],["filter.html", "4 Filter &amp; combine surveys Filter Combine", " 4 Filter &amp; combine surveys You may soon encounter the need to filter a processed cruz object to only certain years, regions, or cruise numbers. You may also need to combine one processed cruz object with another. The function below assist with these tasks. Here we will continue with the cruz object we created on the previous page. As a reminder, here is the data structure of that object: cruz_structure(cruz) &quot;cruz&quot; list structure ======================== $settings $strata --- with 11 polygon coordinate sets $survey --- with 15 input arguments $cohorts --- with 3 cohorts specified, each with 19 input arguments $strata ... containing a summary dataframe of 11 geostrata and their spatial areas ... geostratum names: HI_EEZ, OtherCNP, MHI, WHICEAS, Spotted_OU, Spotted_FI, Spotted_BI, Bottlenose_KaNi, Bottlenose_OUFI, Bottlenose_BI, NWHI $cohorts $all geostrata: WHICEAS, HI_EEZ, OtherCNP $segments --- with 1457 segments (median = 149.5 km) $das --- with 329638 data rows $sightings --- with 3934 detections $subgroups --- with 255 subgroups, 61 sightings, and 389 events $bottlenose geostrata: WHICEAS, HI_EEZ, OtherCNP, Bottlenose_BI, Bottlenose_OUFI, Bottlenose_KaNi $segments --- with 1538 segments (median = 149.2 km) $das --- with 329638 data rows $sightings --- with 523 detections $spotted geostrata: WHICEAS, HI_EEZ, OtherCNP, Spotted_OU, Spotted_FI, Spotted_BI $segments --- with 1540 segments (median = 149.1 km) $das --- with 329638 data rows $sightings --- with 527 detections Filter LTabundR lets you filter a cruz object using the function filter_cruz(). For example, in our WHICEAS case study, we processed surveys from 1986 - 2020, which we needed to do to model our detection functions, but our interest for mapping is specifically valid effort in 2017 and 2020 only, and only within the \"WHICEAS\" geostratum. cruz_1720 &lt;- filter_cruz(cruz, analysis_only = TRUE, years = c(2017, 2020), regions = &#39;WHICEAS&#39;) We will use this filtered cruz object for mapping &amp; sightings summaries downstream. save(cruz_1720,file=&#39;whiceas_cruz_1720.RData&#39;) Note that filter_cruz() has many other filter options. See ?filter_cruz() for details. Combine Say you have two processed cruz objects: one containing survey effort from the Hawaiian EEZ (HI_EEZ) geostratum area only, and one containing survey effort from everywhere else that does not include HI_EEZ effort. Let’s make those fake datasets right now, using filter_cruz(): Hawaiian EEZ-only data: cruz_hi &lt;- filter_cruz(cruz, regions = &#39;HI_EEZ&#39;, verbose = FALSE) Outside Hawaiian EEZ-only data: cruz_other &lt;- filter_cruz(cruz, not_regions = &#39;HI_EEZ&#39;, verbose = FALSE) Say you want to combine these datasets together in order to reconstruct the equivalent of our original cruz object. You can do this with the LTabundR function, cruz_combine(). # Make a list of cruz objects cruzes &lt;- list(cruz_hi, cruz_other) # Now combine cruz_demo &lt;- cruz_combine(cruzes) Re-constituted data structure: cruz_structure(cruz_demo) &quot;cruz&quot; list structure ======================== $settings $strata --- with 11 polygon coordinate sets $survey --- with 15 input arguments $cohorts --- with 3 cohorts specified, each with 19 input arguments $strata ... containing a summary dataframe of 11 geostrata and their spatial areas ... geostratum names: HI_EEZ, OtherCNP, MHI, WHICEAS, Spotted_OU, Spotted_FI, Spotted_BI, Bottlenose_KaNi, Bottlenose_OUFI, Bottlenose_BI, NWHI $cohorts $all geostrata: WHICEAS, HI_EEZ, OtherCNP $segments --- with 1429 segments (median = 149.6 km) $das --- with 233421 data rows $sightings --- with 3097 detections $subgroups --- with 250 subgroups, 60 sightings, and 381 events $bottlenose geostrata: WHICEAS, HI_EEZ, OtherCNP, Bottlenose_BI, Bottlenose_OUFI, Bottlenose_KaNi $segments --- with 1512 segments (median = 149.3 km) $das --- with 233421 data rows $sightings --- with 504 detections $spotted geostrata: WHICEAS, HI_EEZ, OtherCNP, Spotted_OU, Spotted_FI, Spotted_BI $segments --- with 1513 segments (median = 149.2 km) $das --- with 233423 data rows $sightings --- with 512 detections "],["summarize.html", "5 Summarize survey Summarize effort Summarize by Beaufort Summarize sightings Summarize certain species cruz_explorer()", " 5 Summarize survey Here we will summarize the 2017 &amp; 2020 survey data we prepared on the previous page. load(&#39;whiceas_cruz_1720.RData&#39;) Summarize effort The summarize_effort() functions builds tables with total kilometers and days surveyed. effort &lt;- summarize_effort(cruz_1720, cohort=1) This function summarizes effort in several default tables: effort %&gt;% names() [1] &quot;total&quot; &quot;total_by_cruise&quot; &quot;total_by_year&quot; &quot;total_by_effort&quot; [5] &quot;total_by_stratum&quot; Total surveyed The slot $total provides the grand total distance and unique dates surveyed: library(DT) effort$total %&gt;% DT::datatable(options=list(initComplete = htmlwidgets::JS( &quot;function(settings, json) {$(this.api().table().container()).css({&#39;font-size&#39;: &#39;9pt&#39;});}&quot;) )) Total surveyed by effort The slot $total_by_effort provides the total distance and days surveyed, grouped by segments that will be included in the analysis and those that won’t: Total surveyed by stratum The slot $total_by_stratum provides the total distance and days surveyed within each stratum, again grouped by segments that will be included in the analysis and those that won’t: Summarize by Beaufort bft &lt;- summarize_bft(cruz_1720, cohort=1) This function summarizes effort by Beaufort in four default tables: bft %&gt;% names() [1] &quot;overall&quot; &quot;by_year&quot; &quot;by_stratum&quot; &quot;details&quot; Simple overall breakdown The slot $overall provides the total effort – and proportion of effort – occurring in each Beaufort state: Breakdown by year The slot $by_year provides the above for each year separately: Breakdown by stratum The slot $by_stratum provides the above for each geostratum separately: Detailed breakdown The slot $details provides the above for each cruise-year-study area-geostratum combination within the data: 5.0.1 Default filtering By default, this function only summarizes effort that can be used for detection function model fitting, (i.e., where the column use is TRUE). To turn off this filter, you can change the input use_only to FALSE: summarize_bft(cruz_1720, use_only = FALSE)$overall # A tibble: 6 × 3 bftr km prop &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 1 217. 0.0184 2 2 851. 0.0723 3 3 1360. 0.116 4 4 3601. 0.306 5 5 3843. 0.327 6 6 1894. 0.161 Summarize sightings The summarize_sightings() function builds tables summarizing the sightings within each cohort-analysis. (Eventually, we may want to include an option to merge all sightings from all cohort-analyses into a single table.) sightings &lt;- summarize_sightings(cruz_1720, cohort=1) This function summarizes sightings in four default tables: sightings %&gt;% names() [1] &quot;simple_totals&quot; &quot;analysis_totals&quot; [3] &quot;stratum_simple_totals&quot; &quot;stratum_analysis_totals&quot; Simple species totals The slot $simple_totals includes all sightings, even if they will not be included in analysis (i.e., even if the include column is FALSE): Analysis totals The slot $analysis_totals only includes sightings that meet all inclusion criteria for the analysis: Simple totals for each stratum The slot $stratum_simple_totals splits the first table (simple species totals) so that sightings are tallied for each geo-stratum separately: Analysis totals for each stratum The slot $stratum_analysis_totals splits the second table (analysis totals for each species) so that sightings are tallied for each geo-stratum separately: Summarize certain species To deep-dive into details for a ceratin species (or group of species), use the function summarize_species(). species &lt;- summarize_species(spp=&#39;046&#39;, cruz_1720) This functions a list with a variety of summaries: species %&gt;% names [1] &quot;species&quot; &quot;n_total&quot; &quot;n_analysis&quot; [4] &quot;school_size&quot; &quot;yearly_total&quot; &quot;yearly_analysis&quot; [7] &quot;regional_total&quot; &quot;regional_analysis&quot; &quot;detection_distances&quot; [10] &quot;sightings&quot; The slots $n_total and $n_analysis provide the total number of sightings and the number eligible for inclusion in the analysis: species$n_total [1] 14 species$n_analysis [1] 14 School size details This table only includes the sightings eligible for analysis: Annual summaries (all sightings) Annual summaries (analysis only) Regional summaries (all sightings) Regional summaries (analysis only) Detection distances This table can be used to determine the best truncation distance to use, based on the percent truncation you wish and the number of sightings available at each option. All sightings data Finally, this last slot holds a dataframe of all sightings data for the specified species: cruz_explorer() Note that all of these summary tables can be viewed interactively using the function cruz_explorer(), which allows you to efficiently subset the data according to various filters. cruz_explorer(cruz_1720) "],["maps.html", "6 Maps Creating a base map Add stratum boundaries Add effort tracklines Add sightings Other base maps Interactive maps Interactive dashboard", " 6 Maps To facilitate mapping with the processed data objects produced by LTabundR, we provide the following datasets and functions. These features are designed specifically for creating maps in R using the packages ggplot2 and sf. Creating a base map All of the mapping functions in LTabundR are designed to be added onto a pre-existing map that you have already created with ggplot2, which itself has several functions that tie into the spatial functions in the package sf. There are already several great resources out there for creating maps with ggplot and sf. (Here is one example.). library(ggplot2) library(sf) library(LTabundR) Here we provide a couple simple examples to demonstrate some of the spatial datasets that come with LTabundR. To begin, decide upon the CRS projection you wish to use for your map. (This website may be helpful.). my_crs &lt;- 4326 It will also be helpful to create objects for your desired geographic range, since you might need to refer to them multiple times: xlims &lt;- c(-162, -153) ylims &lt;- c(17, 24.2) To create a simple map of the main Hawaiian islands, for example, you can use the LTabundR datasets named \"land\" and \"coastline\", which are global datasets downloaded using the rnaturalearth package, and the dataset \"eez_hawaii, all of which have been provided as sf objects: # Load dataset data(&#39;land&#39;, package=&#39;LTabundR&#39;) data(&#39;coastline&#39;, package=&#39;LTabundR&#39;) data(&#39;eez_hawaii&#39;, package=&#39;LTabundR&#39;) # Look at an example of structure coastline %&gt;% head Simple feature collection with 6 features and 3 fields Geometry type: LINESTRING Dimension: XY Bounding box: xmin: -180 ymin: -85.22194 xmax: 166.2263 ymax: 3.492087 Geodetic CRS: WGS 84 featurecla scalerank min_zoom geometry 1 Coastline 0 0 LINESTRING (59.91603 -67.40... 2 Coastline 0 0 LINESTRING (-51.73062 -82.0... 3 Coastline 6 5 LINESTRING (166.137 -50.864... 4 Coastline 0 0 LINESTRING (-56.66832 -36.7... 5 Coastline 0 0 LINESTRING (-51.07939 3.492... 6 Coastline 6 5 LINESTRING (-80.75935 -33.7... # Specify CRS st_crs(coastline) &lt;- my_crs st_crs(land) &lt;- my_crs st_crs(eez_hawaii) &lt;- my_crs Now build a simple map, specifying the geographic range: p &lt;- ggplot() + geom_sf(data=land, fill=&#39;darkslategrey&#39;, color=NA, alpha=.6) + geom_sf(data=coastline, color=&#39;darkslategrey&#39;, alpha=1, lwd=.2) + geom_sf(data=eez_hawaii, color=&#39;grey50&#39;, alpha=.5, lwd=.4) + coord_sf(xlim=xlims, ylim=ylims) p Remove some of the default features: p &lt;- p + xlab(NULL) + ylab(NULL) + theme_light() + theme(strip.background = element_rect(fill = &quot;grey45&quot;), panel.grid.major = element_blank(), panel.grid.minor = element_blank()) p Make it a bit fancier, using some functions from the package ggspatial: p &lt;- p + ggspatial::annotation_north_arrow(location = &quot;bl&quot;, which_north = &quot;true&quot;, height = unit(1, &#39;cm&#39;), width = unit(1, &#39;cm&#39;), style = ggspatial::north_arrow_fancy_orienteering) + ggspatial::annotation_scale(location=&#39;br&#39;) p Now this base map is ready for some actual survey data! We can use the built-in dataset for Pacific surveys, 1986 - 2020: data(&quot;noaa_10km_1986_2020&quot;, package=&#39;LTabundR&#39;) cruz &lt;- noaa_10km_1986_2020 Add stratum boundaries Use the function gg_geostratum() to add geostratum boundaries from your cruz object to your map: gg_geostratum(p, cruz, strata = c(&#39;MHI&#39;, &#39;WHICEAS&#39;)) + # force limits to stay unchanged coord_sf(xlim=xlims, ylim=ylims) Here that function is again, demonstrating some of the optional inputs: gg_geostratum(p, cruz, strata = c(&#39;MHI&#39;, &#39;WHICEAS&#39;), color = c(&#39;darkorange&#39;, &#39;darkorchid&#39;), lty = 3, lwd = .5) + # force limits to stay unchanged coord_sf(xlim=xlims, ylim=ylims) Add effort tracklines Before adding survey data, let’s filter down to just the WHICEAS cruises from 2017 to 2020: cruz_1720 &lt;- filter_cruz(cruz, eff_types = &#39;S&#39;, bft_range = 0:6, years = c(2017, 2020), regions = &#39;WHICEAS&#39;) Now use the function gg_effort() to add effort tracklines from your cruz object to your map: gg_effort(p, cruz_1720) To color code by Beaufort sea state: gg_effort(p, cruz_1720, color_by_bft = TRUE, alpha=.3) Add faceting (you can use any column from the processed DAS data in subsequent ggplot2 commands: gg_effort(p, cruz_1720, color_by_bft = TRUE, alpha=.3) + facet_wrap(~year) Add sightings Use the function gg_sightings() to add sightings from your cruz object to your map: gg_sightings(p, cruz_1720, cex=2) To plot detections of certain species only: gg_sightings(p, cruz_1720, spp = &#39;076&#39;, color=&#39;firebrick&#39;, cex=2) To color-code detections by species: gg_sightings(p, cruz_1720, spp = c(&#39;076&#39;, &#39;046&#39;), color_by_spp = TRUE, cex = 2, alpha = 1, pch = 15) To translate species codes to full common names, you can supply a data.frame with the necessary translation information. LTabundR comes with a built-in dataset for Pacific species, \"species_codes\". data(&#39;species_codes&#39;, package=&#39;LTabundR&#39;) gg_sightings(p, cruz_1720, spp = c(&#39;076&#39;, &#39;046&#39;), spp_translate = species_codes, color_by_spp = TRUE, alpha = .8, cex = 2) Other base maps Hawaiian EEZ The Hawaii EEZ is tricky to map because it crosses the International Date Line (IDL). To facilitate maps of this region, LTabundR includes a function, ggplot_idl(), to which you can pass a ggplot2 map object. # Load datasets data(&#39;land&#39;, package=&#39;LTabundR&#39;) data(&#39;coastline&#39;, package=&#39;LTabundR&#39;) data(&#39;eez_hawaii&#39;, package=&#39;LTabundR&#39;) # Specify CRS my_crs &lt;- 4326 st_crs(coastline) &lt;- my_crs st_crs(land) &lt;- my_crs st_crs(eez_hawaii) &lt;- my_crs # Make map p &lt;- ggplot() + geom_sf(data=land, fill=&#39;darkslategrey&#39;, color=NA, alpha=.6) + geom_sf(data=coastline, color=&#39;darkslategrey&#39;, alpha=1, lwd=.2) + geom_sf(data=eez_hawaii, color=&#39;grey50&#39;, alpha=.5, lwd=.4) + xlab(NULL) + ylab(NULL) + theme_light() + theme(strip.background = element_rect(fill = &quot;grey45&quot;), panel.grid.major = element_blank(), panel.grid.minor = element_blank()) # Handle IDL ggplot_idl(p, lon_range=c(175, -150), lat_range = c(14.8, 33)) California Current System my_crs &lt;- 4326 xlims &lt;- c(-130, -116) ylims &lt;- c(30, 49) # Load dataset data(&#39;land&#39;, package=&#39;LTabundR&#39;) data(&#39;coastline&#39;, package=&#39;LTabundR&#39;) data(&#39;eez_ccs&#39;, package=&#39;LTabundR&#39;) # Specify CRS st_crs(coastline) &lt;- my_crs st_crs(land) &lt;- my_crs st_crs(eez_ccs) &lt;- my_crs ggplot() + geom_sf(data=land, fill=&#39;darkslategrey&#39;, color=NA, alpha=.6) + geom_sf(data=coastline, color=&#39;darkslategrey&#39;, alpha=1, lwd=.2) + geom_sf(data=eez_ccs, color=&#39;grey50&#39;, alpha=.5, lwd=.4) + coord_sf(xlim=xlims, ylim=ylims) + xlab(NULL) + ylab(NULL) + theme_light() + theme(strip.background = element_rect(fill = &quot;grey45&quot;), panel.grid.major = element_blank(), panel.grid.minor = element_blank()) + ggspatial::annotation_north_arrow(location = &quot;bl&quot;, which_north = &quot;true&quot;, height = unit(1, &#39;cm&#39;), width = unit(1, &#39;cm&#39;), style = ggspatial::north_arrow_fancy_orienteering) + ggspatial::annotation_scale(location=&#39;br&#39;) Eastern tropical Pacific The built-in LTabundR dataset data(eez) has all EEZ boundaries for US territories. my_crs &lt;- 4326 xlims &lt;- c(-175, -60) ylims &lt;- c(-20, 49) # Load dataset data(&#39;land&#39;, package=&#39;LTabundR&#39;) data(&#39;coastline&#39;, package=&#39;LTabundR&#39;) data(&#39;eez&#39;, package=&#39;LTabundR&#39;) # Specify CRS st_crs(coastline) &lt;- my_crs st_crs(land) &lt;- my_crs st_crs(eez) &lt;- my_crs ggplot() + geom_sf(data=land, fill=&#39;darkslategrey&#39;, color=NA, alpha=.6) + geom_sf(data=coastline, color=&#39;darkslategrey&#39;, alpha=1, lwd=.2) + geom_sf(data=eez, color=&#39;grey50&#39;, alpha=.5, lwd=.4) + coord_sf(xlim=xlims, ylim=ylims) + xlab(NULL) + ylab(NULL) + theme_light() + theme(strip.background = element_rect(fill = &quot;grey45&quot;), panel.grid.major = element_blank(), panel.grid.minor = element_blank()) + ggspatial::annotation_north_arrow(location = &quot;bl&quot;, which_north = &quot;true&quot;, height = unit(1, &#39;cm&#39;), width = unit(1, &#39;cm&#39;), style = ggspatial::north_arrow_fancy_orienteering) + ggspatial::annotation_scale(location=&#39;br&#39;) Interactive maps LTabundR also has an interactive map function, which maps survey data using the leaflet package. map_cruz(cruz_1720, cohort=1, eez_show=FALSE, strata_show=FALSE, effort_show=TRUE, sightings_show=TRUE, sightings_color = &#39;firebrick&#39;, verbose=FALSE) Note that you can also click on sightings and tracklines to see their details. Refer to the documentation for this function (?map_cruz) to see all the options available for stylizing these maps. Interactive dashboard Finally, note that LTabundR comes with an interactive data explorer app (a Shiny app) for filtering survey data according to effort scenario and species code, toggling map_cruz() settings, and reviewing summary tables of effort and sightings (including inspection of truncation distances). cruz_explorer(cruz_1720) Screenshots from this app:           "],["g0.html", "7 Estimating g(0) Relative g(0) Weighted average g(0)", " 7 Estimating g(0) Detection function models assume g(0) is 1.0. In distance sampling, a “detection function” is fit to the perpendicular sighting distances to reflect the fact that animals farther out are more difficult to detect. That detection function is a model of how the probability of detection declines with increasing distance from your survey trackline. The equations for detection function models are all constructed to assume that the probability of detecting an animal on your trackline (distance = 0 km) is 1.0 – you never miss an animal on your trackline. This trackline detection probability is referred to as g(0). In reality, though, it almost never is – and it greatly impacts results. When searching for marine mammals at sea, even some of those occurring directly on your survey trackline will be missed. Real g(0) is actually less than 1.0. This technicality makes a big difference: if g(0) is actually 0.5, the assumption that g(0) is 1.0 will underestimate animal abundance by 50%. Some animals, such as pygmy and dwarf sperm whales (Kogia spp.), are very cryptic deep divers and easily missed or unavailable, and thus likely have a true g(0) below 0.1. This means that estimates assuming their g(0) is still 1.0 will underestimate true abundance by 90%! Moreover, all species – whether you are a pygmy sperm whale or a blue whale – become easier to miss when sighting conditions deteriorate (e.g., Beaufort sea states 4 - 6). Therefore, g(0) must be estimated and then used to scale the detection function. So g(0) matters, and the assumption of detection functions that g(0) = 1.0 is nearly always wrong. Luckily, there is a way to handle this that avoids constructing new detection function equations or estimating even more parameters during the detection function fitting process: we use an estimate of g(0) to scale the detection function. Say the detection function predicts that the probability of detection is 1.0, 0.8, and 0.6 at distances 0 km, 1 km, and 2km, respectively. If g(0) is actually 0.5, then we can scale the detection function so that those respective predictions are now 0.5, 0.4, and 0.3. Estimating g(0) for a NOAA Fisheries line-transect cetacean survey generally involves four steps (Barlow 2015): First, you estimate g(0) in “perfect” conditions (e.g., Beaufort sea state 0) to account for animal availability. Second, you scale that estimate downward to approximate g(0) when conditions are less than ideal (i.e., a separate g(0) estimate for each Beaufort sea state from 1 to 6). This is known as the relative trackline detection probability, or relative g(0), or most simply: Rg(0). Third, you determine the weighted average value of g(0) for your particular survey, based on the proportional distribution of effort in each sea state. You pass this single value to your line-transect analysis functions. Fourth, you need to determine the CV of your weighted estimate of g(0). This isn’t straightforward, because you first need to simulate a new distribution for the weighted g(0) estimate, from which you then calculate the CV. The first of these steps is typically the biggest lift analytically, and very few studies provide absolute estimates of g(0) for their species of interest. Doing so involves simulation modeling and specialized field methods (e.g., Barlow 1999). Instead, most NOAA Fisheries studies assume that the absolute g(0) is in fact 1.0 – even though it’s not – and proceed directly to the second step – relative trackline detection probability, Rg(0). This is more common because it can be estimated directly from the survey data, thanks to an approach developed in Barlow (2015): “Inferring trackline detection probabilities, g(0), for cetaceans from apparent densities in different survey conditions”. Relative g(0) Archived Rg(0) estimates As of 2022, most eastern and central Pacific NOAA Fisheries studies use the Rg(0) estimates from Barlow (2015), which are based on NOAA Fisheries cruises in the eastern and central Pacific from 1986 to 2010. LTabundR includes Barlow’s results in a built-in dataset. data(barlow_2015) Here is the top of this dataset, in which each row is a Rg(0) estimate for a single species - Beaufort sea state scenario. barlow_2015 %&gt;% head(12) title scientific spp truncation 1 Delphinus spp Delphinus 005-016-017 5.5 2 Delphinus spp Delphinus 005-016-017 5.5 3 Delphinus spp Delphinus 005-016-017 5.5 4 Delphinus spp Delphinus 005-016-017 5.5 5 Delphinus spp Delphinus 005-016-017 5.5 6 Delphinus spp Delphinus 005-016-017 5.5 7 Delphinus spp Delphinus 005-016-017 5.5 8 Stenella attenuata ssp Stenella attenuata ssp 002-006-089-090 5.5 9 Stenella attenuata ssp Stenella attenuata ssp 002-006-089-090 5.5 10 Stenella attenuata ssp Stenella attenuata ssp 002-006-089-090 5.5 11 Stenella attenuata ssp Stenella attenuata ssp 002-006-089-090 5.5 12 Stenella attenuata ssp Stenella attenuata ssp 002-006-089-090 5.5 pooling regions bft Rg0 Rg0_CV 1 1 none 0 1.000 0.00 2 1 none 1 1.000 0.00 3 1 none 2 0.940 0.25 4 1 none 3 0.722 0.25 5 1 none 4 0.485 0.14 6 1 none 5 0.394 0.20 7 1 none 6 0.404 0.50 8 none none 0 1.000 0.00 9 none none 1 0.728 0.03 10 none none 2 0.531 0.06 11 none none 3 0.386 0.09 12 none none 4 0.282 0.12 This dataset includes Rg(0) estimates for the following species: barlow_2015$title %&gt;% unique [1] &quot;Delphinus spp&quot; &quot;Stenella attenuata ssp&quot; [3] &quot;Stenella longirostris ssp&quot; &quot;Striped dolphin&quot; [5] &quot;Rough-toothed dolphin&quot; &quot;Bottlenose dolphin&quot; [7] &quot;Risso&#39;s dolphin&quot; &quot;Short-finned pilot whale&quot; [9] &quot;Killer whale&quot; &quot;Sperm whale&quot; [11] &quot;Kogia spp&quot; &quot;Cuvier&#39;s beaked whale&quot; [13] &quot;Mesoplodon spp&quot; &quot;Dall&#39;s porpoise&quot; [15] &quot;Minke whale&quot; &quot;Sei/Bryde&#39;s&quot; [17] &quot;Fin whale&quot; &quot;Blue whale&quot; [19] &quot;Humpback whale&quot; &quot;Unidentified dolphin&quot; [21] &quot;Unidentified cetacean&quot; &quot;Pacific white-sided dolphin&quot; [23] &quot;Pygmy killer whale&quot; &quot;False killer whale&quot; If your study species – or one with similar detectability – can be found on this list, then you can consider using this data.frame of Rg(0) values and moving on to the next step, especially if your survey was conducted in the eastern or Central Pacific. Note that if you do not have a large survey dataset, this may be the only option available to you. Estimating new Rg(0) values requires a large number of sightings (i.e., hundreds) across many Beaufort states. New Rg(0) estimates LTabundR includes a function, g0_model(), which you can use to apply the Barlow (2015) modeling methods to generate new estimates of Rg(0) based on your own survey data or by combining your data with those used in the Barlow (2015) estimation. To do this, you first need a cruz object in which effort has been split into short segments (5 - 10 km). If you want to work with NOAA Fisheries DAS data from the eastern and central Pacific, you can use a built-in dataset of 1986 - 2020 surveys that is processed specifically for use in Rg(0) estimation: data(&quot;noaa_10km_1986_2020&quot;) To use this dataset in R(0) estimation, we first filter it to systematic effort within sea states 0 - 6: cruzi &lt;- filter_cruz(noaa_10km_1986_2020, analysis_only = TRUE, eff_types = &#39;S&#39;, bft_range = 0:6, on_off = TRUE) You can then estimate Rg(0) for each Beaufort sea state using the function g0_model(). For example, the code for striped dolphin (Stenella coeruleoalba) is as follows: rg0 &lt;- g0_model(spp = &#39;013&#39;, truncation_distance = 5.5, cruz = cruzi, constrain_shape = TRUE, jackknife_fraction = .1) The jackknife_fraction input indicates that the standard error and CV will be estimated using an iterative jackknife procedure in which 10% of the data is removed in each iteration. Find more details on this process using the function documentation, ?g0_model(). The input constrain_shape provides a way to ensure that the Rg(0) curve declines monotonically. Some modeled Rg(0) curves are liable to erreneously increase with increasing Bft due to low sample sizea at the lowest Bft (0-2) or highest Bft (5-6) states. To coerce monotonic decline, set this to TRUE, and the function will use a shape-constrained GAM (scam() from package scam) instead of a classic mgcv::gam(). We recommend beginning with NULL then modifying this if needed, based on the output: if the Rg(0) curve increases at any Bft state, try running again with constrain_shape set to TRUE. Note that this is an updated approach to the Bft-pooling method used in Barlow (2015). The chief result of this function is a $summary table: rg0$summary bft Rg0 ESW n Rg0_SE Rg0_CV ESW_SE 1 0 1.0000000 3.836271 10 0.00000000 0.0000000 0.17262018 2 1 0.7010295 3.619676 10 0.09606240 0.1370305 0.13829661 3 2 0.4914564 3.383832 10 0.11129915 0.2264680 0.09943560 4 3 0.3445470 3.138456 10 0.09640134 0.2797916 0.06063410 5 4 0.2415623 2.892539 10 0.07125957 0.2949946 0.04334873 6 5 0.1693673 2.652889 10 0.04442899 0.2623232 0.06671872 7 6 0.1187552 2.429804 10 0.02174702 0.1831249 0.10025878 The model predicts that Rg(0) declines rapidly with deteriorating sea state: plot(Rg0 ~ bft, data = rg0$summary, ylim=c(0,1), type=&#39;o&#39;, pch=16) abline(h=seq(0,1,by=.1), col=&#39;grey85&#39;, lty=3) In addition to the summary above, this function returns various details, including the details of the Generalized Additive Model (GAM) (for both the estimate and the jackknifed datasets), and the raw sightings and segments used in the model. rg0 %&gt;% names [1] &quot;Rg0&quot; &quot;gam&quot; &quot;jackknife&quot; &quot;summary&quot; &quot;sightings&quot; &quot;segments&quot; To produce these estimates efficiently for many species, you can use the wrapper function g0_table(), as follows. First you build a list of parameters for each species/species group: species &lt;- list( list(spp = c(&#39;005&#39;, &#39;016&#39;, &#39;017&#39;), title = &#39;Delphinus spp&#39;, constrain_shape = TRUE, truncation = 5.5), list(spp = &#39;021&#39;, title = &quot;Risso&#39;s dolphin&quot;, truncation = 5.5, constrain_shape = TRUE), list(spp = &#39;046&#39;, title = &#39;Sperm whale&#39;, truncation = 5.5), list(spp = c(&#39;047&#39;, &#39;048&#39;, &#39;080&#39;), title = &#39;Kogia spp&#39;, truncation = 4.0, constrain_shape = FALSE), list(spp = &#39;061&#39;, title = &quot;Cuvier&#39;s beaked whale&quot;, constrain_shape = TRUE, truncation = 4.0), list(spp = &#39;074&#39;, title = &#39;Fin whale&#39;, truncation = 5.5, regions = &#39;CCS&#39;)) Note that we used shorter truncation distances for cryptic species. Note also that we had to pool Beaufort sea states 0-2 for Risso’s dolphins in order to maintain a monotonic decline in the Rg(0) ~ Beaufort curve. Note also that we limited the geostrata used to model the fin whale Rg(0) curve to the California Current System (‘CCS’, after Barlow 2015), so that zero-inflated segments did not confound the model. In general we recommend that survey effort should only be included for regions within a species’ normal expected range. Otherwise zero-inflation may have an effect on Rg(0) model results. We then pass this species list to g0_table(). In this example, we are only estimating the Rg(0) relationship, without conducting jackknife estimation: rg0s &lt;- g0_table(cruzi, species, eff_types = &#39;S&#39;, jackknife_fraction = NULL) Now plot the result using a dedicated LTabundR function: g0_plot(rg0s, panes=1) Weighted average g(0) Since g(0) clearly depends upon survey conditions, and since each survey is carried out in a specific sequence of conditions, a unique, weighted g(0) value must be estimated for each species in each geostratum and survey of interest. This will be done automatically by the line-transect analysis functions coming up (see LTabundR::lta()), meaning you only need a table of Rg(0) values in order to proceed to the next step. But you can also calculate weighted g(0) values separately as an isolated analysis, which we show below. Let’s say we want to estimate the average g(0) for striped dolphins in the WHICEAS study area during the survey years of 2017 and 2020. From above, we have an estimate of the Rg(0) and its CV for each Beaufort state: rg0$summary %&gt;% select(bft, Rg0, Rg0_CV) bft Rg0 Rg0_CV 1 0 1.0000000 0.0000000 2 1 0.7010295 0.1370305 3 2 0.4914564 0.2264680 4 3 0.3445470 0.2797916 5 4 0.2415623 0.2949946 6 5 0.1693673 0.2623232 7 6 0.1187552 0.1831249 We also have a processed cruz object with 2017 data: load(&#39;whiceas_cruz_1720.RData&#39;) cruz_17 &lt;- filter_cruz(cruz_1720, years = 2017, verbose=FALSE) To view the distribution of effort in this WHICEAS 2017 across sea states, we can use the function summarize_bft(): summarize_bft(cruz_17)$overall # A tibble: 6 × 3 bftr km prop &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 1 132. 0.0203 2 2 585. 0.0899 3 3 920. 0.141 4 4 2143. 0.329 5 5 1875. 0.288 6 6 853. 0.131 We then use the function g0_weighted_var() to compute the weighted Rg(0) for our survey as well as its CV. This function carries out an automated optimization routine to simulate a new distribution for the weighted g(0), which is then used to estimate the weighted CV. weighted_g0_2017 &lt;- g0_weighted(Rg0 = rg0$summary$Rg0, Rg0_cv = rg0$summary$Rg0_CV, cruz = cruz_17) The result: weighted_g0_2017$g0[1:2] weighted_g0 wt.mean 1 0.2510412 0.258 Now let’s do the same for WHICEAS 2020 and compare the weighted g(0) estimate: # Filter to 2020 cruz_20 &lt;- filter_cruz(cruz_1720, years = 2020, verbose=FALSE) # Summarize Bft effort summarize_bft(cruz_20)$overall # A tibble: 6 × 3 bftr km prop &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 1 84.3 0.0160 2 2 266. 0.0505 3 3 440. 0.0838 4 4 1458. 0.277 5 5 1968. 0.374 6 6 1042. 0.198 Note that conditions were a bit worse in 2020 compared to 2017. We therefore expect the weighted g(0) estimate to be lower: weighted_g0_2020 &lt;- g0_weighted(Rg0 = rg0$summary$Rg0, Rg0_cv = rg0$summary$Rg0_CV, cruz = cruz_20) The result: weighted_g0_2020$g0[1:2] weighted_g0 wt.mean 1 0.2188324 0.226 Confirmed: weighted g(0) in the WHICEAS study area in 2020 is slightly lower than in 2017, due to generally worse survey conditions. These year-specific estimates should prevent those different conditions from impacting their respective abundance estimates. "],["lta.html", "8 Line-transect analysis Overview Key inputs Variance estimation Other inputs Output Unusual estimate scenarios Behind the scenes", " 8 Line-transect analysis Overview Line-transect analysis is used to produce estimates of animal density and/or abundance based upon survey data. The main LTabundR function for conducting line-transect analysis is lta(). This is a complex function with many steps, but the function’s three most important jobs are these: to estimate a detection function for a species or pool of similar species. A “detection function” is a statistical model of how the species’ detectability declines the farther individuals occur from the survey trackline. The model allows the user to estimate the width of their survey strip, which thus allows the user to estimate the total area that was effectively surveyed during the transect. To avoid biases and other statistical pitfalls, detection functions have to be estimated using data from formal transect effort during which observer effort adheres to standard protocols. Typically that means OnEffort is TRUE, EffType is \"S\" or \"N\" (and in some cases \"F\"), and Bft is 6 or lower — but you can specify those conditions yourself when you create your list of settings. Detection function performance hinges upon an adequate sample size of sightings, so it is typical for the dataset used in this stage to span as many years and regions as are available and appropriate. to estimate animal density based on the detection function’s survey strip estimate and the encounter rate for the species during periods of systematic effort, in which transects are placed in a systematic, design-based way throughout the study area. It is essential – at least in the vast majority of cases – that only systematic effort is used here (i.e., EffType is \"S\") in order to avoid bias. This means that only a subset of the survey data used to estimate the detection function may be used for estimating abundance, but that is acceptable as long as observer protocols do not differ between the two subsets of the data. Furthermore, density is typically estimated for a single year and region at a time, which means the data used in this stage of the analysis will be reduced even further. Once density is estimated, it is straightforward to then estimate animal abundance within a given area, such as a geostratum. to describe the uncertainty in these estimates, usually in the form of a Coefficient of Variation (CV) and/or 95% confidence intervals (95% CI). To estimate uncertainty, an iterative bootstrapping routine occurs in which segments of survey data are resampled with replacement and the entire estimation processes is repeated a large number of times (typically 1,000 to 10,000 times). This is the major time-consuming aspect of the lta() process. To prepare survey data for this kind of analysis, the data need to be processed in a few specific ways: (1) the data need to adhere to a common structure, (2) the data need columns indicating which sections of the survey should be used for estimating the detection function, (3) the sightings need columns confirming that they are also valid to include when estimating the detection function, and (4) the survey needs to be split into segments of similar effort and common length for resampling during bootstrap process for estimating uncertainty. This is precisely what process_surveys() does based on the settings you feed to it. The process_surveys() function (1) formats the data into an R object of a specific structure (we refer to it as a cruz object); (2) adds a column named use to every row indicating whether or not it should be used in detection function estimation (TRUE means use, FALSE means don’t); (3) adds a column named included to every sighting indicating whether or not it should be used in detection function estimation (TRUE means include, FALSE means don’t); and (4) adds a column named seg_id to every row indicating the numeric ID of the effort segment it falls within. That brings us to this chapter, in which we feed the cruz object returned by process_surveys() to the lta() function. This function will produce detection functions using the data where use is TRUE and the sightings where included is TRUE, then filter those data further – to systematic effort only (typically, EffType = \"S\" and Bft is 6 or less) for a single year and/or region – in order to estimate density/abundance. To emphasize that final point: all segments used in abundance estimation are also used for modeling the detection function, but not necessarily vice versa; the data used in abundance estimation will usually be a subset of those used for detection function modeling. At minimum, the lta() function calls for four primary arguments in addition to your cruz object: lta(cruz, Rg0, fit_filters, df_settings, estimates) For context, the full set of input options for the lta() function is as follows: lta(cruz, Rg0, fit_filters, df_settings, estimates, use_g0 = TRUE, ss_correction = 1, abund_eff_types = c(&quot;S&quot;), abund_bft_range = 0:6, bootstraps = 0, results_file = NULL, toplot = TRUE, verbose = TRUE ) Below we explain each of the key required inputs, discuss other optional inputs, and explore the results produced by lta(). Key inputs cruz This is the cruz object you have generated with process_surveys(). Before running lta(), ensure that this cruz object is filtered only to the years, regions, and sighting conditions you would like to use for detection function fitting. Filter your cruz object with full flexibility using LTabundR::filter_cruz(). Note that filtering for detection function fitting is typically less stringent than filtering for downstream steps for abundance estimation, since as many sightings are included as possible to combat low sample sizes, as long as sightings were observed using standard methods in an unbiased search pattern, and as long as you do not expect detectability to vary across years and regions. Here we will work with a version of the 1986-2020 Central North Pacific survey data we processed a few pages back. load(&#39;whiceas_cruz.RData&#39;) As it is provided, this dataset does not need any filtering for our purposes here. We will use these data to estimate the abundance of striped dolphins (Stenella coeruleoalba), Fraser’s dolphins (Lagenodelphis hosei), and melon-headed whales (Preponocephala electra) within the WHICEAS study area in 2017 and 2020. We will group these three species into a ‘species pool’ in order to gain a sufficient sample size for fitting a detection function. We will then use “Species” as a covariate within the detection function model, along with other variables including Beaufort Sea State, ship name, and log-transformed group size. Rg0 This input is the result of LTabundR::g0_model(), which is a data.frame with relative trackline detection probabilities, Rg(0), for each species in each Beaufort sea state. See LTabundR dataset data(\"g0_results\"), used below, as an example, or see the vignette chapter here. This is an optional input. If not provided, g(0) will be assumed to 1.0, and its CV will be assumed to be 0. Alternatively, you can manually specify values for g(0) and its CV in the estimates argument below. Here we will use a data.frame of Rg(0) estimates based on the same survey years, 1986 - 2020, which has been provided as a built-in dataset: data(&quot;g0_results&quot;) Rg0 &lt;- g0_results This dataset looks as follows. Each row is a Rg(0) estimate for a species group in a given Beaufort state, with details on the data used to generate that estimate and the CV of the estimate. For the lta() routine, the critical columns are spp, bft, Rg0, and Rg0_CV. Rg0 %&gt;% head title spp bft Rg0 ESW n Rg0_SE Rg0_CV 1 Delphinus spp 005-016-017 0 1.0000000 4.104915 10 0.00000000 0.00000000 2 Delphinus spp 005-016-017 1 0.7971695 3.844669 10 0.03347850 0.04199672 3 Delphinus spp 005-016-017 2 0.6354869 3.550071 10 0.05453631 0.08581815 4 Delphinus spp 005-016-017 3 0.5066066 3.235079 10 0.06703240 0.13231648 5 Delphinus spp 005-016-017 4 0.4038746 2.915404 10 0.07372469 0.18254354 6 Delphinus spp 005-016-017 5 0.3219857 2.604946 10 0.07656784 0.23779889 ESW_SE sits sits_p segs segs_p 1 0.26425179 38 0.01747126 225 0.004405631 2 0.20477770 188 0.08643678 1652 0.032347125 3 0.12560948 483 0.22206897 5057 0.099019013 4 0.06357847 574 0.26390805 9877 0.193397427 5 0.11723356 606 0.27862069 19463 0.381096904 6 0.20273631 270 0.12413793 13451 0.263378434 fit_filters The fit_filters input specifies how to filter the data before fitting the detection function. The filtering here is focused on species and truncation distances, not the regions or years included in the data. To filter by region, year or cruise number, use the function filter_cruz() before beginning the lta() process. The fit_filters input accepts a named list, which in our example will look like this: fit_filters = list(spp = c(&#39;013&#39;, &#39;026&#39;, &#39;031&#39;), pool = &#39;Multi-species pool 1&#39;, cohort = &#39;all&#39;, truncation_distance = 5, other_species = &#39;remove&#39;) spp: A character vector of species codes. Using multiple species codes may be useful when you have low sample sizes for a cohort of similar species. cohort: The cohort containing these species, provided as a name or a number indicating which slot in cruz$cohorts should be referenced. truncation_distance: The truncation distance to apply during model fitting. The remaining inputs are optional (i.e., they all have defaults): pool: A character string, providing a title for this species pool. If not specified, the species codes used will be concatenated to produce a title automatically. other_species: A character vector with four recognized values: If \"apply\" (the default if not specified), the species code will be changed to \"Other\" for sightings in which the species was in a mixed-species group but was not the species with the largest percentage of the total school size. In those cases, the focal species was not as relevant to the detection of the group as the “other” species was, which may bias the detection function. This option creates a factor level for the detection function to use (when \"species\" is a covariate) to distinguish between cue-relevant species that are within the specified pool and those that are not. The second option for other_species is \"ignore\", which does not reassign species codes to \"Other\", and ignores whether the species of interest held the plurality for a mixed-species group. The third option is \"remove\": any species re-assigned to \"Other\" will be removed before the detection function is fit; this can be useful if only a small number of species are re-assigned to \"Other\", which would then obviate species as a viable covariate (since the sample size of all species levels would be unlikely to exceed df_settings$covariates_n_per_level – see below). The fourth and final option is coerce, which forces all species codes to \"Other\" for the purposes of detection function fitting and abundance estimation. This is effectively the same as removing ‘species’ from the list of covariates, but this option can be a convenience if you want to quickly toggle the use of species as a covariate for a specific species pool and/or produce abundance estimates for unidentified taxa (e.g., an ‘Unidentified dolphins’ species pool that includes multiple species codes).   df_settings The df_settings input specifies how to fit a detection function to the filtered data. It accepts a named list, which in our example will look like this: df_settings = list(covariates = c(&#39;bft&#39;,&#39;lnsstot&#39;,&#39;cruise&#39;,&#39;year&#39;,&#39;ship&#39;,&#39;species&#39;), covariates_factor = c(FALSE, FALSE, TRUE, TRUE, TRUE, TRUE), covariates_levels = 2, covariates_n_per_level = 10, simplify_cue = TRUE, simplify_bino = TRUE, detection_function_base = &#39;hn&#39;, base_model = &#39;~1&#39;, delta_aic = 2) (Note that all of these inputs have defaults, and are therefore optional.) covariates Covariates you wish to include as candidates in detection function models, provided as a character vector. The covariates must match columns existing within cruz$cohorts$&lt;cohort_name&gt;$sightings. Note that the function will ignore case, coercing all covariates to lowercase. Default: no covariates. covariates_factor A Boolean vector, which must be the same length as covariates, indicating whether each covariate should be treated as a factor instead of a numeric. Default: NULL. covariates_levels The minimum number of levels a factor covariate must have in order to be included as an eligible covariate. Default: 2. covariates_n_per_level The minimum number of observations within each level of a factor covariate. If this condition is not met, the covariate is excluded from the candidates. Default: 10. simplify_cue A Boolean, with default TRUE, indicating whether or not cue codes should be simplified before being included as a covariate in detection function fitting. This can help to overcome the factor sample size limitations that may prevent inclusion as a covariate. If TRUE, cues 0, 1, 2, 4, and 7 are coerced to 5, representing ‘other’ cues. simplify_bino A Boolean, with default TRUE, indicating whether or not sighting method codes should be simplified before being included as a covariate in detection function fitting. This can help to overcome the factor sample size limitations that may prevent inclusion as a covariate. If TRUE, methods other than 4 are all coerced to 5, representing ‘other’ methods. detection_function_base The base key for the detection function, provided as a character vector. Accepted values are \"hn\" (half-normal key, the default, which exhibits greater stability when fitting to cetacean survey data; Gerrodette and Forcada 2005), \"hr\" (hazard-rate), or c(\"hn\", \"hr), which will loop through both keys and attempt model fitting. base_model The initial model formula, upon which to build using candidate covariates. If not provided by the user, the default is \"~ 1\". delta_aic The AIC difference between the model yielding the lowest AIC and other candidate models, used to define the best-fitting models. Typically, AIC differences of less than 2 (the default) indicate effectively equal model performance. If this value is not zero, then model averaging will be done: if multiple models are within delta_aic of the model with the lowest AIC, all “best” models will be used in subsequent steps and their results will be averaged. See Details below. estimates The estimates input specifies which estimates of density and abundance to produce based on the fitted detection function. This input accepts a list of sub-lists, which in our example will look something like this: estimates &lt;- list(list(spp = &#39;013&#39;, title = &#39;Striped dolphin&#39;, years = 2017, regions = &#39;WHICEAS&#39;)) This example shows only a single sub-list, specifying how to generate a density/abundance estimate for striped dolphins (species code 013) within the “WHICEAS” geostratum for 2017. spp: A character vector of species codes. title: A title for this abundance estimate, given as a character vector, ’ e.g., \"Striped dolphin - pelagic\". If left blank, the species code(s) will be concatenated to use as a title. years: A numeric vector of years, used to filter data to include only effort/sightings from these years. regions: A character vector of geostratum names, used to filter the data. Any segment or sighting occurring within any (but not necessarily all) of the provided regions will be returned. This holds true for nested regions: for example, in analyses from the Central North Pacific, in which the Hawaii EEZ geostratum (\"HI-EEZ\") is nested within the larger geostratum representing the entire CNP study area (\"OtherCNP\"), an input of regions = \"OtherCNP\" will return segments/sightings both inside the Hawaii EEZ and outside of it. You also have the option of manually specifying other filters &amp; arguments. Each of these sub-lists accepts the following named slots: cruises: An optional numeric vector of cruise numbers, used to filter data to include effort/sighting from only certain cruises. Ignored if NULL. regions_remove: A character vector of geostratum names, similar to above. Any segment or sighting occurring within any of these regions_remove will not be returned. Using the example above, if regions = \"OtherCNP\" and regions_remove = \"HI-EEZ\", only segments occuring within OtherCNP and outside of HI-EEZ will be returned. This can be particularly useful for abundance estimates for pelagic stock that exclude nested insular stocks. g0: If left as the default NULL, the lta() function will automatically estimate the weighted trackline detection probability (g0) according to the distribution of Beaufort sea states contained within the survey years/regions for which density/abundance is being estimated (this is done using the LTabundR function g0_weighted()). This will only be done if the Rg0 input above is not NULL; if it is and you do not provide g(0) values here, g0 will be coerced to equal 1. To coerce g(0) to a certain value of your own choosing, you can provide a numeric vector of length 1 or 2. If length 1, this value represents g(0) for all groups regardless of size. If length 2, these values represent g(0) for small and large group sizes, as defined by g0_threshold below. g0_cv: Similar to g0 above: if left NULL, the CV of the g(0) estimate will be automatically estimated based on weighted survey conditions. Alternatively, you can manually specify a CV here, using a numeric vector of length 1 or 2. If you do not specify a value and Rg0 input is NULL, g0_cv will be coerced to equal 0. g0_threshold: The group size threshold between small and large groups. alt_g0_spp: An alternate species code to use to draw Rg(0) values from the Rg0 input. This is useful in the event that Rg(0) was not estimated for the species whose density/abundance you are estimating, but there is a similarly detectable species whose Rg(0) parameters have been estimated. combine_g0: A Boolean, with default FALSE. If TRUE, weighted g0 estimates will be produced separately for each species code provided (specifically, for each unique row in the Rg0 table that is found after filtering by the species codes you provide in this estimate), THEN average those estimates together. This can be useful when you do not have a Rg(0) estimates for a certain species, but you can approximate g(0) by averaging together estimates from multiple species (e.g., averaging together weighted g(0) from across rorqual species in order to get a weighted g(0) estimate for ‘Unidentified rorquals’). region_title: An optional character vector indicating the title you would like to give to the region pertaining to this estimate. This can be useful if you have a complicated assemblage of regions you are combining and/or removing. If not supplied, the function will automatically generate a region_title based on regions and regions_remove. forced_effort: If this is a single numeric value instead of NULL (NULL is the default), this value will be used as the survey effort, in km, in a brute-force method; this same value will be used for every year and region. This is only helpful if you are looking for a relatively easy way to compare results from your own analysis to another (e.g., comparing LTabundR results to NOAA Fisheries reports of cetacean abundance in the central and eastern Pacific through 2022, in which effort was calculated slightly differently). area: If this is a single numeric value instead of NULL (NULL is the default), this value will be used as the area of the region in which abundance is being estimated, in square km, in a brute-force approach. If left NULL, the function will calculate the final area of the survey area resulting from the regions and regions_remove filters above. remove_land: A Boolean, with default TRUE, indicating whether or not land area should be removed from the survey area before calculating its area for abundance estimation. This term is only referenced if area is not specified manually. Here is the full estimates list for all the species-year-geostratum combinations for which we want to estimate density/abundance: estimates &lt;- list( list(spp = &#39;013&#39;, title = &#39;Striped dolphin&#39;, years = 2017, regions = &#39;WHICEAS&#39;), list(spp = &#39;013&#39;, title = &#39;Striped dolphin&#39;, years = 2020, regions = &#39;WHICEAS&#39;), list(spp = &#39;026&#39;, title = &#39;Frasers dolphin&#39;, years = 2017, regions = &#39;WHICEAS&#39;), list(spp = &#39;026&#39;, title = &#39;Frasers dolphin&#39;, years = 2020, regions = &#39;WHICEAS&#39;), list(spp = &#39;031&#39;, title = &#39;Melon-headed whale&#39;, years = 2017, regions = &#39;WHICEAS&#39;), list(spp = &#39;031&#39;, title = &#39;Melon-headed whale&#39;, years = 2020, regions = &#39;WHICEAS&#39;)) Each of these sub-lists specifies the details for a single estimate of density/abundance, making it possible to produce multiple estimates from the same detection function model. Generally, there needs to be a sub-list for each species-region-year combination of interest. You can imagine that building up these sub-lists can get tedious. It can also introduce the possibility of error or inconsistencies across estimates of multiple species. To address that issue, LTabundR includes the function lta_estimates(), which makes your code for preparing an estimates object much more efficient. That function is demonstrated in the Case Studies chapters. To run lta() with the above inputs, use this code: results &lt;- lta(cruz, Rg0, fit_filters, df_settings, estimates) After running lta(), quickly review results: results$estimate title species Region Area year segments km 1 Striped dolphin 013 (WHICEAS) 402948.7 2017 29 2981.643 2 Striped dolphin 013 (WHICEAS) 402948.7 2020 32 4439.758 3 Frasers dolphin 026 (WHICEAS) 402948.7 2017 29 2981.643 4 Frasers dolphin 026 (WHICEAS) 402948.7 2020 32 4439.758 5 Melon-headed whale 031 (WHICEAS) 402948.7 2017 29 2981.643 6 Melon-headed whale 031 (WHICEAS) 402948.7 2020 32 4439.758 Area_covered ESW_mean n g0_est ER_clusters D_clusters N_clusters 1 11206.15 3.758381 3 0.2428454 0.0010061565 0.00055482111 223.56446 2 17060.72 3.842713 3 0.2146882 0.0006757124 0.00041176947 165.92199 3 NA NA 0 0.4813348 0.0000000000 0.00000000000 0.00000 4 17766.04 4.001577 1 0.4524198 0.0002252375 0.00006220681 25.06615 5 10094.61 3.385586 2 0.8038000 0.0006707710 0.00012380395 49.88665 6 15551.03 3.502675 2 0.7894918 0.0004504750 0.00008167707 32.91167 size_mean size_sd ER D N g0_small g0_large 1 31.95079 0.5780968 0.03214749 0.01774864 7151.791 0.2428454 0.2428454 2 53.95865 2.7402089 0.03646053 0.02220682 8948.210 0.2146882 0.2146882 3 NA NA 0.00000000 0.00000000 0.000 0.4813348 0.4813348 4 261.32841 NA 0.05886095 0.01625641 6550.498 0.4524198 0.4524198 5 185.98103 96.4369707 0.12475069 0.02245695 9048.998 0.8038000 0.8038000 6 324.68110 58.0826568 0.14626070 0.02634228 10614.589 0.7894918 0.7894918 g0_cv_small g0_cv_large 1 0.177 0.177 2 0.174 0.174 3 0.854 0.854 4 0.923 0.923 5 0.455 0.455 6 0.491 0.491 More details on the lta() output are provided below. Variance estimation By default, the lta() function produces a single estimate of the detection function and a single estimate of density/abundance estimate for each sub-list within estimates(). However, you can obtain the coefficient of variation (CV) of those estimates by activating the function’s bootstrap variance estimation feature. To do this, add bootstraps as an input specifying a large number of iterations (1,000 iterations is standard, but we suggest first testing your code with 5 - 10 bootstraps before committing; the function typically requires ~1 hour per 100 bootstraps.). For the purposes of example, we will use just 100 bootstrap iterations: results &lt;- lta(cruz, Rg0, fit_filters, df_settings, estimates, bootstraps = 100) This command will first produce official estimates of the detection function and density/abundance, then it will repeat the analysis for the number of iterations you have specified. In each iteration, survey segments are re-sampled according to standard bootstrap variance estimation methods (see more details below, in “Behind the Scenes”). We describe the output of lta() in detail below, but here is a brief snapshot of the bootstrap results, for one of the species-region-year estimates: results$bootstrap$summary[2, ] %&gt;% as.data.frame title Region year species iterations ESW_mean g0_mean 1 Frasers dolphin (WHICEAS) 2020 026 100 4.072463 0.4651321 g0_cv km ER D size Nmean Nmedian Nsd 1 0.8702518 4535.045 0.06197543 33.4549 261.3284 13480611 4752.936 81686026 CV L95 U95 1 6.059519 320966.9 566185668 The Delta Method LTabundR provides a function, lta_delta_method(), that allows you to re-calculate the CV and 95% Confidence Intervals of your density/abundance estimate using a modification of the Delta Method. The reason we provide this is that abundance estimates using our bootstrap approach have been found found to be extremely sensitive to the distribution of g(0) when it is included in the bootstrap procedure. Given that the true distribution of errors in g(0) is not known, an alternative approach is to add the uncertainty attributed to g(0) estimates after the bootstrap process using the delta method, which assums that the resulting abundance estimates are log-normally distributed. This is what we implement in lta_delta_method(): that function takes the output of lta() call in which bootstrap iterations were performed, then replaces the CV and L95 and U95 confidence intervals within the output using the delta method. # Put your lta() output into a list ltas &lt;- list(results) # Run delta method results_dm &lt;- lta_delta_method(ltas, verbose=FALSE) Now view how results have changed from above: results_dm[[1]]$bootstrap$summary[2, ] %&gt;% as.data.frame title Region year species iterations ESW_mean g0_mean 1 Frasers dolphin (WHICEAS) 2020 026 100 4.072463 0.4651321 g0_cv km ER D size Nmean Nmedian Nsd 1 0.8702518 4535.045 0.06197543 33.4549 261.3284 13480611 4752.936 81686026 CV L95 U95 D_real N_real CV_delta L95_delta U95_delta 1 1.445196 1124.594 38155.11 0.01625641 6550.498 1.445196 1124.594 38155.11 CV_boot L95_boot U95_boot 1 6.059519 320966.9 566185668 Other inputs As mentioned near the top, there are several optional input arguments that lend further control over the lta() procedure. lta(cruz, Rg0, fit_filters, df_settings, estimates, use_g0 = TRUE, ss_correction = 1, abund_eff_types = c(&quot;S&quot;), abund_bft_range = 0:6, bootstraps = 10 toplot = TRUE, verbose = TRUE,) use_g0: A Boolean, with default TRUE, indicating whether or not to use custom g(0) value(s). If FALSE, the assumed g(0) value will be 1. This is an easy way to toggle on-and-off automated g(0) estimation and/or ignore manually supplied g(0) values. ss_correction: Should a correction be applied to school sizes? School sizes will be scaled by this number. The default, 1, means no changes will occur. abund_eff_types: The EffType values that are acceptable as systematic effort when estimating density/abundance. The default is just \"S\" (systematic effort), but in some surveys/cases you may wish to use fine-scale effort (\"F\") too. abund_bft_range: The Beaufort sea state values that are acceptable during systematic effort for estimating density/abundance. The default is sea states 0 - 6. toplot: A Boolean, with default TRUE, indicating whether detection function plots (Distance::plot.ds()) should be displayed as the candidate models are tested. verbose: A Boolean, with default TRUE, indicating whether or not updates should be printed to the Console. Output During processing While lta() is running, it will print things to the Console (if verbose is TRUE), plot diagnostic plots of how the study area is being calculated (if toplot is TRUE), and plot detection function model candidates (if toplot is TRUE). To demonstrate this, we will run the estimate for striped dolphins in 2017 only, without variance bootstrapping. To expedite processing, we will manually supply g(0) values from Bradford et al. (2022) (this saves about 3 minutes per estimate): new_estimates &lt;- list( list(spp = &#39;013&#39;, title = &#39;Striped dolphin&#39;, years = 2017, regions = &#39;WHICEAS&#39;, g0 = 0.35, g0_cv = 0.19)) # Run it: demo &lt;- lta(cruz, Rg0, fit_filters, df_settings, new_estimates, bootstraps = 0) Additionally, windows will appear showing details for the detection function models and details of the density/abundance estimate. Outputs The lta() function returns a list of objects. To demonstrate this output, we will pull back in the dataset representing the result of the analysis above, for all three species in both years (with 10 bootstrap iterations): This list of results has five slots: names(results) [1] &quot;pool&quot; &quot;inputs&quot; &quot;estimate&quot; &quot;df&quot; &quot;g0_tables&quot; &quot;bootstrap&quot; pool: The species pool pertaining to these estimates. inputs: A list of the inputs used to produce these estimates. estimate: A table of density/abundance estimates for each species/region/year combination specified in the estimates input. results$estimate title species Region Area year segments km 1 Striped dolphin 013 (WHICEAS) 402948.7 2017 29 2981.643 2 Striped dolphin 013 (WHICEAS) 402948.7 2020 32 4439.758 3 Frasers dolphin 026 (WHICEAS) 402948.7 2017 29 2981.643 4 Frasers dolphin 026 (WHICEAS) 402948.7 2020 32 4439.758 5 Melon-headed whale 031 (WHICEAS) 402948.7 2017 29 2981.643 6 Melon-headed whale 031 (WHICEAS) 402948.7 2020 32 4439.758 Area_covered ESW_mean n g0_est ER_clusters D_clusters N_clusters 1 11206.15 3.758381 3 0.2428454 0.0010061565 0.00055482111 223.56446 2 17060.72 3.842713 3 0.2146882 0.0006757124 0.00041176947 165.92199 3 NA NA 0 0.4813348 0.0000000000 0.00000000000 0.00000 4 17766.04 4.001577 1 0.4524198 0.0002252375 0.00006220681 25.06615 5 10094.61 3.385586 2 0.8038000 0.0006707710 0.00012380395 49.88665 6 15551.03 3.502675 2 0.7894918 0.0004504750 0.00008167707 32.91167 size_mean size_sd ER D N g0_small g0_large 1 31.95079 0.5780968 0.03214749 0.01774864 7151.791 0.2428454 0.2428454 2 53.95865 2.7402089 0.03646053 0.02220682 8948.210 0.2146882 0.2146882 3 NA NA 0.00000000 0.00000000 0.000 0.4813348 0.4813348 4 261.32841 NA 0.05886095 0.01625641 6550.498 0.4524198 0.4524198 5 185.98103 96.4369707 0.12475069 0.02245695 9048.998 0.8038000 0.8038000 6 324.68110 58.0826568 0.14626070 0.02634228 10614.589 0.7894918 0.7894918 g0_cv_small g0_cv_large 1 0.177 0.177 2 0.174 0.174 3 0.858 0.858 4 0.919 0.919 5 0.459 0.459 6 0.494 0.494 df: A named list with details for the detection function. results$df %&gt;% names [1] &quot;best_models&quot; &quot;all_models&quot; &quot;best_objects&quot; &quot;sightings&quot; &quot;sample_size&quot; [6] &quot;curve&quot; results$df$best_models Model Key_function Formula Pmean AIC 1 9 hn ~1 + ship + bft + lnsstot 0.5609582 939.916 2 11 hn ~1 + ship + bft + lnsstot + species 0.5552118 940.489 3 6 hn ~1 + ship + bft 0.5658167 941.858 $\\\\Delta$AIC Covariates tested pool 1 0.000 bft, lnsstot, ship, species Multi-species pool 1 2 0.573 bft, lnsstot, ship, species Multi-species pool 1 3 1.942 bft, lnsstot, ship, species Multi-species pool 1 bootstrap: If bootstrap variance estimation was carried out, the output would also include bootstrap, a named list with results from the bootstrap process, only returned if the bootstraps input is greater than 1. results$bootstrap %&gt;% names [1] &quot;summary&quot; &quot;details&quot; &quot;df&quot; results$bootstrap$summary %&gt;% head # A tibble: 6 × 18 # Groups: title, Region [3] title Region year species iterations ESW_mean g0_mean g0_cv km ER &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 Frasers d… (WHIC… 2017 026 100 NaN 0.417 0.990 3037. 0 2 Frasers d… (WHIC… 2020 026 100 4.07 0.465 0.870 4535. 0.0620 3 Melon-hea… (WHIC… 2017 031 100 3.25 0.777 0.501 3037. 0.115 4 Melon-hea… (WHIC… 2020 031 100 3.41 0.784 0.490 4535. 0.142 5 Striped d… (WHIC… 2017 013 100 3.77 0.242 0.174 3037. 0.0311 6 Striped d… (WHIC… 2020 013 100 3.95 0.225 0.182 4535. 0.0389 # ℹ 8 more variables: D &lt;dbl&gt;, size &lt;dbl&gt;, Nmean &lt;dbl&gt;, Nmedian &lt;dbl&gt;, # Nsd &lt;dbl&gt;, CV &lt;dbl&gt;, L95 &lt;dbl&gt;, U95 &lt;dbl&gt; Unusual estimate scenarios Most line-transect estimates in most areas are relatively straightforward: you want an estimate for a single species in a single year, and your geostrata do not overlap, nor do they need to be stratified or combined in atypical ways. But there will also be unusual and slightly more complicated scenarios. We outline some of those below and demonstrate how they can be handled within the LTabundR framework. Species combinations &amp; g(0) Mixed-species groups Mixed-species groups can confound detection function model fitting, since your species of interest may not be the predominant species in the group, which means that the other species present may be having a greater influence over the detection function. You can decide how to account for this using the other_species slot in your fit_filters list. See the details on this discussed above. Species pools When you don’t have enough sightings of individual species to model a detection function effectively, it can be useful to pool sightings from multiple species who have similar detection characteristics. This is a common tactic by NOAA Fisheries analysts who conduct cetacean surveys in the Central North Pacific. When you do this, you typically need to make the following changes to a “normal” lta() call: In your df_settings list, consider adding species as a covariate, and ensure that you specify that it should be treated as a factor. This may improve detection function model fit. In your fit_filters list, specify multiple species codes and name your species pool accordingly (e.g., “Multi-species pool 1”). In your fit_filters list, specify how to handle “Other” species (see above). In your estimates list, add a sub-list for each species-region-year for which you want a density/abundance estimate. We took all of these steps in the example above with striped dolphins, Fraser’s dolphins, and melon-headed whales. Use that code as a guide. Pooling similar species Species that can be confused with one another may need to be pooled together for abundance estimation. For example, in the northeast Pacific, sei whales, Bryde’s whales, and fin whales can co-occur but they are difficult to distinguish in the field. They are also relatively uncommon, and may need to be pooled with other species in order to obtain a sound detection function model. There is also a species code, \"199\", to indicate sightings that could be any of these species. That species code could be used to estimate the “abundance” of this ambiguous species code, but it does not have a sample size sufficient for estimating Relative g(0). To handle this special case, we will follow all the steps taken for a multi-species pool, as discussed above. Additionally, in our estimates sub-list(s), we will specify (1) that the ambiguous species code should be used for the abundance estimate, and (2) that the g(0) should represent the average of the weighted g(0) estimates for each of the candidate species: estimates &lt;- list( list(spp = &#39;199&#39;, title = &quot;Sei/Bryde&#39;s/Fin whale&quot;, alt_g0_spp = c(&#39;072&#39;, &#39;073&#39;, &#39;099&#39;, &#39;074&#39;), combine_g0 = TRUE)) You can also multiple species within the spp input, which would indicate that you want to estimate the pooled abundance of those species together. Rare unidentified taxa A similar problem occur when you have species codes for unidentified taxa that have been identified down to a family- or genus-level. For example, “Unidenfitied Mesoplodon” is a species code (the code is \"050\") for any beaked whale that is definitely in the genus Mesoplodon. There are plenty of these sightings, which means g(0) and its CV can be estimated just fine without referring to other species codes. Other unidentified taxa, however, are less common. For example, in Hawaiian studies, density/abundance is estimated for “Unidentified rorquals”. In the field there is a species code for this group, \"070\", but it is rarely used – there are enough sightings to model the detection function, but not nearly enough sightings to estimate g(0) or its CV. In this case, we need to combine g(0) from more common species codes in order to estimate the unidentified rorqual’s g(0). To do this, we fit a detection function using species code \"070\" … fit_filters &lt;- list(spp = c(&#39;070&#39;), pool = &#39;Unidentified rorqual&#39;, cohort = &#39;all&#39;, truncation_distance = 5.5) … then, in our estimates list, we specify some alternate g(0) species designations. estimates &lt;- list( list(spp = &#39;070&#39;, title = &#39;Unidentified rorqual&#39;, years = 2017, regions = &#39;WHICEAS&#39;, alt_g0_spp = c(&#39;071&#39;,&#39;099&#39;,&#39;074&#39;,&#39;075&#39;), combine_g0 = TRUE)) Geostratum combinations In your estimates sub-lists, the regions and regions_remove slots give you control of the geographic scope of (1) the weighted g(0) and CV used in density estimation, (2) the effort and sightings used to estimate density, (3) and the area used to calculate abundance. A note on cohort geostrata Recall that, when processing your survey data to create a cruz object, you provide a list of geostrata as an argument in your process_surveys() call. You also have the option to specify a subset of those geostrata for each species cohort (see load_cohort_settings()), which is an option that you should almost always use. Selecting a subset of geostrata is important because that subset is used to “segmentize” your survey data – i.e., break effort into discrete sections that can be bootstrap-sampling during the lta() variance estimation routine – and the segmentizing procedure always breaks segments when a survey passes from one geostratum to another. This matters because the lta() bootstrapping routine will re-sample survey segments in a way that preserves the proportion of segments occurring in each geostratum, to ensure that all geostrata are represented in the same proportion as the original estimate. When segments are unncessarily broken into small segments by irrelevant geostrata that have been included in the analysis, the bootstrap estimate of the CV is likely to be artificially enlarged. For example: in the Central North Pacific, there are about 11 geostrata commonly used. These include the Hawaiian EEZ geostratum, the Main Hawaiian Islands geostratum, and the larger CNP geostratum that represents the maximum range of the study area. These three geostrata are typically all you need for most density estimates for most species. However, a few species – e.g., bottlenose dolphin, pantropical spotted dolphin, and false killer whale – have special geostrata that represent insular stock boundaries and/or pelagic stock boundaries. If those insular geostrata are used in density estimates for which they do not apply, they will confound the bootstrap estimate of density/abundance CV. Punchline: be sure to specify only the relevant geostrata in each cohort’s settings. Combining disparate geostrata For example, in Hawaii bottlenose dolphins belong to a pelagic stock as well as several insular stocks. If you wished to estimate the abundance of all insular stocks together, you simply provide their respective geostratum names in the regions slot of your estimates sub-list: estimates &lt;- list(list(spp = &#39;018&#39;, title = &#39;Bottlenose dolphin&#39;, years = 2017, regions = c(&#39;Bottlenose_KaNi&#39;, &#39;Bottlenose_OUFI&#39;, &#39;Bottlenose_BI&#39;))) Removing insular geostrata Conversely, you may wish to estimate density/abundance for pelagic bottlenose dolphins only, ignoring the insular stocks. You can subtract geostrata using the regions_remove slot: estimates &lt;- list(list(spp = &#39;018&#39;, title = &#39;Bottlenose dolphin&#39;, years = 2017, regions = &#39;HI_EEZ&#39;, regions_remove = c(&#39;Bottlenose_KaNi&#39;, &#39;Bottlenose_OUFI&#39;, &#39;Bottlenose_BI&#39;))) Combine partially overlapping geostrata Say you want to estimate the density/abundance for a set of geostrata that partially overlap. An example of this is that the Northwestern Hawaiian Islands geostratum overlaps slightly with the Main Hawaiian Islands geostratum. This is not an issue; when study area is calculated within lta() (actually, that function calls another function, strata_area(), to do this. That function is demonstrated below), overlap among strata is accounted for. estimates &lt;- list(list(spp = &#39;018&#39;, title = &#39;Bottlenose dolphin&#39;, years = 2017, regions = c(&#39;MHI&#39;,&#39;NWHI&#39;))) Regionally stratified analysis Field surveys are sometimes stratified such that trackline design and/or density can differ substantially across regions. Also, analysts may sometimes wish to estimate density/abundance for individual regions separately, regardless of design stratification. In lta(), you can accommodate a stratified analysis by providing an estimates sub-list for each geostratum. For example, in 2002 the Hawaiian EEZ was surveyed with different effort intensity in the Main Hawaiian Islands region compared to pelagic waters. For that reason, density/abundance estimates ought to be stratified by region: estimates &lt;- list( list(spp = &#39;013&#39;, title = &#39;Striped dolphin&#39;, years = 2002, regions = &#39;MHI&#39;), list(spp = &#39;013&#39;, title = &#39;Striped dolphin&#39;, years = 2002, regions = &#39;HI_EEZ&#39;, regions_remove = &#39;MHI&#39;)) Here we have one 2002 estimate for the Main Hawaiian Islands, and a second for the pelagic Hawiian EEZ, achieved by subtracting the \"MHI\" stratum from the \"HI_EEZ\" stratum. Once lta() processing is complete, you can summarize and plot the results for each study area separately. The next step is to combine the stratified estimates to generate a grand estimate for the entire EEZ. This is achieved using the LTabundR functions lta_enlist() and lta_destratify(). We discuss this further in the next chapter. Subgroup-based analyses After 2010, NOAA Fisheries Pacific Islands Fisheries Science Center began a sub-group protocol referred to as the “PC Protocol” after the scientific name for false killer whales, Pseudorca crassidens, the species for which the protocol was designed. False killer whales are rare and occur in dispersed subgroups, which complicates conventional distance sampling approaches to line-transect analysis. To handle this, a separate, subgroup-based analytical approach was developed (Bradford et al. 2014). This approach could theoretically be used for other species that occur in subgroups. We cover this in a subsequent chapter. Other scenarios Study periods that span years, such as a December - January survey. This is not yet handled well within the LTabundR framework. More will go here. Behind the scenes Step-by-step operations The following list is a summary of each step of the lta() function, in order of operations. Function inputs are checked for valid formatting, and any optional inputs that are not user-provided are given default values. For each sublist in your estimates input, the following actions are taken: Sublist inputs are checked for valid formatting, and any optional inputs that are not user-provided are given default values. A polygon is created based on the sublist’s slots for regions (strata to include in abundance estimation) and region_remove (strata to exclude). This process is detailed in the Area Estimation section below. The spatial area, in square km, is calculated for that polygon. Weighted g(0) values are estimated based on the survey effort related to this sublist: Determine the species code(s) to be used to filter the Rg0 dataset. Usually this is the species code(s) in the spp slot, but if the sublist has an alt_g0 slot, the code(s) in that slot is used to filter the Rg(0) dataset. If multiple codes are provided, and the sublist has a slot named combine_g0 with the value TRUE, then weighted g(0) values will be estimated for each species then those values will be averaged together a few steps down. (If multiple species codes are provided but combine_g0 is not provided or is FALSE, then only the first species code will be used.) If the species code(s) are not found in the Rg0 dataset, weighted g(0) will be coerced to 1.0 with CV of 0. Survey effort is filtered down according to the relevant slots in the sublist – years, regions (the strata to include), and, optionally, regions_remove (the strata to exclude) — as well as the lta() inputs abund_eff_types (which EffortType values are usable for abundance estimation) and abund_bft_range (which Beaufort sea states are usable). That survey effort is used to estimate weighted g(0) and its CV for each species code using the LTabundR function g0_weighted(). That function does the following: Determines the proportion of survey effort occurring in each Beaufort state (this is done using the LTabundR function summarize_bft()). Uses those sea state proportions as weights to calculate the weighted mean g(0) based on the values in Rg(0). For example, if the Rg(0) values for species code \"046\" (sperm whale) in Beaufort states 0 to 6 are 1.00, 0.87, 0.75, 0.65, 0.57, 0.49, and 0.42, respectively, and the proportion of effort in those respective sea states are 0.01, 0.03, 0.07, 0.19, 0.35, 0.24, and 0.11, then the weighted g(0) is 0.679. This weighted g(0) is the value provided in the final results table for the function. To estimate the CV of this weighted g(0), a MCMC routine is used that first approximates the distribution of g(0) … (details needed here) … This is essentially a direct adaptation of the code developed by Jeff Moore and Amanda Bradford, and described in Bradford et al. (2021). If multiple species codes were provided and therefore multiple estimates of weighted g(0) were produced, those estimates are averaged using the LTabundR function g0_combine(). That function uses the same equations as the Excel spreadsheet used in the Bradford et al. (2022) WHICEAS study. The sublist is updated with these resulting values: the study area polygon, the estimated of weighted g(0), its CV, and the proportional effort in each Beaufort state. A reference list is built containing details for the g(0) distributions for each sublist in estimates. This is done in order to facilitate easy recall during the bootstrapping phase later on in the function. Each weighted g(0) and its CV are passed to the LTabundR function g0_optimize(), which returns the parameters needed to generate a distribution from which to draw bootstrap values of g(0). That function is a near-exact replica of the code developed by Jay Barlow to produce bootstrapped estimates in Barlow (2006). It involves an optimization routine which can take several seconds to complete, so this step is conducted once here rather than repeatedly during bootstrap iterations. The cruz object is filtered to survey data for the specified cohort, to segments for which the column use is TRUE, and to sightings for which the column included is TRUE. The sightings are filtered further to include only the species specified in the input fit_filters$spp. The resulting datasets will be used for modeling the detection function, and a subset of the datasets will be used for estimating abundance. Handling “Other” species designations: if the input fit_filters has a slot for other_species, certain species codes in mixed-species sightings may be modified if they do not constitute the plurality of the group size. See details in Fit filters above. If any of the covariates requested within the input df_settings$covariates have missing values (i.e., NA), those sightings will be removed from the dataset used for fitting detection functions and estimating abundance. Similarly, any sightings with invalid group size estimates (the column ss_valid == FALSE) are removed. The sample sizes used in fitting the detection function are stored. If the input df_settings$simplify_cue exists and is TRUE, cue codes (column Cue in the sightings dataset) are simplified to aid in overcoming factor level sample size limitations. If the input df_settings$simplify_bino exists and is TRUE, sighting method codes (column Method in the sightings dataset) are simplified to aid in overcoming factor level sample size limitations. If any of the covariates to be used in modeling the detection function should be treated as a factor (this is specified in the input df$covariates_factor), then the covariates columns are updated as such. All covariates specified as a factor are first tested for eligibility. Only factors with at least two levels (or whatever you specified with df_settings$covariates_levels) and 10 observations in each level (or whatever you specified with df_settings$covariates_n_per_level) are eligible for inclusion in the final set of covariates. The final results object is staged; this is a list with slots for data regarding the species pool, the inputs used, the detection function model, and the resulting estimates. A large loop is then initiated, with the first iteration being the formal estimate. Subsequent iterations are part of the bootstrapping process for estimating CV and confidence intervals. The detection function is modelled based on the sightings, covariates and truncation distance provided. This is done by calling the LTabundR function df_fit(), which depends upon the detection function modeling function mrds() from the package Distance. See further details in the section below on “Fitting a detection function”. Detailed results of the best-fit model are stored, its detection curve is generated using the LTabundR function df_curve(), and the Effective Strip Half-Width estimates for each sighting is added to the sightings data. Segments and sightings are filtered to only data appropriate for estimating density/abundance. This means OnEffort is TRUE, EffortType is one of the variables in the lta() input abund_efftypes (the defaults is just \"S\"), Bft can be rounded to one of the values provided in the input abund_bft_range (the default is 0:6), and sightings occur within the truncation distance. Loop through each sublist in the lta() input estimates: Segments and sightings are filtered to the species, years, cruises, and regions specified. Abundance is estimated using the LTabundR() function abundance(), based upon the filtered data and the g(0) value for this sublist. The resulting Effective Strip Half-Width (ESW) is based on the mean ESW of the sightings used to estimate abundance. The resulting g(0) is based on the mean weighted g(0) value for each sighting, since in some use cases a different g(0) value is supplied for small vs. large schools. Results are stored. If a filepath was supplied for the lta() input results_file, an RData object with all of the results up to this point is saved to file. The loop is repeated for bootstrap iterations if the lta() input bootstraps is not NULL and is a number greater than 0. The differences in these bootstrap iterations from the original estimate routine are as follows: Before the detection function is modeled, bootstrapped versions of the segments and sightings are generated using the LTabundR function prep_bootstrap_datasets(). This function (1) determines the number of segments occurring within each stratum; (2) resamples the segment IDs within each stratum (with replacement), to yield the same number of segments (though some segments may be replicated) and preserve the relative distribution of effort across strata; then (3) gathers the sightings associated with each of those bootstrapped segments. If a segment is counted twice, two copies of its respective sightings are used. These new versions of the segments and sightings are used to model the detection function, and are then filtered to estimate density/abundance. Before abundance is estimated, a new g(0) value is drawn randomly from a distribution that is based on the parameters stored in the reference table described in step 3 above. The code for doing this is copied directly from the routine written by Jay Barlow’s for his 2006 report. After abundance is estimated, details of the bootstrap iteration are saved and the results_file is updated. At the end of all bootstrap iterations, a summary of the bootstrap iteration process is generated and the results_file is updated. The CV of g(0) in this summary table is based on the bootstrapped values of g(0). The confidence intervals for density/abundance in this summary table are generated using the LTabundR function lta_ci(). Other details Area estimation {-} Unless you manually specify the study area in your estimates list, lta() will calculate your study area for you based on the geostrata you provide. It does so by calling the LTabundR function strata_area(), which you can use on your own to explore geostratum combination options. This function was designed using the sf package to handle complex polygon combinations, and it uses Natural Earth datasets to remove land within your study area (this is a feature you can turn off, if you want). Here are some examples of how strata_area() handles complex scenarios. Say you want to estimate abundance in the ‘WHCEAS’ study area, but you want to make sure the study area estimate is accurately removing land: demo &lt;- strata_area(strata_all = cruz$settings$strata, strata_keep = c(&#39;WHICEAS&#39;), verbose = FALSE) Say you want to estimate abundance in the pelagic Hawaiian EEZ, ignoring effort and sightings within the Main Hawaiian Islands stratum and accurately removing the area of the small Northwestern Hawaaian Islands: demo &lt;- strata_area(strata_all = cruz$settings$strata, strata_keep = c(&#39;HI_EEZ&#39;), strata_remove = c(&#39;MHI&#39;), verbose = FALSE) strata_all = cruz$settings$strata strata_keep = c(&#39;HI_EEZ&#39;) strata_remove = c(&#39;MHI&#39;) verbose = FALSE Say you want to estimate abundance of pelagic bottlenose dolphins within the WHICEAS study area, ignoring the insular stocks: demo &lt;- strata_area(strata_all = cruz$settings$strata, strata_keep = c(&#39;WHICEAS&#39;), strata_remove = c(&#39;Bottlenose_KaNi&#39;,&#39;Bottlenose_OUFI&#39;,&#39;Bottlenose_BI&#39;), verbose = FALSE) Say you want to estimate abundance for only the insular bottlenose stocks: demo &lt;- strata_area(strata_all = cruz$settings$strata, strata_keep = c(&#39;Bottlenose_KaNi&#39;,&#39;Bottlenose_OUFI&#39;,&#39;Bottlenose_BI&#39;), verbose = FALSE) Say you want to estimate abundance for false killer whales within the Northwestern Hawaiian Islands and Main Hawaiian Islands study areas combined, but those geostrata partially overlap: demo &lt;- strata_area(strata_all = cruz$settings$strata, strata_keep = c(&#39;MHI&#39;,&#39;NWHI&#39;), verbose = FALSE) Say you want to estimate abundance for the Hawaiian EEZ outside of those partially overlapping geostrata: demo &lt;- strata_area(strata_all = cruz$settings$strata, strata_keep = &#39;HI_EEZ&#39;, strata_remove = c(&#39;MHI&#39;,&#39;NWHI&#39;), verbose = FALSE) g(0) estimation If you want lta() to calculate a weighted g(0) estimate (and associated CV) that is specific to the conditions associated with your estimates sub-list parameters, all you need to do is provide the Rg0 input. When you do this, the lta() function will find the Rg0 values associated with the species code(s) in your estimates sub-list, then calculate weighted g(0) and its CV using the LTabundR function, g0_weighted(), which we discussed and demonstrated on the previous page. If lta() can’t find your species code in the Rg0 table you provide, it will give up and assume that g(0) is 1.0 and that g0_cv is 0.0. If your estimates sub-list has a alt_g0_spp slot, lta() will use that species code instead to filter the Rg0 table. If your estimates sub-list has a combine_g0 slot that is TRUE, lta() will filter the Rg0 table using all species codes you provide. If that filtration results in multiple Rg0 species being found, weighted g(0) will be calculated for each of those species separately, then those g(0) estimates will be combined using a geometric mean (using the LTabundR function g0_combine()). If combine_g0 is FALSE, only the first species code provided in your estimates sub-list will be used to filter Rg0. If you want to supply a weighted g(0) estimate and its CV yourself, you can add the g0 and g0_cv slots to your estimates sublist, as explained above. If you want to coerce g(0) to be assumed to be 1.0 (with CV = 0.0), you can either (1) not supply the Rg0 input, or (2) manually specify the g0 and g0_cv slots in your estimates sub-list accordingly. Fitting a detection function {-} The detection function is estimated using functions in the package mrds, primarily the main function mrds::ddf(), which uses a Horvitz-Thompson-like estimator to predict the probability of detection for each sighting. If multiple base key functions (e.g., half-normal or hazard-rate) are provided, and/or if covariates are specified, model fitting is done in a forward stepwise procedure: In the first round, the base model (no covariates, i.e., \"~1\") is fit first. In the second round, each covariate is added one at a time; at the end of the round, the covariate, if any, that produces the lowest AIC below the AIC from the previous round is added to the formula. This process is repeated in subsequent rounds, adding a new covariate term in each round, until the AIC no longer improves. If a second base key is provided, the process is repeated for that second key. All models within delta_aic of the model with the lowest AIC qualify as best-fitting models. The best-fitting model(s) is(are) then used to estimate the Effective Strip half-Width (ESW) based on the covariates associated with each sighting. If multiple best-fitting models occur, we will find the average ESW for each sighting across all models, using a weighted mean approach in which we weight according to model AIC. To turn off this model averaging step, set delta_aic to 0 to avoid passing multiple models to the abundance estimation stage. Note that if lnsstot is included as a covariate, the function will (1) check to see if the sightings dataframe has a column named ss_valid (all cruz objects do), then, if so, (2) filter sightings only to rows where ss_valid is TRUE, meaning the group size estimate for that sighting is a valid estimate. This means that the sample size used for fitting a detection function could be reduced if there are sightings with ss_valid = FALSE and lnsstot is used as a covariate in the fitting routine. However, this will not affect the sample size used for abundance estimation in the subsequent step (see details below). This stage of the lta() command is executed within a backend function, LTabundR::fit_df(), which has its own documentation for your reference. Estimating density &amp; abundance Estimates are produced for various combinations of species, regions, and years, according to the arguments specified in your estimates list(s). Before these estimates are produced, we filter the data used to fit the detection function to strictly systematic (design-based) effort (i.e., EffType = \"S\"), in which standard protocols are in use (i.e., OnEffort = TRUE) and the Beaufort sea state is less than 7 (though these controls can be modified using the lta() inputs abund_eff_types and abund_bft_range (see above). Note that if sightings has a column named ss_valid (all standard cruz objects do) and if any of the rows in that column are FALSE, those rows will have their best group size estimate (which will be NA or 1, since they are invalid) replaced by the mean best estimate for their respective species in the year for which abundance is being estimated. Currently the data used for that mean estimate are not specific to a given region, just the year of the abundance estimate. This stage of the lta() command is executed within a back-end function, LTabundR::abundance(), which has its own documentation for your reference. Bootstrap variance estimation If the bootstraps input value is greater than 1, bootstrap variance estimation will be attempted. In each bootstrap iteration, survey segments are re-sampled with replacement before fitting the detection function and estimating density/abundance. Re-sampling is done in a routine that preserves the proportion of segments from each geostratum. Note that the entire process is repeated in each bootstrap: step-wise fitting of the detection function, averaging of the best-fitting models, and density/abundance estimation for all species/region/year combinations specified in your estimates input. At the end of the bootstrap process, results are summarized for each species/region/year combination. 95% confidence intervals are calculated using the BCA method (package coxed, function bca()). g(0) values during bootstrapping When conducting the non-parametric bootstrap routine to estimate the CV of density and abundance, uncertainty is incorporated into the g(0) value in each iteration using a parametric bootstrapping subroutine: First, a logit-transformed distribution is modeled based upon the mean and CV of g(0) provided by the user in the estimates input (see documentation for LTabundR::g0_optimize() for details on this step). This modeled distribution is used to randomly draw a g(0) value for each iteration of the density/abundance bootstrap routine. In this way, the uncertainty in g(0) is propagated into uncertainty in density/abundance. Returned estimates for weighted g(0) Note that the results object returned by lta() (we will refer to this object as lta_result here) contains various g(0) estimates in various places. Here is a key: In lta_result$estimate, you will find the columns g0_small (the weighted g(0) estimate for small group sizes) and g0_large (the weighted g(0) estimate for large group sizes). These values may differ if you manually provided different weighted g(0) estimates for small and large schools. In most cases, however, the two values will be the same. You will also find the column g0_est, which is the average g(0) of the detections used for the point estimate. This can differ from g0_small and g0_large since some schools may be large and some may be small. You will also find g0_cv_small and g0_cv_large which are the CV’s of g0_small and g0_large, respectively, that are estimated using a Monte Carlo Markov Chain routine described in the Step-by-Step outline provided above. If small vs large estimates of g(0) do not differ, the CVs will be the same. These four values (g0_small, g0_large, g0_cv_small, and g0_cv_large) are the weighted g(0) estimates reported by the LTabundR functions provided for summarizing your results (e.g., lta_diagnostics() and lta_report()). In lta_result$bootstrap$summary, you will find the columns g0_mean and g0_cv, which report the mean and CV, respectively, of the parametric bootstrap values of g(0) used in each bootstrap iteration. These should be similar to the g(0) values you find in lta_result$estimate but will maybe not exactly equal. In lta_result$bootstrap$details, you will find a column g0_est which provides the bootstrapped g(0) value used in each iteration of the bootstrap routine. "],["destratify.html", "9 Stratified analysis", " 9 Stratified analysis Field surveys are sometimes stratified such that trackline design and/or density can differ substantially across regions. Also, analysts may sometimes wish to estimate density/abundance for individual regions separately, regardless of design stratification. On the previous page, we demonstrated how to accommodate a stratified analysis by providing an estimates sub-list for each geostratum. For example, in 2002 the Hawaiian EEZ was surveyed with different effort intensity in the Main Hawaiian Islands region compared to pelagic waters. Here is the code that generates density/abundance estimates of striped dolphins in 2002 (stratified) and 2010 (unstratified), with only 10 bootstrap iterations: # Survey data data(&quot;cnp_150km_1986_2020&quot;) cruz &lt;- cnp_150km_1986_2020 # Rg0 table data(&quot;g0_results&quot;) Rg0 &lt;- g0_results # Detection function filters fit_filters &lt;- list(spp = c(&#39;013&#39;, &#39;026&#39;, &#39;031&#39;), pool = &#39;Multi-species pool 1&#39;, cohort = &#39;all&#39;, truncation_distance = 5, other_species = &#39;remove&#39;) # Detection function settings df_settings &lt;- list(covariates = c(&#39;bft&#39;,&#39;lnsstot&#39;,&#39;cruise&#39;,&#39;year&#39;,&#39;ship&#39;,&#39;species&#39;), covariates_factor = c(FALSE, FALSE, TRUE, TRUE, TRUE, TRUE), covariates_levels = 2, covariates_n_per_level = 10, detection_function_base = &#39;hn&#39;, base_model = &#39;~1&#39;, delta_aic = 2) # Estimates estimates &lt;- list( list(spp = &#39;013&#39;, title = &#39;Striped dolphin&#39;, years = 2002, regions = &#39;MHI&#39;), list(spp = &#39;013&#39;, title = &#39;Striped dolphin&#39;, years = 2002, regions = &#39;HI_EEZ&#39;, regions_remove = &#39;MHI&#39;), list(spp = &#39;013&#39;, title = &#39;Striped dolphin&#39;, years = 2010, regions = &#39;HI_EEZ&#39;)) # Run it results &lt;- lta(cruz, Rg0, fit_filters, df_settings, estimates, bootstraps = 10) # Save it locally saveRDS(results, file=&#39;lta/multispecies_pool_1.RData&#39;) Let’s read these results back in using the LTabundR function lta_enlist(), which stores LTA results in a flexible list structure. ltas &lt;- lta_enlist(&#39;lta/&#39;) As these results stand, 2002 estimates are stratified into 2 separate regions: (ltas %&gt;% lta_report(verbose = FALSE))$table4 2002 (HI_EEZ) - (MHI) 2002 1 Species or category Density Abundance CV 95% CI Density 2 Striped dolphin 19.64 44,436 0.46 28,104-112,414 17.49 (MHI) 2010 (HI_EEZ) 1 Abundance CV 95% CI Density Abundance CV 95% CI 2 3,709 1.40 372-23,787 32.29 79,911 0.37 40,538-162,150 Now let’s process these LTA results through an LTabundR function, lta_destratify(), which will combine the separate regional estimates from 2002 into a single estimate for the year. ltas_2a &lt;- lta_destratify(ltas, years = 2002, combine_method = &#39;arithmetic&#39;, new_region = &#39;(HI_EEZ)&#39;) The new_region argument specifies how to refer to the combined region. In this case we want the 2002 study area to be named the same as the unstratified 2010 study area, hence \"(HI_EEZ)\". The combine_method argument is explained below. Now let’s re-check the summary table: (ltas_2a %&gt;% lta_report(verbose=FALSE))$table4 2002 (HI_EEZ) 2010 (HI_EEZ) 1 Species or category Density Abundance CV 95% CI Density Abundance 2 Striped dolphin 19.46 48,145 1.29 6,896-336,137 32.29 79,911 1 CV 95% CI 2 0.37 40,538-162,150 There is now only one set of columns for 2002, and the values therein are combinations of the stratified regions. The “de-stratification” routine within lta_destratify() sums abundance estimates across regions to get combined abundance. To estimate density in the combined regions, the function uses weighted averaging in which the area of a geostratum serves as its weight. To estimate CV and confidence interval of the combined result, the function uses one of two combine_methods (this is an argument in the function). The default method is \"arithmetic\", which uses classic formulae to estimate the combined variance and the corresponding confidence interval. This is done in a way that allows multiple geostrata to be combined, not just two. The second option, \"bootstrap\", uses an iterative method that draws bootstrap samples, with replacement, from the bootstrap estimate of density within each stratified region. ltas_2b &lt;- lta_destratify(ltas, years = 2002, combine_method = &#39;bootstrap&#39;, new_region = &#39;(HI_EEZ)&#39;) (ltas_2b %&gt;% lta_report(verbose=FALSE))$table4 2002 (HI_EEZ) 2010 (HI_EEZ) 1 Species or category Density Abundance CV 95% CI Density Abundance 2 Striped dolphin 19.46 48,145 0.42 14,475-108,926 32.29 79,911 1 CV 95% CI 2 0.37 40,538-162,150 Note that use of lta_destratify() only makes sense if the stratified regions have zero overlap. "],["trend-analysis.html", "10 Trend analysis", " 10 Trend analysis When a species’ density appears to change dramatically from one survey year to the next, it could be due to several factors: the species’ abundance may have changed; its range may have shifted; or the timing of its migratory movements may have shifted. This apparent change could also be due solely to random chance: you can sample the exact same population in two different surveys, and you are liable to produce different abundance estimates due simply to random variation in how often you encounter your target species. In other words, random variation in the encounter rate may lead you to estimate a change in abundance, when in fact there is no change. For this reason, whenever you suspect that abundance has changed between years – i.e., whenever the confidence intervals for two years do not overlap – it is good practice to carry out follow-up tests. One such test was developed in (Bradford et al. 2020 and Bradford et al. (2021). That test has been provided in LTabundR with the function er_simulator(), which refers to a simulation-based test of random variation in the encounter rate (ER). This function uses randomization simulations to test for the probability that year-to-year changes observed in a species’ encounter rate are due to random sampling variation (and not actual change in the encounter rate). More specifcially, this function uses bootstrap sampling of survey segments to see if random variation in sampling could possibly produce an apparent but immaterial change in encounter rate across years. You will find full analytical details in the Appendix to Bradford et al. (2020) for analytical details, but briefly: in each bootstrap iteration, survey segments are resampled in a way that preserves the proportion of effort occurring within each geostratum in the data. The resampled data are used to calculate the overall ER across all survey years, since the null hypothesis is that the ER does not change across years. This overall ER is used to predict the number of sightings in each year, based on the distance covered by the resampled segments in each year. This process is repeated (typically hundreds to thousands of times) to produce a distribution of predicted sighting counts in each year. This distribution reflects the range of ERs that could be possible due simply to random variation and not to underlying changes in abundance. These distributions are compared to the actual number of sightings observed in their respective year. The fraction of simulated sightings counts that are more extreme than the observed count reflects the probability that the observed count is due to random sample variation alone. For example, Bradford et al. (2021) found non-overlapping confidence intervals in their estimates of Bryde’s whale abundance in 2002, 2010, and 2017. To test for the significance of these trends, they carried out the “ER simulator” routine described above. In LTabundR, we would carry out the same analysis as follows: Take your processed data: data(cnp_150km_1986_2020) cruz &lt;- cnp_150km_1986_2020 Filter it to systematic effort in the years of interest: cruzi &lt;- filter_cruz(cruz, analysis_only = TRUE, years = c(2002, 2010, 2017), regions = &#39;HI_EEZ&#39;, eff_types = &#39;S&#39;, bft_range = 0:6) Conduct the ER simulation, passing the species code for the Bryde’s whale (\"072\"). er_results &lt;- er_simulator(spp = &#39;072&#39;, cruz = cruzi, iterations = 1000, verbose = FALSE) This routine provides a list with three slots: er_results %&gt;% names [1] &quot;summary&quot; &quot;details&quot; &quot;p&quot; The summary slot returns the p-value for each year, i.e., the chances that the observed number of sightings was due purely to random variation in the encounter rate. er_results$summary years observed p 1 2010 19 0.000 2 2002 10 0.596 3 2017 2 1.000 In this example, the encounter rates observed in 2002 and 2010 are very likely due to some process other than random variation in the encounter rate, such as range shifts, seasonal movement timing shifts, and/or changes in abundance. However, the observed encounter rate in 2017 could easily be explained by random variation in the ER. The details slot returns the simulation predictions for each year: er_results$details %&gt;% head year p obs sim 1 2010 0 19 9.544628 2 2010 0 19 10.259973 3 2010 0 19 9.590184 4 2010 0 19 7.233890 5 2010 0 19 13.285679 6 2010 0 19 7.543467 er_results$details %&gt;% tail year p obs sim 2995 2017 1 2 8.278693 2996 2017 1 2 7.249285 2997 2017 1 2 10.514959 2998 2017 1 2 10.790559 2999 2017 1 2 10.730847 3000 2017 1 2 7.336112 The p slot returns a faceted histogram of the results: er_results$p "],["subgroups2.html", "11 Subgroup-based analysis Manual data updates Inputs to lta_subgroup() Running lta_subgroup() Outputs Behind the scenes", " 11 Subgroup-based analysis False killer whales (Pseudorca crassidens) are rare and occur in dispersed subgroups, which complicates conventional distance sampling approaches to line-transect analysis (Bradford et al. 2014). To better estimate their abundance in Hawaiian waters, the Pacific Islands Fisheries Science Center initiated a sub-group protocol referred to as the “PC Protocol”, a reference to the species’ scientific name. Data collected using the PC Protocol is then analyzed using a subgroup-based analytical approach (Bradford et al. 2014, 2020). An additional complication is that false killer whales in Hawaiian waters belong to three discrete populations – the Main Hawaiian Islands insular population, the Northwestern Hawaiian Islands (NWHI) population, and a pelagic population – whose ranges partially overlap, which means that population assignment cannot always be based simply on the geographic location of sightings. When geographic assignment of population is not possible, biopsy-sampled genetics or photo-identification inference, if available, is used to assign each sighting to a population post-hoc. To accommodate these special circumstances for false killer whales (and potentially other species with subgroup structure) with an appropriate balance of flexibility and efficiency, LTabundR includes a function named lta_subgroup(), whose use will look something like this: lta_subgroup(df_sits, truncation_distance, ss, density_segments, density_das, density_sightings, Rg0= NULL, cruz10 = NULL, g0_spp = NULL, g0_truncation = NULL, g0_constrain_shape = FALSE, g0_jackknife_fraction = 0.1, abundance_area = NULL, iterations = 5000, output_dir = NULL, toplot = TRUE, verbose = TRUE Note that there are several required inputs (without any defaults) as well as many optional inputs (with defaults provided). We will step through each of these inputs below, using a case study in which we estimate the abundance of the pelagic false killer whale population in the Hawaiian EEZ for 2017 (Bradford et al. 2020). Reminder on data processing LTabundR has been designed to make a first-pass attempt at processing subgroup data resulting from the PC protocol. We recommend that you review the details of how subgroup data are processed within the LTabundR framework. For convenience, below we provide the minimum data processing code needed to reproduce the analysis we present here. # Local path to DAS file das_file &lt;- &#39;data/surveys/CenPac1986-2020_Final_alb.das&#39; # Strata to use data(strata_cnp) strata_cnp %&gt;% names [1] &quot;HI_EEZ&quot; &quot;OtherCNP&quot; &quot;MHI&quot; &quot;WHICEAS&quot; [5] &quot;Spotted_OU&quot; &quot;Spotted_FI&quot; &quot;Spotted_BI&quot; &quot;Bottlenose_KaNi&quot; [9] &quot;Bottlenose_OUFI&quot; &quot;Bottlenose_BI&quot; &quot;nwhi_fkw&quot; &quot;mhi_fkw&quot; my_strata &lt;- strata_cnp[c(1, 2, 11, 12)] my_strata %&gt;% names [1] &quot;HI_EEZ&quot; &quot;OtherCNP&quot; &quot;nwhi_fkw&quot; &quot;mhi_fkw&quot; # Survey-wide settings data(species_codes) data(ships) data(group_size_coefficients) survey &lt;- load_survey_settings( out_handling = &#39;remove&#39;, min_row_interval = 2, max_row_interval = 3600, max_row_km = 100, km_filler = 1, speed_filler = 10 * 1.852, segment_method = &quot;equallength&quot;, segment_target_km = 150, segment_max_interval = 24, segment_remainder_handling = c(&quot;segment&quot;), ship_list = ships, species_codes = species_codes, group_size_coefficients = group_size_coefficients, smear_angles = FALSE) # Cohort-specific settings all_species &lt;- load_cohort_settings( id = &quot;all&quot;, probable_species = FALSE, sighting_method = 0, cue_range = 0:7, school_size_range = c(0, 10000), school_size_calibrate = TRUE, calibration_floor = 0, use_low_if_na = TRUE, io_sightings = 0, geometric_mean_group = TRUE, truncation_km = 7.5, beaufort_range = 0:6, abeam_sightings = FALSE, strata_overlap_handling = c(&quot;smallest&quot;), distance_types = c(&#39;S&#39;,&#39;F&#39;,&#39;N&#39;), distance_modes = c(&#39;P&#39;,&#39;C&#39;), distance_on_off = TRUE) # Gather settings settings &lt;- load_settings(strata = my_strata, survey = survey, cohorts = list(all_species)) # Process data cruz &lt;- process_surveys(das_file, settings) This cruz object has the geostrata relevant to the subgroup analysis we are replicating here: cruz$strata stratum area 1 HI_EEZ 2474595.8 [km^2] 2 OtherCNP 33817779.1 [km^2] 3 nwhi_fkw 449375.6 [km^2] 4 mhi_fkw 171283.8 [km^2] The all cohort for this cruz object has a subgroups slot… cruz$cohorts$all %&gt;% names [1] &quot;segments&quot; &quot;das&quot; &quot;sightings&quot; &quot;subgroups&quot; … which has three slots inside it: cruz$cohorts$all$subgroups %&gt;% names [1] &quot;sightings&quot; &quot;subgroups&quot; &quot;events&quot; The events slot is the closest thing to raw DAS data: each row is a subgroup size estimate from a single observer. The subgroups slot has a line for each subgroup, with the subgroup size averaged across observers. The sightings slot has a line for each sighting within each phase of the PC protocol, with group size representing the sum of all subgroups in the respective phase. Here are all the columns within the events slot: cruz$cohorts$all$subgroups$events %&gt;% names [1] &quot;Cruise&quot; &quot;ship&quot; &quot;Date&quot; &quot;DateTime&quot; [5] &quot;Lat&quot; &quot;Lon&quot; &quot;OnEffort&quot; &quot;EffType&quot; [9] &quot;Bft&quot; &quot;SwellHght&quot; &quot;RainFog&quot; &quot;HorizSun&quot; [13] &quot;VertSun&quot; &quot;Glare&quot; &quot;Vis&quot; &quot;ObsL&quot; [17] &quot;Rec&quot; &quot;ObsR&quot; &quot;ObsInd&quot; &quot;SightNo&quot; [21] &quot;Obs_Sight&quot; &quot;Species&quot; &quot;Line&quot; &quot;SubGrp&quot; [25] &quot;Event&quot; &quot;GSBest&quot; &quot;GSH&quot; &quot;GSL&quot; [29] &quot;Angle&quot; &quot;RadDist&quot; &quot;seg_id&quot; &quot;stratum_HI_EEZ&quot; [33] &quot;stratum_OtherCNP&quot; &quot;stratum_nwhi_fkw&quot; &quot;stratum_mhi_fkw&quot; &quot;stratum&quot; [37] &quot;ObsStd&quot; &quot;Obs&quot; &quot;PerpDist&quot; &quot;sgid&quot; [41] &quot;sitid&quot; &quot;phase&quot; &quot;population&quot; &quot;pop_prob&quot; Note that each subgroup has automatically been assigned to a phase of the PC protocol: cruz$cohorts$all$subgroups$events$phase %&gt;% head(20) [1] 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 And that each subgroup has been given a blank placeholder of NA in the population column: cruz$cohorts$all$subgroups$events$population %&gt;% head() [1] NA NA NA NA NA NA Manual data updates In most cases the processed subgroup data will need to be reviewed carefully and it is probable that some manual edits will be necessary. To facilitate that process, LTabundR offers functions that allow the user to review the processed data, stage reproducible edits using code, and re-process the data to apply those edits. In short, here are the functions related to each step in this editing workflow: To review the default processing applied to the subgroup data when you ran process_surveys(), use the function subgroup_explorer(). To edit the way subgroup data are processed – e.g., assigning detections to certain populations or changing the phase of the PC protocol for a detection – use the functions subgroup_populations() and subgroup_edits(). Collect the outputs of those functions into a list, then … To re-process the subgroup data with your list of staged edits, run process_subgroups() and pass your list to the input named edits. We provide details for each of those steps in the following subsections. Reviewing processed subgroup data For reviewing processed subgroup data, use the subgroup_explorer() function. subgroup_explorer(cruz, cohort=&#39;all&#39;) This function launches an interactive Shiny dashboard for exploring the processed subgroups events. It will look like this: To stage code for reproducible edits to the processed data, two functions are available: subgroup_populations() and subgroup_edits(). We review those functions below, then show how to apply those coded edits by re-processing the subgroup data. Staging subgroup edits subgroup_populations() The function subgroup_populations() allows you to batch-assign subgroup events to certain populations based upon spatial polygons. Its use looks like this: subgroup_populations(populations, cruz, cohort=&#39;all&#39;, default_pop = &#39;pelagic&#39;) In this function, the populations input is a named list of polygon coordinates, very similar to the way strata are supplied in the settings for process_surveys(). Here’s an example of what that list could look like if you chose to use some of the strata in your cruz settings: # Pull polygons from settings mhi_fkw &lt;- cruz$settings$strata$mhi_fkw nwhi_fkw &lt;- cruz$settings$strata$nwhi_fkw # Example of data structure mhi_fkw %&gt;% head Lon Lat 1 -155.6959 18.26111 2 -155.6966 18.26112 3 -155.6995 18.26118 4 -155.7002 18.26120 5 -155.7016 18.26123 6 -155.7023 18.26125 # Create populations object populations &lt;- list(MHI = mhi_fkw, NWHI = nwhi_fkw) The name of each list slot will be treated as the name of the population occurring within the respective polygon. When we used these polygons as the population boundaries, we get the following variety of population assignments and probabilities: pops &lt;- subgroup_populations(populations, cruz, cohort = &#39;all&#39;, default_pop = &#39;pelagic&#39;) pops$population %&gt;% table . MHI MHI;NWHI NWHI pelagic 127 47 74 141 pops$pop_prob %&gt;% table . 0.5;0.5 1 47 342 When you run the function subgroup_populations(), it returns a data.frame of instructions in which each row is a population assignment for a subgroup event: subgroup_populations(populations, cruz, cohort=&#39;all&#39;, default_pop = &#39;pelagic&#39;, verbose=FALSE) %&gt;% head # A tibble: 6 × 5 edit cohort sgid population pop_prob &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; 1 population all 1108-20111026-004-A pelagic 1 2 population all 1108-20111026-004-B pelagic 1 3 population all 1108-20111026-004-C pelagic 1 4 population all 1108-20111026-004-D pelagic 1 5 population all 1108-20111026-004-D pelagic 1 6 population all 1108-20111026-004-SA pelagic 1 This data.frame is formatted exactly as needed to pass these edits to the re-processing routine process_subgroups(), as shown below. The column population contains the population assignment, and pop_prob contains the probability that the event is assigned to this population. The latter will be 1.0 unless the event occurs in overlapping polygons, in which case the population column will display all eligible population names separated by a semicolon (e.g., if there are two overlapping polygons named \"MHI\" and \"NWHI\", the population will be shown as \"MHI;NWHI\"), and the pop_prob column will display an equal probability of assignment for each of those populations (e.g., \"0.5;0.5\"). If an event does not occur within any of the polygons provided, it will be given the default_pop name, with a default pop_prob of 1.0. Note that if your data do not have population structure, you can skip this step and proceed. Stage specific edits To manually stage specific edits to the processed subgroup data, use the subgroup_edits() function. Its use will look something like this, though the latter inputs depend on the type of edit you are staging: subgroup_edits(cohort = &#39;all&#39;, sgid = &#39;1108-20111026-004-A&#39;, ...) # remaining inputs depend on the type of edit The input sgid refers to the unique subgroup ID assigned, which you can find when you look at the subgroup data with subgroup_explorer() (described above). You can supply a single sgid or a vector of `sgid values. This subgroup_edits() function returns a list of staged edits that can be passed directly to the subgroup re-processing routine we explain further below. Edit population If you need to fix population assignments on a case-by-case basis after using the batch-assignment function, subgroup_populations() discussed above, use the following syntax: subgroup_edits(cohort = &#39;all&#39;, sgid = &#39;1108-20111026-004-A&#39;, population = &#39;MHI;NWHI&#39;, pop_prob = &#39;0.5;0.5&#39;) The population input takes a character vector, with length of either 1 or the same as sgid, indicating the new population(s) to assign the sgid(s). If you want a sgid to be eligible for multiple populations, separate the population names with a semicolon. Both this input and pop_prob needs to be provided in order for the edit to work properly. The pop_prob input takes a vector, with length of either 1 or the same as sgid and values ranging between 0 and 1, indicating the probability of each population assignment for the sgid(s). If you want a sgid to be eligible for multiple populations, provide a character vector and separate the probabilities by a semicolon, e.g., \"0.5;0.5\". Both this input and population need to be provided in order for the edit to work properly. Edit phases If you need to fix the phase assignment for subgroup event(s), use the following syntax: subgroup_edits(cohort = &#39;all&#39;, sgid = &#39;1108-20111026-004-A&#39;, phase = 2) The input phase takes a numeric vector, with length of either 1 or the same as sgid, indicating the new phase to assign the sgid(s). If you wish to remove a phase assignment for a sgid, use NA. Edit primary observer If you want to control whether a subgroup event was seen by the primary/standard observer (the column ObsStd is either TRUE or FALSE), use the following syntax: subgroup_edits(cohort = &#39;all&#39;, sgid = &#39;1108-20111026-004-A&#39;, ObsStd = TRUE) The input ObsStd takes a Boolean vector, with length of either 1 or the same as sgid, indicating the value for ObsStd for each sgid. Remove/exclude subgroup In rare cases you may need to delete a subgroup so that it does not affect any aspect of the analysis. To do so, use this syntax: subgroup_edits(cohort = &#39;all&#39;, sgid = &#39;1108-20111026-004-A&#39;, exclude = TRUE) The input exclude takes a Boolean vector, with length of either 1 or the same as sgid, indicating which sgid(s) to erase: TRUE means exclude, FALSE means keep. We recommend implementing these edits last, since the removal of a row of data may have downstream effects on how other edits are applied. Re-processing subgroup data Here is an arbitrary example of how a set of staged edits may look in your code: # Batch-edit population assignments ======== mhi_fkw &lt;- cruz$settings$strata$mhi_fkw nwhi_fkw &lt;- cruz$settings$strata$nwhi_fkw new_pops &lt;- subgroup_populations(populations = list(MHI = mhi_fkw, NWHI = nwhi_fkw), cruz, cohort = &#39;all&#39;, default_pop = &#39;pelagic&#39;) # Case-by-case edits ======================= cohort &lt;- &#39;all&#39; # Population changes edit1 &lt;- subgroup_edits(cohort=cohort, sgid = &#39;1108-20111026-004-A&#39;, population = &#39;space whales&#39;, pop_prob = &#39;1&#39;) # Phase changes edit2 &lt;- subgroup_edits(cohort=cohort, sgid = &#39;1108-20111026-004-D&#39;, phase = 2) # Multiple changes to same sgid edit3 &lt;- subgroup_edits(cohort=cohort, sgid = &#39;1108-20111026-004-C&#39;, phase=2, population = &#39;space whales&#39;, pop_prob = &#39;1&#39;), # Exclusion edit4 &lt;- subgroup_edits(cohort=cohort, sgid = &#39;1108-20111026-004-D&#39;, exclude = TRUE)) Now that you have these edits staged, you can put them into a listand re-process your subgroup data using the function process_subgroups(): edits &lt;- list(new_pops, edit1, edit2, edit3, edit4) cruz_revised &lt;- process_subgroups(cruz, edits = edits) You can use the subgroup_explorer() function to verify that changes were applied as intended. We will use most of these functions in the code below that prepares inputs for the lta_subgroups() function to replicate the false killer whale abundance estimates from HICEAS 2017 (Bradford et al. 2020). Inputs to lta_subgroup() df_sits This is a data.frame of sightings you want to use to fit the detection function model. For false killer whales in Bradford et al. (2020), this is a combination of filtered sightings prior to 2011 (made without the PC protocol, but where the sighting was considered to represent the first subgroup detection) and “Phase 1” subgroup detections from 2011 onwards (using the PC protocol). No filtering will be applied to these sightings within this function, so make sure you provide the data pre-filtered. Bradford et al. (2020) used a single detection function for all populations of false killer whales. This dataset needs to be filtered for use in the detection function model. On-Effort sightings from 1986-2010 were eligible for use, specifically: OnEffort is TRUE, EffType can be \"S\", \"F\", or \"N\"; no mixed-species schools; no sightings past beam; and only detections made by a primary observer. Here we draw those sightings from the above cruz object, filtering as needed, and to simplify we will select only a few key columns. # Start with a safe copy cruz_df &lt;- cruz sits_1986_2010 &lt;- cruz_df$cohorts$all$sightings %&gt;% filter(OnEffort == TRUE, year &gt;= 1986, year &lt;= 2010, OnEffort == TRUE, EffType %in% c(&#39;S&#39;, &#39;F&#39;, &#39;N&#39;), species == &#39;033&#39;, # code for false killer whales ObsStd == TRUE, mixed == FALSE, Bearing &lt;= 90 | Bearing &gt;= 270) %&gt;% select(DateTime, Lat, Lon, Cruise, PerpDistKm) %&gt;% arrange(DateTime) sits_1986_2010 %&gt;% nrow [1] 35 sits_1986_2010 %&gt;% head DateTime Lat Lon Cruise PerpDistKm 1 1986-11-13 09:43:00 10.466667 -139.2833 990 1.17493742 2 1987-08-19 15:30:00 12.050000 -133.3000 1080 2.24543067 3 1987-12-01 09:23:00 8.266667 -122.5500 1080 0.42589077 4 1989-08-22 06:45:00 11.800000 -141.7333 1268 0.40838431 5 1989-08-22 16:39:00 12.716667 -143.1000 1268 0.68815211 6 1989-09-10 17:18:00 7.350000 -129.5333 1268 0.07751339 We add one sighting from 2016 that is going to be treated like a pre-2011 sighting, since it was not made with the PC protocol: sit_1604_018 &lt;- cruz_df$cohorts$all$sightings %&gt;% filter(Cruise == 1604, SightNo == &#39;018&#39;) %&gt;% select(DateTime, Lat, Lon, Cruise, PerpDistKm) sit_1604_018 DateTime Lat Lon Cruise PerpDistKm 1 2016-07-04 15:45:21 21.66367 -158.269 1604 0.58338 To add to this pool of sightings, we also include Phase 1 subgroup detections from 2011 to 2017 based on the PC protocol. Specific criteria: PC Protocol is Phase 1, OnEffort is TRUE, EffType can be \"S\", \"F\", or \"N\"; no mixed-species schools; no sightings past beam; detection made by primary observer. Before we apply these filters, we need to make a few edits for a some special cases: # These subgroups should have a standard observer # according to ALB et al. (2020): edit1 &lt;- subgroup_edits(cohort = &#39;all&#39;, sgid = &#39;1705-20170912-056-B&#39;, ObsStd = TRUE) edit2 &lt;- subgroup_edits(cohort = &#39;all&#39;, sgid = &#39;1706-20170727-039-D&#39;, ObsStd = TRUE) edit3 &lt;- subgroup_edits(cohort = &#39;all&#39;, sgid = &#39;1706-20170912-119-K&#39;, ObsStd = TRUE) edit4 &lt;- subgroup_edits(cohort = &#39;all&#39;, sgid = &#39;1705-20170929-073-C&#39;, ObsStd = TRUE) # This subgroup was found to be a mixed-species after the fact # so it is excluded from the dataset: edit5 &lt;- subgroup_edits(cohort = &#39;all&#39;, sgid = &#39;1203-20120513-069-A&#39;, exclude = TRUE) # Now update the cruz object with these edits edits &lt;- list(edit1, edit2, edit3, edit4, edit5) cruz_df &lt;- process_subgroups(cruz_df, edits = edits) Now apply filter: sits_2011_2017 &lt;- cruz_df$cohorts$all$subgroups$subgroups %&gt;% filter(OnEffort == TRUE, lubridate::year(DateTime) &gt;= 2011, lubridate::year(DateTime) &lt;= 2017, ObsStd == TRUE, Angle &lt;= 90 | Angle &gt;= 270, Species == &#39;033&#39;, Phase == 1) %&gt;% select(DateTime, Lat, Lon, Cruise, PerpDistKm = PerpDist) %&gt;% arrange(DateTime) sits_2011_2017 %&gt;% nrow [1] 71 sits_2011_2017 %&gt;% head DateTime Lat Lon Cruise PerpDistKm 1 2011-10-26 13:16:06 7.240500 -164.9347 1108 1.4714466 2 2011-10-26 13:31:17 7.207333 -164.9572 1108 2.1211999 3 2011-10-26 13:48:26 7.169667 -164.9830 1108 0.9876483 4 2011-10-26 14:02:29 7.139000 -165.0043 1108 1.5731017 5 2013-05-13 07:09:54 24.302333 -168.3195 1303 1.6787670 6 2013-05-13 07:24:54 24.343500 -168.3315 1303 0.9470481 To create df_sits for detection function fitting, we combine these datasets together: df_sits &lt;- rbind(sits_1986_2010, sit_1604_018, sits_2011_2017) df_sits %&gt;% nrow [1] 107 To check that this sample size matches that used in Bradford et al. 2020 (n=100), we can preliminarily filter this set of sightings to those whose perpendicular distance from the trackline is within 4.5 km (see the next section for how this was determined). df_sits %&gt;% filter(PerpDistKm &lt;= 4.5) %&gt;% nrow [1] 100 truncation_distance The truncation distance, in km, will be applied during detection function model fitting. Typically the farthest 5 - 10% of sightings are truncated, but this needs to be balanced by sample size considerations. Tabulate detection distances using a LTabundR function: dists &lt;- summarize_distances(df_sits$PerpDistKm) %&gt;% select(-total_within) dists km_min_incl km_max_excl sightings percent_beyond total_beyond km_mid 1 0.0 0.0 4 96.2616822 103 0.00 2 0.0 0.5 28 70.0934579 75 0.25 3 0.5 1.0 16 55.1401869 59 0.75 4 1.0 1.5 17 39.2523364 42 1.25 5 1.5 2.0 10 29.9065421 32 1.75 6 2.0 2.5 5 25.2336449 27 2.25 7 2.5 3.0 6 19.6261682 21 2.75 8 3.0 3.5 5 14.9532710 16 3.25 9 3.5 4.0 3 12.1495327 13 3.75 10 4.0 4.5 6 6.5420561 7 4.25 11 4.5 5.0 2 4.6728972 5 4.75 12 5.0 5.5 1 3.7383178 4 5.25 13 5.5 6.0 0 3.7383178 4 5.75 14 6.0 6.5 1 2.8037383 3 6.25 15 6.5 7.0 1 1.8691589 2 6.75 16 7.0 7.5 0 1.8691589 2 7.25 17 7.5 8.0 1 0.9345794 1 7.75 18 8.0 8.5 1 0.0000000 0 8.25 Plot these options: ggplot(dists, aes(x=km_max_excl, y=percent_beyond)) + geom_point() + geom_path() + geom_hline(yintercept = 10, lty=2, color=&#39;firebrick&#39;) + ylab(&#39;Percent of sightings beyond this distance&#39;) + xlab(&#39;Perpendicular distance (km)&#39;) + scale_x_continuous(n.breaks = 10) + scale_y_continuous(limits = c(0, 100), n.breaks = 10) Based on these results, we will choose a truncation distance of 4.5 km. truncation_distance &lt;- 4.5 ss This is a numeric vector of subgroup sizes. The function will find this vector’s arithmetic mean and bootstrapped CV. In Bradford et al. (2020), these data come from Phase 1 and Phase 2 estimates of subgroup sizes from 2011 onwards. In the processed cruz object, each of those estimates is the geometric mean of repeat estimates from separate observers. Some of these estimates need to be removed for various reasons that were not captured in processing the DAS file. However, these subgroups may only need to be removed from the dataset used for subgroup size estimation – they may still be used in df_sits and other inputs as long as their details meet the respective criteria of each parameter. The vectors below provide the subgroup ID’s that should be removed for the purposes of subgroup size estimation: # Subgroup sighted by non-primary observer missed_by_primary &lt;- c(&#39;1706-20170912-119-M&#39;, &#39;1706-20170912-119-P&#39;, &#39;1706-20170913-122-C&#39;, &#39;1705-20170913-122-E&#39;, &#39;1706-20170917-133-G&#39;) # Subgroup occurred in a mixed species sighting mixed_spp &lt;- c(&quot;1203-20120513-069-A&quot;, &quot;1203-20120513-069-SB&quot;, &quot;1203-20120513-069-SC&quot;, &quot;1203-20120513-069-ZA&quot;, &quot;1203-20120513-069-ZC&quot;) # Phase assignments compromised phase_compromised &lt;- c(&quot;1303-20130526-059-B&quot;, &quot;1303-20130526-059-C&quot;, &quot;1303-20130526-059-D&quot;, &quot;1604-20160704-018-A&quot;, &quot;1705-20170820-016-A&quot;, &quot;1705-20170911-055-SA&quot;, &quot;1705-20171009-086-A&quot;, &quot;1705-20171102-116-A&quot;, &quot;1705-20171102-116-B&quot;, &quot;1705-20171117-136-A&quot;) # Subgroup fluidity subgroup_fluidity &lt;- c(&#39;1203-20120516-076-SA&#39;) We first remove these subgroup IDs from the raw subgroup data, found in the events slot in the subgroups slot of the cruz cohort: events &lt;- cruz$cohorts$all$subgroups$events events &lt;- events %&gt;% filter(! sgid %in% missed_by_primary, ! sgid %in% mixed_spp, ! sgid %in% phase_compromised, ! sgid %in% subgroup_fluidity) We then use the LTabundR function subgroup_subgroups() to turn these observer-species estimates into a single row for each subgroup, with subgroup sizes averaged using the geometric mean: subgroups &lt;- subgroup_subgroups(events) We can then use these subgroups to conduct additional filtering for each phase of the PC protocol. Phase 1 detections need to be within 4.5km, not past the beam, and made by a primary observer with a valid “best” estimate. Phase 2 detections need to be within 4.5km and have a valid “best” estimate. # Filters that apply to both phases ss &lt;- subgroups %&gt;% filter(lubridate::year(DateTime) &gt;= 2011, lubridate::year(DateTime) &lt;= 2017, GSBest_geom_valid == TRUE, stratum_OtherCNP == TRUE, Species == &#39;033&#39;, PerpDist &lt;= 4.5) # Additional Phase 1 filters ss_phase1 &lt;- ss %&gt;% filter(Phase == 1, Angle &lt;= 90) # (Ideally, ObsStd == TRUE would have been used to filter # for primary observer sightings, but the way data were collected # for a couple of sightings necessitated the manual way above) ss_phase1 %&gt;% nrow [1] 63 # Additional Phase 2 ss_phase2 &lt;- ss %&gt;% filter(Phase == 2) ss_phase2 %&gt;% nrow [1] 64 # Combine ss &lt;- rbind(ss_phase1, ss_phase2) # Pull out group size vector ss &lt;- ss %&gt;% pull(GSBest_geom) # Check sample size ss %&gt;% length [1] 127 ss [1] 5.00 7.00 1.00 1.00 1.00 4.00 4.47 1.00 1.00 5.00 1.00 1.00 2.00 4.12 7.09 [16] 2.83 3.00 2.88 2.00 1.00 1.00 1.00 1.00 1.00 2.00 1.00 2.00 1.00 1.00 2.00 [31] 3.63 2.00 2.00 8.00 4.00 1.00 1.00 4.00 2.00 2.00 2.00 1.00 1.00 2.00 1.00 [46] 1.00 1.00 1.00 1.00 2.00 2.00 1.73 7.48 1.41 1.00 3.00 1.41 1.00 1.00 2.83 [61] 2.00 1.00 1.00 1.00 3.66 1.26 1.00 1.73 4.76 1.73 1.00 3.78 4.00 1.00 1.00 [76] 1.00 1.00 1.00 2.00 3.46 4.24 2.00 1.00 1.00 1.00 1.86 2.47 1.41 2.52 1.26 [91] 2.00 6.00 4.16 2.00 1.00 3.00 5.00 5.67 2.00 2.00 4.00 4.47 2.00 2.00 2.00 [106] 4.47 2.00 1.41 4.47 1.00 1.00 3.17 2.88 2.00 2.00 6.00 5.00 8.12 2.38 1.00 [121] 1.00 2.00 1.00 2.00 2.83 3.00 2.00 The sample size in Bradford et al. (2020) is n=127 (63 from phase 1, 64 from phase 2). Rg0 This is a data.frame with estimates of Relative g(0) and its CV at each Beaufort sea state. If this input is left NULL, then these estimates will be produced by the function using the subsequent g0_ inputs below. If this input is not supplied and any of the subsequent g0_ inputs are missing, then g(0) will be assumed to be 1.0 with CV of 0.0. When you do supply this Rg0 input, the data.frame has three required columns: bft (Beaufort sea state, numbers between 0 and 6), Rg0 (Rg(0) estimates for each Beaufort state), and Rg0_CV (the CV of the Rg(0) estimate in each Beaufort state). Other columns are allowed but will be ignored. Here is an example of a valid Rg0 input based on the values reported for false killer whales in Bradford et al. (2020). (These numbers are also available in the built-in dataset data(barlow_2015)). Rg0 &lt;- data.frame(bft = 0:6, Rg0 = c(1, 1, 0.72, 0.51, 0.37, 0.26, 0.19), Rg0_CV = c(0, 0, 0.11, 0.22, 0.34, 0.46, 0.59)) Rg0 bft Rg0 Rg0_CV 1 0 1.00 0.00 2 1 1.00 0.00 3 2 0.72 0.11 4 3 0.51 0.22 5 4 0.37 0.34 6 5 0.26 0.46 7 6 0.19 0.59 Again: if you supply this input, then the following g0_ inputs will be ignored. cruz10 This is a processed cruz object with short segment lengths, ideally 10 km or less (hence the 10 in the input name). This cruz object will be used to estimate Rg(0), i.e., the relative trackline detection probability (see its chapter), using the following g0_ inputs. The built-in dataset we have already loaded, data(\"noaa_10km_1986_2020\") is already processed with 10-km segments. cruz10 &lt;- cruz g0_spp This and the following g0_ inputs will be used to model Relative g(0) estimates and their CV in various Beaufort sea states. If the previous input, Rg0 is provided, then these g0_ inputs will be ignored, and no Rg(0) modeling will take place. Furthermore, if any of these g0_ inputs are not provided, Rg(0) will be coerced to 1.0 with a CV of 0.0 for all sea states. This input, g0_spp, is a character vector of species code(s) to use to estimate Rg(0). In most cases this will be a single species, e.g., \"033\" for false killer whales. g0_spp &lt;- &#39;033&#39; g0_truncation The truncation distance to use when estimating Rg(0). In Bradford et al. (2020) this is 5.5 km. g0_truncation &lt;- 5.5 g0_constrain_shape Some Rg(0) curves will not decline monotonically due to sample size issues at low Bft (0-2) or high Bft (5-6) states. To coerce monotonic decline, set this input to TRUE, and the function will use a shape-constrained GAM (scam() from package scam) instead of a classic mgcv::gam(). g0_constrain_shape = FALSE g0_jackknife_fraction The proportion of data to leave out within each jackknife permutation. The default is 0.1 (i.e., 10% of the data, yielding 10 jackknife loops), after Barlow (2015). g0_jackknife_fraction = 0.1 density_segments The survey segments to be used in density/abundance estimation. For example, Bradford et al. (2020) used 150-km segments to estimate false killer whale density in the Hawaiian EEZ in 2017. For this we can use the same cruz we processed above and have been using throughout this chapter. Note that no filtering will be applied to these segments by the lta_subgroup() function, so we need to filter them ourselves first: we want only systematic-effort segments for the Hawaiian EEZ in 2017 (specifically, just cruises 1705 and 1706). cruzi &lt;- filter_cruz(cruz = cruz, analysis_only = TRUE, years = 2017, cruises = c(1705, 1706), regions = &#39;HI_EEZ&#39;, bft_range = 0:6, eff_types = &#39;S&#39;, on_off = TRUE) At this point we need to batch-assign detections to their respective populations. To do so, we draw stock boundary polygons from the cruz strata: mhi &lt;- cruz$settings$strata$mhi_fkw nwhi &lt;- cruz$settings$strata$nwhi_fkw populations &lt;- list(MHI = mhi_fkw, NWHI = nwhi_fkw) We then run the function subgroup_populations() to automatically assign each subgroup to a population based on their location within those polygons: # Stage edits pops &lt;- subgroup_populations(populations, cruzi, cohort=1, default_pop = &#39;pelagic&#39;, verbose=FALSE) Here is a look at what this batch of staged edits looks like: pops %&gt;% head # A tibble: 6 × 5 edit cohort sgid population pop_prob &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; 1 population 1 1705-20170921-064-A pelagic 1 2 population 1 1705-20170921-064-B pelagic 1 3 population 1 1705-20170921-064-B pelagic 1 4 population 1 1705-20170921-064-B pelagic 1 5 population 1 1705-20170929-073-A pelagic 1 6 population 1 1705-20170929-073-B pelagic 1 pops$population %&gt;% table . MHI MHI;NWHI NWHI pelagic 1 8 15 24 We also need to stage a few specific edits to the processed data, based on the some of the extenuating circumstances discussed in Bradford et al. (2020). Sighting 133 subgroups did not have biological data to assign them to NWHI, so they could be NWHI or pelagic according to an established probability (based on survey data): sit133 &lt;- subgroup_edits(cohort=1, sgid = c(&quot;1706-20170917-133-A&quot;, &quot;1706-20170917-133-B&quot;, &quot;1706-20170917-133-C&quot;, &quot;1706-20170917-133-D&quot;, &quot;1706-20170917-133-E&quot;, &quot;1706-20170917-133-F&quot;, &quot;1706-20170917-133-G&quot;), population = &#39;NWHI;pelagic&#39;, pop_prob = &#39;0.4;0.6&#39;) # Check it out sit133 [[1]] edit cohort sgid population pop_prob 1 population 1 1706-20170917-133-A NWHI;pelagic 0.4;0.6 2 population 1 1706-20170917-133-B NWHI;pelagic 0.4;0.6 3 population 1 1706-20170917-133-C NWHI;pelagic 0.4;0.6 4 population 1 1706-20170917-133-D NWHI;pelagic 0.4;0.6 5 population 1 1706-20170917-133-E NWHI;pelagic 0.4;0.6 6 population 1 1706-20170917-133-F NWHI;pelagic 0.4;0.6 7 population 1 1706-20170917-133-G NWHI;pelagic 0.4;0.6 Sighting 033 subgroups were automatically placed in the NWHI population, but biological data collected indicated they were from the pelagic population (Bradford et al. 2020). sit033 &lt;- subgroup_edits(cohort=1, sgid = c(&quot;1706-20170723-033-A&quot;, &quot;1706-20170723-033-B&quot;, &quot;1706-20170723-033-C&quot;, &quot;1706-20170723-033-D&quot;, &quot;1706-20170723-033-E&quot;), population = &#39;pelagic&#39;, pop_prob = &#39;1&#39;) # Check it out sit033 [[1]] edit cohort sgid population pop_prob 1 population 1 1706-20170723-033-A pelagic 1 2 population 1 1706-20170723-033-B pelagic 1 3 population 1 1706-20170723-033-C pelagic 1 4 population 1 1706-20170723-033-D pelagic 1 5 population 1 1706-20170723-033-E pelagic 1 The same goes for the subgroups in Sighting 122: sit122 &lt;- subgroup_edits(cohort=1, sgid = paste0(&quot;1706-20170913-122-&quot;, c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;D&quot;, &quot;E&quot;)), population = &#39;pelagic&#39;, pop_prob = &#39;1&#39;) Sighting 039 has a subgroup that was in fact seen by the primary observer: sit039 &lt;- subgroup_edits(cohort=1, sgid = &quot;1706-20170727-039-D&quot;, ObsStd = TRUE) The same goes for a subgroup in Sighting 073: sit073 &lt;- subgroup_edits(cohort=1, sgid = &quot;1705-20170929-073-C&quot;, ObsStd = TRUE) We can now re-process the subgroup data while applying these edits: cruzi &lt;- process_subgroups(cruzi, edits=list(pops, sit133, sit033, sit122, sit039, sit073), verbose=FALSE) Inspect the result: cruzi$cohorts$all$subgroups$events$population %&gt;% table . MHI NWHI;pelagic pelagic 1 10 37 cruzi$cohorts$all$subgroups$events$pop_prob %&gt;% table . 0.4;0.6 1 10 38 Now we are (finally!) ready to prepare segments and sightings for estimating the encounter rate. From this filtered cruz object, we will isolate the segments data: density_segments &lt;- cruzi$cohorts$all$segments density_das This is the complete survey data corresponding to the above segments. These data will be used to determine the proportion of survey effort occurring in each Beaufort sea state, which is needed to compute the estimate of weighted mean g(0) from the Rg(0) values. density_das &lt;- cruzi$cohorts$all$das density_sightings These are the encounters to use in density/abundance estimation. In Bradford et al. (2020), these were the Phase 1 detections of false killer whale subgroups within a given region and year. For this demonstration, our focus is the Hawaiian EEZ in 2017. Criteria are: OnEffort is TRUE, EffType is \"S\", within truncation distance, no abeam sightings, primary observer, pelagic and NWHI populations only. No filtering is applied to these sightings within the lta_subgroups() function, so make sure only the subgroups you wish to use are included and nothing more. density_sightings &lt;- cruzi$cohorts$all$subgroups$subgroups %&gt;% filter(lubridate::year(DateTime) == 2017, EffType == &#39;S&#39;, OnEffort == TRUE, PerpDist &lt;= truncation_distance, Angle &lt;= 90, ObsStd == TRUE, Species == &#39;033&#39;, Phase == 1, population %in% c(&#39;pelagic&#39;, &#39;NWHI;pelagic&#39;, &#39;NWHI&#39;)) # Sample size density_sightings %&gt;% nrow [1] 26 # Subgroups from each sighting density_sightings %&gt;% select(Cruise, Date, SightNo, population, pop_prob) %&gt;% arrange(Date) %&gt;% group_by(Cruise, Date, SightNo, population, pop_prob) %&gt;% summarize(n=n()) #%&gt;% pull(n) %&gt;% sum() # A tibble: 7 × 6 # Groups: Cruise, Date, SightNo, population [7] Cruise Date SightNo population pop_prob n &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; 1 1705 2017-09-21 064 pelagic 1 2 2 1705 2017-09-29 073 pelagic 1 6 3 1706 2017-07-23 033 pelagic 1 4 4 1706 2017-07-27 039 pelagic 1 4 5 1706 2017-09-13 122 pelagic 1 3 6 1706 2017-09-17 133 NWHI;pelagic 0.4;0.6 6 7 1706 2017-09-20 141 pelagic 1 1 Bradford et al. (2020) had a sample size of 23.6 for the pelagic stock in the Hawaiian EEZ. The fraction comes from the fact that one sighting (SightNo 133, which has 6 subgroups) has a prorated population designation, with a 0.6 probability that it belongs to the pelagic stock and a 0.4 probability that is belongs to the Northwest Hawaiian Islands (NWHI) stock. The 3.6 comes from 0.6 of the sighting's6` subgroups. abundance_area This is the area, in square km, of the region of interest. The density estimate will be scaled by this area. We have two options for finding this area. The first is to draw the area from our cohort$strata slot: cruz$strata$area[cruz$strata$stratum == &#39;HI_EEZ&#39;] 2474596 [km^2] The second is to calculate it ourselves using the LTabundR function strata_area(). This second option will be useful if your study area is a complicated combination/substraction of multiple geostrata. data(strata_cnp) abundance_area &lt;- strata_area(strata_all = strata_cnp, strata_keep = &#39;HI_EEZ&#39;)$km2 abundance_area [1] 2474596 Remaining inputs iterations: Number of iterations to use in the various CV bootstrapping procedures occurring throughout this function, specifically: effective strip width CV estimation, subgroup size CV estimation, weighted g(0) CV estimation, encounter rate estimation, and density/abundance estimation. output_dir: The path in which results RData files should be stored. If left ““, the current working directory will be used. toplot: A Boolean, with default FALSE, indicating whether to plot various aspects of the analysis. verbose: A Boolean, with default TRUE, indicating whether to print status updates to the Console. Running lta_subgroup() To demonstrate how the lta_subgroup() function works, we use it here without re-modeling the Relative g(0) parameters. We first estimate parameters for the Hawaii EEZ stratum. Our main interest there is the pelagic stock, but we need to include both populations (pelagic and NWHI), since one of the sightings has a prorated population assignment. After running lta_subgroup(), we will only focus on the results for the pelagic stock. We then estimate parameters for the NWHI stratum, then focus on the results for the NWHI stock. Hawaii EEZ stratum results_eez &lt;- lta_subgroup(df_sits = df_sits, truncation_distance = truncation_distance, ss = ss, density_segments = density_segments, density_das = density_das, density_sightings = density_sightings, Rg0 = Rg0, abundance_area = abundance_area, iterations = 5000, output_dir = &#39;subgroup_eez/&#39;, toplot = TRUE, verbose = TRUE) Northwest Hawaiian Islands stratum To re-run this for the other geostratum of interest in the Bradford et al. (2020), the Northwest Hawaiian Islands (NWHI), we make the following adjustments: # Filter DAS to rows within the NWHi stratum das_nwhi &lt;- density_das %&gt;% filter(stratum_nwhi_fkw == TRUE) # Use these rows to get segment IDs nwhi_segment_ids &lt;- das_nwhi$seg_id %&gt;% unique # Filter segments to those segment IDs segments_nwhi &lt;- density_segments %&gt;% filter(seg_id %in% nwhi_segment_ids) # Filter sightings to those within NWHI stratum sightings_nwhi &lt;- density_sightings %&gt;% filter(stratum_nwhi_fkw == TRUE) # Update abundance area area_nwhi &lt;- strata_area(strata_all = strata_cnp, strata_keep = &#39;nwhi_fkw&#39;)$km2 area_nwhi Now run lta_subgroup(): results_nwhi &lt;- lta_subgroup(df_sits = df_sits, truncation_distance = truncation_distance, ss = ss, density_segments = segments_nwhi, density_das = das_nwhi, density_sightings = sightings_nwhi, Rg0 = Rg0, abundance_area = area_nwhi, iterations = 5000, output_dir = &#39;subgroup_nwhi/&#39;, toplot = TRUE, verbose = TRUE) Outputs The function returns a list with many slots: results_eez %&gt;% names [1] &quot;estimate&quot; &quot;bft&quot; &quot;g0_details&quot; &quot;df&quot; &quot;bootstraps&quot; [6] &quot;iterations&quot; The first slot holds the most relevant results: # Hawaii EEZ results_eez$estimate %&gt;% filter(population == &#39;pelagic&#39;) population L n_segments g0 g0_cv ESW ESW_CV group 1 pelagic 15886.89 128 0.357 0.325 2.432073 0.1039014 2.370472 group_sd group_CV n ER ER_CV D CV 1 0.1447589 0.06106753 23.6 0.001485501 0.4340444 0.002027837 0.6987982 D_L95 D_U95 Area N N_L95 N_U95 1 0.0007568562 0.007646896 2474596 5018 1872.759 18923 # NWHI results_nwhi$estimate %&gt;% filter(population == &#39;NWHI&#39;) population L n_segments g0 g0_cv ESW ESW_CV group 1 NWHI 2966.304 27 0.387 0.29 2.432073 0.1002385 2.370472 group_sd group_CV n ER ER_CV D CV D_L95 1 0.1464288 0.06177198 2.4 0.0008090878 1.963101 0.001018856 2.451958 0 D_U95 Area N N_L95 N_U95 1 0.01838116 449375.6 458 0 8259.988 (Note density is provided in units of individuals per square km, not individuals per 100 square km as in Bradford et al. 2020.). The bft slot shows the proportion of effort in each Beaufort sea state: results_eez$bft # A tibble: 7 × 3 bftr km prop &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 0 12.6 0.000775 2 1 154. 0.00943 3 2 692. 0.0425 4 3 1993. 0.122 5 4 5141. 0.316 6 5 5603. 0.344 7 6 2687. 0.165 results_nwhi$bft # A tibble: 7 × 3 bftr km prop &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 0 12.6 0.00421 2 1 65.5 0.0219 3 2 230. 0.0767 4 3 417. 0.139 5 4 870. 0.291 6 5 905. 0.302 7 6 494. 0.165 The g0_details slot includes the results from the g0_model() and g0_weighted() functions called internally by lta_subgroup() (or alternatively, it simply returns the values manually supplied in the Rg0 input). See those functions’ documentation pages for details. results_eez$g0_details $summary bft Rg0 Rg0_CV 1 0 1.00 0.00 2 1 1.00 0.00 3 2 0.72 0.11 4 3 0.51 0.22 5 4 0.37 0.34 6 5 0.26 0.46 7 6 0.19 0.59 The df slot includes details of the detection function fit. See the documentation for df_plot() for details. results_eez$df %&gt;% names [1] &quot;best_models&quot; &quot;all_models&quot; &quot;best_objects&quot; &quot;sightings&quot; The bootstraps slot has the bootstrapped values for various parameters, in case they are useful for troubleshooting, subsequent analyses, and/or plotting: results_eez$bootstraps %&gt;% names [1] &quot;i&quot; &quot;population&quot; &quot;n&quot; &quot;L&quot; &quot;er&quot; [6] &quot;ss&quot; &quot;esw&quot; &quot;g0&quot; &quot;D&quot; &quot;N&quot; Note that there are bootstrap rows for each population: results_eez$bootstraps %&gt;% nrow [1] 10000 results_eez$bootstraps %&gt;% head i population n L er ss esw g0 818 818 pelagic 36 16235.55 0.0022173559 2.168740 2.653525 0.2569263 1762 1762 pelagic 23 16167.34 0.0014226211 2.296063 2.438877 0.5417949 2512 2512 pelagic 10 15997.78 0.0006250867 2.370394 2.426707 0.5029441 860 860 pelagic 17 16225.26 0.0010477493 2.230472 2.162505 0.5604787 1297 1297 pelagic 13 16034.82 0.0008107354 2.290787 2.688269 0.1790180 1637 1637 pelagic 23 15426.49 0.0014909420 2.519449 2.360364 0.2320215 D N 818 0.0035268026 8727 1762 0.0012359996 3059 2512 0.0006070069 1502 860 0.0009640690 2386 1297 0.0019295877 4775 1637 0.0034294852 8487 results_eez$bootstraps %&gt;% tail i population n L er ss esw g0 3208.11 3208 NWHI 12 15678.09 0.0007653992 2.241102 2.558020 0.5454540 3151.2 3151 NWHI 0 15779.70 0.0000000000 2.084252 2.188341 0.2967570 3090.1 3090 NWHI 0 16829.43 0.0000000000 2.257953 2.536585 0.3407843 4231 4231 NWHI 6 16842.64 0.0003562387 2.382205 2.579602 0.3538213 248.1 248 NWHI 0 15088.26 0.0000000000 2.245512 2.331708 0.1778774 4133.3 4133 NWHI 0 16095.29 0.0000000000 2.257244 2.667602 0.2889813 D N 3208.11 0.0006146921 1521 3151.2 0.0000000000 0 3090.1 0.0000000000 0 4231 0.0004648935 1150 248.1 0.0000000000 0 4133.3 0.0000000000 0 Some examples: Behind the scenes This function performs the following operations: Fits a detection function to df_sits without covariates, using the LTabundR function df_fit(), in order to estimate the effective strip width (ESW). Conducts bootstrap re-sampling of the detection function fitting routine in order to estimate the CV of ESW. Estimates the arithmetic mean subgroup size based on the ss input. Creates a bootstrap-resampled distribution of subgroup sizes, with which CV is estimated. Optional: models the Relative g(0) in different survey conditions using the LTabundR function g0_model(). This function also estimates the CV of the Rg(0) estimate in each Beaufort sea state using jackknife resampling. Estimates the encounter rate for each population (subgroup detections / trackline surveyed). If any sighting has a prorated population assignment, the number of subgroups used in each population is determined by multiplying the number of subgroups by the population’s probability. For example, sighting 133 above, which involved 6 subgroups, has a 0.6 probability of belonging to the pelagic population and a 0.4 probabilty of belonging to the NWHI population. Therefore, for the encounter rate, this sighting contributed 3.6 subgroups to the pelagic population (6 * 0.6) and 2.4 subgroups to the NWHI population (6 * 0.4). Creates a bootstrap-resampled distribution of encounter rate estimates. If any sighting has a prorated population assignment, it will be assigned to only one population for each iteration based on a stochastic routine that compares a randomly drawn value between 0 and 1 to the population assignment probabilities. This is explained in further detail in Bradford et al. (2020). Calculates an empirical weighted g(0) estimate according to the proportion of effort occurring in each Beaufort sea state, then uses an automated parameter MCMC optimization routine (see details in LTabundR function g0_weighted()) to estimate the CV of the weighted g(0) estimate. Creates a bootstrap-resampled distribution of the weighted g(0) estimate. Estimates density using the point estimates of effective strip width, subgroup size, g(0), and the encounter rate. Estimates abundance by scaling the density estimate by the provided abundance_area input. Creates a bootstrap-resampled distribution of the density estimate by iteratively drawing values from the resampled distributions of the constituent parameters of the density equation. Creates a bootstrap-resampled distribution of the abundance estimate by scaling the density distribution by the abundance_area input. Note that this approach could theoretically be used for other species that occur in subgroups. "],["diagnostic-plots.html", "12 Diagnostic plots", " 12 Diagnostic plots To demonstrate how LTA results can explored quickly and reviewed for QA/QC using diagnostic plots, we will use a built-in LTabundR dataset, which has density/abundance estimates for the Hawaiian EEZ in 2010 and 2017 for striped dolphins, Fraser’s dolphins, and melon-headed whales, ran with only 100 iterations: data(lta_result) We created these LTA results using the following built-in processed dataset: data(cnp_150km_1986_2020) The function lta_diagnostics() can be used to review the object returned by the LTabundR function lta(), which is the primary function in this package for line-transect analysis. The typical way to use this function is simply: lta_diagnostics(lta_result) When you run this, the function will step through many diagnostic outputs (there are currently 8), some of which are tables and some of which are plots. Between each output, the function will wait for the user to press &lt;Enter&gt;. To turn that waiting feature off, you can add the input wait = FALSE. To see which outputs are currently available from this function, use the following code: lta_diagnostics(lta_result, options = c(), describe_options = TRUE) List of options for outputs to provide: =============== (use numbers in the input `options`) 1 - Point estimate (encounter rate, density, abundance, g(0), etc.) 2 - Summary of bootstrap iterations, including CV of density/abundance 3 - Plot of detection function 4 - Histogram of bootstrapped detection counts 5 - Histogram of bootstrapped g(0) values 6 - Histogram of bootstrapped abundance estimates 7 - Scatterplot of abundance ~ g(0) relationship in boostraps 8 - Time series of point estimate CV as bootstraps accumulate ====================================================== To call specific outputs and not others, use the options input. We demonstrate this be stepping through each output below. Option 1: The point estimate lta_diagnostics(lta_result, options = 1) title species Region Area year segments km Area_covered 1 Striped dolphin 013 (HI_EEZ) 2474596 2010 124 17004 60472 2 Striped dolphin 013 (HI_EEZ) 2474596 2017 131 16281 58036 3 Fraser&#39;s dolphin 026 (HI_EEZ) 2474596 2010 124 17004 59198 4 Fraser&#39;s dolphin 026 (HI_EEZ) 2474596 2017 131 16281 47615 5 Melon-headed whale 031 (HI_EEZ) 2474596 2010 124 17004 NA 6 Melon-headed whale 031 (HI_EEZ) 2474596 2017 131 16281 54317 ESW_mean n g0_est ER_clusters D_clusters N_clusters size_mean size_sd ER 1 3.56 19 0.33 0.0011 0.0005 1202.4 51.4 47.2 0.0574 2 3.56 17 0.32 0.0010 0.0005 1172.1 35.4 18.1 0.0369 3 3.48 3 0.33 0.0002 0.0001 190.9 236.2 129.0 0.0417 4 2.92 2 0.32 0.0001 0.0001 163.4 355.6 91.4 0.0437 5 NA 0 0.33 0.0000 0.0000 0.0 NA NA 0.0000 6 3.34 3 0.32 0.0002 0.0001 214.3 189.2 68.4 0.0349 D N g0_small g0_large g0_cv_small g0_cv_large 1 0.0238 58784 0.33 0.33 0.20 0.20 2 0.0157 38912 0.32 0.32 0.21 0.21 3 0.0186 46047 0.33 0.33 0.20 0.20 4 0.0232 57289 0.32 0.32 0.21 0.21 5 0.0000 0 0.33 0.33 0.20 0.20 6 0.0161 39906 0.32 0.32 0.21 0.21 Option 2: Summary of bootstrap iterations lta_diagnostics(lta_result, options = 2) title Region year species iterations ESW_mean g0_mean 1 Fraser&#39;s dolphin (HI_EEZ) 2010 026 100 3.583041 0.3416702 2 Fraser&#39;s dolphin (HI_EEZ) 2017 026 100 3.094781 0.3281904 3 Melon-headed whale (HI_EEZ) 2010 031 100 NaN 0.3231020 4 Melon-headed whale (HI_EEZ) 2017 031 100 3.239299 0.3168325 5 Striped dolphin (HI_EEZ) 2010 013 100 3.565911 0.3311607 6 Striped dolphin (HI_EEZ) 2017 013 100 3.580167 0.3159498 g0_cv km ER D size Nmean Nmedian Nsd 1 0.2221788 17055.42 0.04245259 0.02025920 231.01534 50133.33 44839.58 39680.14 2 0.2242684 16240.37 0.03999801 0.02057141 348.60057 50905.92 43858.56 39747.42 3 0.1836888 17055.42 0.00000000 0.00000000 NaN 0.00 0.00 0.00 4 0.2421172 16240.37 0.03601738 0.01983840 190.01994 49092.02 41762.63 32261.45 5 0.2099688 17055.42 0.05758173 0.02482534 52.52283 61432.68 57097.42 26481.48 6 0.2258010 16240.37 0.03592027 0.01641222 35.43593 40613.61 38207.79 16051.23 CV L95 U95 1 0.7914923 12533.33 200533.31 2 0.7808014 12726.48 203623.70 3 NaN NaN NaN 4 0.6571629 16364.01 147276.07 5 0.4310651 30716.34 122865.35 6 0.3952181 20306.80 81227.22 Option 3: Plot of detection function lta_diagnostics(lta_result, options = 3) Option 4: Histogram of bootstrapped detection counts lta_diagnostics(lta_result, options = 4) Option 5: Histogram of bootstrapped g(0) values lta_diagnostics(lta_result, options = 5) Option 6: Histogram of bootstrapped abundance estimates lta_diagnostics(lta_result, options = 6) Option 7: Relationship between bootstrap g(0) and abudance lta_diagnostics(lta_result, options = 7) Option 8: Running calculation of CV during bootstrap process lta_diagnostics(lta_result, options = 8) "],["tables.html", "13 Tables Summary tables Appendix tables", " 13 Tables To demonstrate how LTA results can be summarized, tabularized, and plotted, we will use the same built-in dataset as the previous chapter: density/abundance estimates for the Hawaiian EEZ in 2010 and 2017 for striped dolphins, Fraser’s dolphins, and melon-headed whales, ran with only 100 iterations: data(lta_result) We created these LTA results using the following built-in processed dataset: data(cnp_150km_1986_2020) Summary tables To summarize lta() results using the standard table format provided in recent NOAA stock assessment reports, use the function lta_report(). tables &lt;- lta_report(lta_result, cruz = cnp_150km_1986_2020, verbose = TRUE) Providing the cruz object is not required, but if it is not provided, one of the five summary tables ($table1a below) will not be returned. tables %&gt;% names [1] &quot;table1a&quot; &quot;table1b&quot; &quot;table2&quot; &quot;table3&quot; &quot;table4&quot; &quot;tableA1&quot; &quot;tableA2&quot; Table 1 in reports: Sample sizes Table 1a If cruz was provided, $table1a 1a will include total sighting counts for all species in the years from lta_result, broken down by region. The Ntot column holds all sightings, regardless of effort status or Beaufort sea state. Nsys holds counts of systematic-only sightings (i.e., EffType = “S” and Bft &lt;= 6), which may still include sightings that are beyond the species-specific truncation distance and were therefore excluded from density/abundance estimation. These counts are provided separately from the $table1b slot below, since those counts are based on the lta_result object, and will not include sightings for species that did not have a specific LTA estimate specified when it was made. We also include this separately so as to give the user full flexibility in how they summarize sighting counts by region/population/stock. tables$table1a %&gt;% DT::datatable(options=list(initComplete = htmlwidgets::JS( &quot;function(settings, json) {$(this.api().table().container()).css({&#39;font-size&#39;: &#39;9pt&#39;});}&quot;) )) Table 1b This table holds sighting counts used in estimates of density/abundance. The rows match the rows for Tables 3 and 4. In this table, columns are still prepared for total sightings (Ntot) and systematic sightings (Nsys), but they are left blank, since it is not clear how sightings from multiple regions in $table1a would be concatenated for this table. The user can fill in those gaps accordingly. tables$table1b %&gt;% DT::datatable(options=list(initComplete = htmlwidgets::JS( &quot;function(settings, json) {$(this.api().table().container()).css({&#39;font-size&#39;: &#39;9pt&#39;});}&quot;) )) Table 2 in reports: Detection functions Table 3: Parameter estimates Table 4: Density/abundance Appendix tables Table A1: Study areas Table A2: Effort totals (parsed by Beaufort sea state) tables$tableA2 $`2010` # A tibble: 1 × 10 Species Stratum Effort B0 B1 B2 B3 B4 B5 B6 &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 all HI_EEZ 15147 0.00127 0.0127 0.0383 0.116 0.478 0.306 0.0481 $`2017` # A tibble: 1 × 10 Species Stratum Effort B0 B1 B2 B3 B4 B5 B6 &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 all HI_EEZ 14925 0.000845 0.0103 0.0366 0.115 0.311 0.353 0.173 "],["plots.html", "14 Plots Abundance plots Detection function plots", " 14 Plots Abundance plots To plot the abundance estimate with error bars representing the 95% confidence interval, use the function lta_plot(): lta_plot(lta_result, nrows=1) Detection function plots To plot the best-fit detection model for a species pool, use the df_plot() function. df_plot(lta_result) This function provides various stylization options, including the option to show multiple best-fitting models atop a single histogram of detections: df_plot(lta_result, model_colors=RColorBrewer::brewer.pal(n = 4, name = &quot;Dark2&quot;), model_pch = 16, pt_show=2, pt_alpha=.3, bootstrap_show = FALSE, legend_show=TRUE, legend_x=2.8) "],["combining-lta-results.html", "15 Combining lta() results", " 15 Combining lta() results LTabundR provides a function, lta_pool(), that allows you to pool, or concatenate, the results of two separate lta() runs in order to achieve the desired number of bootstrap iterations. An example use case: you already have a lta() result, which you have named lta1, for a species pool with 1,000-iterations. You want to see if the CV estimate is liable to change if you do more iterations, so you can run the same lta() call again and save the result to a new object, lta2. You can then pool these two results together: # Put the lta outputs into a list ltas &lt;- list(lta1, lta2) # Pool bootstraps lta3 &lt;- lta_pool(ltas) The resulting object can be treated the same as another other lta() result, for example you can run lta_diagnostics() or lta_report(). Another use case: You have two lta() outputs, one with 600 iterations and another with 500. You want to pool the two outputs but only keep 1,000 iterations total. Specify the bootstraps input in the lta_pool() call in order to randomly select 1,000 of the 1,100 bootstraps available. # Put the lta outputs into a list ltas &lt;- list(lta1, lta2) # Pool bootstraps lta3 &lt;- lta_pool(ltas, bootstraps=1000) "],["whiceas.html", "16 WHICEAS Data processing Rg0 Density &amp; abundance Results tables", " 16 WHICEAS Here we demonstrate code that reproduces the Bradford et al. (2022) WHICEAS report within the new LTabundR framework. This study estimates cetacean abundance for Hawaiian WHICEAS study area for 2017 and 2020. Here we use survey data from 1986 to 2020 to estimate Relative g(0) and detection functions. Currently, coefficients of variation (CV) of density and abundance are estimated using only 100 bootstrap iterations (the publication uses 1,000) to reduce processing time. library(dplyr) library(LTabundR) library(ggplot2) Data processing We will use the same settings and edits presented earlier in this manual, provided here for convenience. Edits das_file = &#39;data/surveys/CenPac1986-2020_Final_alb.das&#39; edits_file &lt;- &#39;cnp_1986_2020_edits.RData&#39; #### Acoustic ID update ======================================================== # During the 2020 WHICEAS study, a sighting was assigned the species code 051 # for Unidentified *Mesoplodon* in the field. # Later the acoustics team used click detections to re-classify this species # to Blainsville&#39;s beaked whale (species code `&quot;059&quot;`). # To update the `DAS` file accordingly, we can edit the species code directly # within the appropriate line: edit_acoustic &lt;- list(das_file = das_file, type = &#39;text&#39;, rows = 606393, chars = 62:64, edit = &#39;059&#39;) #### Cruise 1607 sighting 55 =================================================== # This sighting, at sequence ID `032` below, # currently triggers errors in `swfscDAS` due to a manually entered `R` event # a few lines above that does not have the `P` (observer positions) event that # typically follows it. Without tht `P` entry, the observer positions of the # sighting are unknown. # To fix this, we can stage an edit that copies the &quot;P&quot; line that occurs minutes # earlier and pastes that line just below the rogue &quot;R&quot; line. edit_1607_55 &lt;- list(das_file = das_file, type = &#39;copy&#39;, rows = 128111, chars = NULL, edit = 128118) #### Cruise 1607 sighting 68 ================================================== # This sighting faces a similar issue: # a rogue `R` event without the follow-up `P` event. # This case is also missing the follow-up `V` event (viewing conditions). # To fix this we will stage a similar edit, #this time copying and pasting two rows (`P` and `V` events) # below the rogue `R` event: edit_1607_68 &lt;- list(das_file = das_file, type = &#39;copy&#39;, rows = c(129982, 129983 , 129985), chars = NULL, edit = 129987) #### Cruise 1621 sighting 245 ================================================= # This is another case of a rogue `R` event, # again missing both the requisite `P` and the `V` post-`R` events. edit_1621_245 &lt;- list(das_file = das_file, type = &#39;copy&#39;, rows = 271932:271933, chars = NULL, edit = 271937) #### Timestamp issues with Cruise 1004 ======================================== # This edit will correct for the fact that all of Cruise 1004 # was conducted using UTC timestamps instead of local timestamps. # A safe approximation of this edit would be to simply adjust the timezone # by the GMT offset for Guam (UTC + 10 hours). edit_1004_gmt10 &lt;- list(das_file = das_file, type = &#39;function&#39;, rows = 433327:437665, chars = 6:39, edit = &#39;function(x){das_time(x, tz_adjust = 10)$dt}&#39;) # Combining and saving edits ================================================== # Finally, we will collect these edits into a single `list` # and save them for use during survey processing (next page). edits &lt;- list(edit_acoustic, edit_1607_55, edit_1607_68, edit_1621_245, edit_1004_gmt10) # Save to file saveRDS(edits,file=edits_file) # Load them in edits &lt;- readRDS(&#39;cnp_1986_2020_edits.RData&#39;) Settings Survey-wide settings data(species_codes) data(ships) data(group_size_coefficients) survey &lt;- load_survey_settings( out_handling = &#39;remove&#39;, min_row_interval = 2, max_row_interval = 3600, max_row_km = 100, km_filler = 1, speed_filler = 10 * 1.852, segment_method = &quot;equallength&quot;, segment_target_km = 150, segment_max_interval = 24, segment_remainder_handling = c(&quot;segment&quot;), ship_list = ships, species_codes = species_codes, group_size_coefficients = group_size_coefficients, smear_angles = FALSE ) Geostrata data(strata_cnp) Cohort-specific settings Cohort 1: all species all_species &lt;- load_cohort_settings( id = &quot;all&quot;, # * species = NULL, strata = c(&#39;WHICEAS&#39;, &#39;HI_EEZ&#39;, &#39;OtherCNP&#39;), # * probable_species = FALSE, sighting_method = 0, cue_range = 0:7, school_size_range = c(0, 10000), school_size_calibrate = TRUE, calibration_floor = 0, use_low_if_na = TRUE, io_sightings = 0, geometric_mean_group = TRUE, truncation_km = 7.5, # * beaufort_range = 0:6, abeam_sightings = TRUE, strata_overlap_handling = c(&quot;smallest&quot;), distance_types = c(&#39;S&#39;,&#39;F&#39;,&#39;N&#39;), distance_modes = c(&#39;P&#39;,&#39;C&#39;), distance_on_off = TRUE ) Cohort 2: bottlenose dolphins bottlenose &lt;- load_cohort_settings( id = &quot;bottlenose&quot;, species = c(&#39;015&#39;, &#39;018&#39;, &#39;021&#39;, &#39;032&#39;), strata = c(&#39;WHICEAS&#39;, &#39;HI_EEZ&#39;, &#39;OtherCNP&#39;, &#39;Bottlenose_BI&#39;, &#39;Bottlenose_OUFI&#39;, &#39;Bottlenose_KaNi&#39;), truncation_km = 7.5) Cohort 3: pantropical spotted dolphins spotted &lt;- load_cohort_settings( id = &quot;spotted&quot;, species = &#39;002&#39;, strata = c(&#39;WHICEAS&#39;, &#39;HI_EEZ&#39;, &#39;OtherCNP&#39;, &#39;Spotted_OU&#39;,&#39;Spotted_FI&#39;,&#39;Spotted_BI&#39;), truncation_km = 7.5) Process settings &lt;- load_settings(strata = strata_cnp, survey = survey, cohorts = list(all_species, bottlenose, spotted)) cruz &lt;- process_surveys(das_file = &#39;data/surveys/CenPac1986-2020_Final_alb.das&#39;, settings = settings, edits = edits) # Save cruz object to file save(cruz, file=&#39;whiceas/whiceas_cruz.RData&#39;) load(&#39;whiceas/whiceas_cruz.RData&#39;) Processed data structure cruz_structure(cruz) Rg0 To estimate Relative g(0) for our species of interest, we use the routine below, based on data from the same time period: # Bring in 10km-segment data from all NOAA/NMFS surveys, 1986 - 2020 data(&quot;noaa_10km_1986_2020&quot;) # Filter to analysis only cruzi &lt;- filter_cruz(noaa_10km_1986_2020, analysis_only = TRUE, eff_types = &#39;S&#39;, bft_range = 0:6, on_off = TRUE) #### Prepare settings list for each species ==================================== species &lt;- list( list(spp = c(&#39;005&#39;, &#39;016&#39;, &#39;017&#39;), title = &#39;Delphinus spp&#39;, truncation = 5.5, constrain_shape=TRUE), list(spp = c(&#39;002&#39;,&#39;006&#39;,&#39;089&#39;,&#39;090&#39;), title = &#39;Stenella attenuata spp&#39;, truncation = 5.5), list(spp = c(&#39;003&#39;,&#39;010&#39;,&#39;011&#39;,&#39;088&#39;,&#39;100&#39;,&#39;101&#39;,&#39;102&#39;,&#39;103&#39;,&#39;107&#39;), title = &#39;Stenella longirostris spp&#39;, truncation = 5.5), list(spp = &#39;013&#39;, title = &#39;Striped dolphin&#39;, truncation = 5.5, constrain_shape=TRUE), list(spp = &#39;015&#39;, title = &#39;Rough-toothed dolphin&#39;, truncation = 5.5), list(spp = &#39;018&#39;, title = &#39;Bottlenose dolphin&#39;, truncation = 5.5, constrain_shape=TRUE), #pool_bft = &#39;12&#39;), list(spp = &#39;021&#39;, title = &quot;Risso&#39;s dolphin&quot;, truncation = 5.5, constrain_shape=TRUE), #pool_bft = &#39;12&#39;), list(spp = &#39;026&#39;, title = &quot;Fraser&#39;s dolphin&quot;, truncation = 5.5), list(spp = &#39;031&#39;, title = &#39;Melon-headed whale&#39;, truncation = 5.5), list(spp = &#39;032&#39;, title = &#39;Pygmy killer whale&#39;, truncation = 5.5), list(spp = &#39;036&#39;, title = &#39;Short-finned pilot whale&#39;, truncation = 5.5, constrain_shape=TRUE), #pool_bft = &#39;12&#39;), list(spp = &#39;037&#39;, title = &#39;Killer whale&#39;, truncation = 5.5, constrain_shape=TRUE), #pool_bft = &#39;12&#39;), list(spp = &#39;046&#39;, title = &#39;Sperm whale&#39;, truncation = 5.5), list(spp = c(&#39;047&#39;, &#39;048&#39;, &#39;080&#39;), title = &#39;Kogia spp&#39;, truncation = 4.0, constrain_shape = FALSE), list(spp = &#39;061&#39;, title = &quot;Cuvier&#39;s beaked whale&quot;, truncation = 4.0, constrain_shape=TRUE), list(spp = &#39;049&#39;, title = &#39;Unid. beaked whale&#39;, truncation = 5.5, constrain_shape=TRUE), list(spp = c(&#39;001&#39;,&#39;051&#39;,&#39;052&#39;,&#39;053&#39;,&#39;054&#39;,&#39;055&#39;, &#39;056&#39;,&#39;057&#39;,&#39;058&#39;,&#39;059&#39;,&#39;060&#39;,&#39;081&#39;, &#39;082&#39;,&#39;083&#39;,&#39;106&#39;,&#39;109&#39;), title = &#39;Mesoplodon spp&#39;, truncation = 4.0, constrain_shape=TRUE), list(spp = &#39;044&#39;, title = &quot;Dall&#39;s porpoise&quot;, truncation = 5.5, regions = &#39;CCS&#39;, constrain_shape=TRUE), list(spp = &#39;071&#39;, title = &#39;Minke whale&#39;, truncation = 4.0, regions=&#39;CCS&#39;, constrain_shape = TRUE), list(spp = c(&#39;072&#39;,&#39;073&#39;,&#39;099&#39;), title = &quot;Sei/Bryde&#39;s&quot;, truncation = 5.5), list(spp = &#39;074&#39;, title = &#39;Fin whale&#39;, truncation = 5.5, regions = &#39;CCS&#39;), list(spp = &#39;075&#39;, title = &#39;Blue whale&#39;, truncation = 5.5, regions = &#39;CCS&#39;), list(spp = &#39;076&#39;, title = &#39;Humpback whale&#39;, truncation = 5.5, regions = &#39;CCS&#39;), list(spp = c(&#39;177&#39;,&#39;277&#39;,&#39;377&#39;,&#39;077&#39;), title = &#39;Unid. dolphin&#39;, truncation = 5.5, constrain_shape=TRUE), list(spp = c(&#39;078&#39;,&#39;079&#39;,&#39;098&#39;,&#39;096&#39;), title = &#39;Unid. cetacean&#39;, truncation = 5.5, constrain_shape=TRUE)) #### Run Rg0 estimation analysis =============================================== Rg0 &lt;- g0_table(cruzi, species, eff_types = &#39;S&#39;, jackknife_fraction = 0.1) # Save result to file save(Rg0, file=&#39;whiceas/Rg0.RData&#39;) # Plot the results: g0_plot(Rg0, panes = 3) Density &amp; abundance First we can define common values that will be constant across all estimates we produce: results_path &lt;- &#39;whiceas/lta/&#39; bootstraps &lt;- 200 # use a low number for this demo years &lt;- 1986:2020 fit_regions &lt;- NULL fit_not_regions &lt;- NULL toplot = TRUE verbose = TRUE df_settings &lt;- list(covariates = c(&#39;bft&#39;,&#39;lnsstot&#39;,&#39;cruise&#39;,&#39;year&#39;,&#39;ship&#39;,&#39;species&#39;), covariates_factor = c(FALSE, FALSE, TRUE, TRUE, TRUE, TRUE), covariates_levels = 2, covariates_n_per_level = 10, simplify_cue = TRUE, simplify_bino = TRUE, detection_function_base = &#39;hn&#39;, base_model = &#39;~1&#39;, delta_aic = 2) For most species, we want to estimate density/abundance for the same set of year-region scenarios. To reduce code redundancy, as well as the risk of typing errors (and our work!), we can use the LTabundR function lta_estimates() to economize how we prepare our estimates input. For most species, these are the year-region scenarios for which we want estimates: scenarios &lt;- list(list(years = 2017, regions = &#39;WHICEAS&#39;), list(years = 2020, regions = &#39;WHICEAS&#39;)) The lta_estimates() function will generate a custom function that makes it easy to create a set of estimates sub-lists for each species of interest: estimator &lt;- lta_estimates(scenarios) That result, estimator, is actually a function. Here’s an example of how this function will work, using the first species pool as an example: estimates &lt;- c(estimator(spp = &#39;013&#39;, title = &quot;Striped dolphin&quot;), estimator(spp = &#39;026&#39;, title = &quot;Fraser&#39;s dolphin&quot;, alt_g0_spp = &#39;013&#39;), estimator(spp = &#39;031&#39;, title = &quot;Melon-headed whale&quot;, alt_g0_spp = &#39;013&#39;)) estimates [[1]] [[1]]$years [1] 2017 [[1]]$regions [1] &quot;WHICEAS&quot; [[1]]$spp [1] &quot;013&quot; [[1]]$title [1] &quot;Striped dolphin&quot; [[2]] [[2]]$years [1] 2020 [[2]]$regions [1] &quot;WHICEAS&quot; [[2]]$spp [1] &quot;013&quot; [[2]]$title [1] &quot;Striped dolphin&quot; [[3]] [[3]]$years [1] 2017 [[3]]$regions [1] &quot;WHICEAS&quot; [[3]]$spp [1] &quot;026&quot; [[3]]$title [1] &quot;Fraser&#39;s dolphin&quot; [[3]]$alt_g0_spp [1] &quot;013&quot; [[4]] [[4]]$years [1] 2020 [[4]]$regions [1] &quot;WHICEAS&quot; [[4]]$spp [1] &quot;026&quot; [[4]]$title [1] &quot;Fraser&#39;s dolphin&quot; [[4]]$alt_g0_spp [1] &quot;013&quot; [[5]] [[5]]$years [1] 2017 [[5]]$regions [1] &quot;WHICEAS&quot; [[5]]$spp [1] &quot;031&quot; [[5]]$title [1] &quot;Melon-headed whale&quot; [[5]]$alt_g0_spp [1] &quot;013&quot; [[6]] [[6]]$years [1] 2020 [[6]]$regions [1] &quot;WHICEAS&quot; [[6]]$spp [1] &quot;031&quot; [[6]]$title [1] &quot;Melon-headed whale&quot; [[6]]$alt_g0_spp [1] &quot;013&quot; The output of estimator() is a list of sub-lists specifying a set of density/abundance estimates you want to produce based on the detection function for a single species pool. Here is the full code for producing those estimates for all species from Bradford et al. (2021): Multi-species pool 1 # Striped dolphin (013), Fraser&#39;s dolphin (026), Melon-headed whale (031) if(TRUE){ # toggle # Detection function specifications fit_filters &lt;- list(spp = c(&#39;013&#39;, &#39;026&#39;, &#39;031&#39;), pool = &#39;Multi-species pool 1&#39;, cohort = &#39;all&#39;, truncation_distance = 5, other_species = &#39;remove&#39;, years = years, regions = fit_regions, not_regions = fit_not_regions) # Density / abundance estimation plan estimates &lt;- c(estimator(spp = &#39;013&#39;, title = &quot;Striped dolphin&quot;), estimator(spp = &#39;026&#39;, title = &quot;Fraser&#39;s dolphin&quot;, alt_g0_spp = &#39;013&#39;), estimator(spp = &#39;031&#39;, title = &quot;Melon-headed whale&quot;, alt_g0_spp = &#39;013&#39;)) # Run analysis results &lt;- lta(cruz, Rg0, fit_filters, df_settings, estimates, bootstraps = bootstraps, toplot=toplot, verbose=verbose) # Save result (results_file &lt;- paste0(results_path, fit_filters$pool, &#39;.RData&#39;)) saveRDS(results, file=results_file) } Multi-species pool 2 # Rough-toothed dolphin (15), Common bottlenose dolphin (18), Risso&#39;s (21), # Pygmy killer whale (32) # Notes # Bottlenose abundance is estimated in a separate cohort, but included here for DF fitting if(TRUE){ # toggle # Detection function specifications fit_filters &lt;- list(spp = c(&#39;015&#39;, &#39;018&#39;, &#39;021&#39;, &#39;032&#39;), pool = &#39;Multi-species pool 2&#39;, cohort = &#39;all&#39;, truncation_distance = 5, years = years, regions = fit_regions, not_regions = fit_not_regions) # Density / abundance estimation plan estimates &lt;- c(estimator(spp = &#39;015&#39;, title = &quot;Rough-toothed dolphin&quot;), estimator(spp = &#39;021&#39;, title = &quot;Risso&#39;s dolphin&quot;), estimator(spp = &#39;032&#39;, title = &quot;Pygmy killer whale&quot;)) # Run analysis results &lt;- lta(cruz, Rg0, fit_filters, df_settings, estimates, use_g0 = TRUE, bootstraps = bootstraps, toplot=toplot, verbose=verbose) # Save result (results_file &lt;- paste0(results_path, fit_filters$pool, &#39;.RData&#39;)) saveRDS(results, file=results_file) } Multi-species pool 3 # Short-finned pilot whale (036), Longman&#39;s beaked whale (065) # No Rg(0) available for Longman&#39;s -- will use SF pilot whale instead to estimate its weighted g0 if(TRUE){ # toggle # Detection function specifications fit_filters &lt;- list(spp = c(&#39;036&#39;, &#39;065&#39;), pool = &#39;Multi-species pool 3&#39;, cohort = &#39;all&#39;, truncation_distance = 5, years = years, regions = fit_regions, not_regions = fit_not_regions) # Density / abundance estimation plan estimates &lt;- c(estimator(spp = &#39;036&#39;, title = &quot;Short-finned pilot whale&quot;), estimator(spp = &#39;065&#39;, title = &quot;Longman&#39;s beaked whale&quot;, alt_g0_spp = &#39;036&#39;)) # Run analysis results &lt;- lta(cruz, Rg0, fit_filters, df_settings, estimates, use_g0 = TRUE, bootstraps = bootstraps, toplot=toplot, verbose=verbose) # Save result (results_file &lt;- paste0(results_path, fit_filters$pool, &#39;.RData&#39;)) saveRDS(results, file=results_file) } Multi-species pool 4 # Killer whale (37), sperm whale (46) if(TRUE){ # toggle # Detection function specifications fit_filters &lt;- list(spp = c(&#39;037&#39;, &#39;046&#39;), pool = &#39;Multi-species pool 4&#39;, cohort = &#39;all&#39;, truncation_distance = 5.5, other_species = &#39;remove&#39;, years = years, regions = fit_regions, not_regions = fit_not_regions) # Density / abundance estimation plan estimates &lt;- c(estimator(spp = &#39;037&#39;, title = &quot;Killer whale&quot;), estimator(spp = &#39;046&#39;, title = &quot;Sperm whale&quot;)) # Run analysis results &lt;- lta(cruz, Rg0, fit_filters, df_settings, estimates, use_g0 = TRUE, bootstraps = bootstraps, toplot=toplot, verbose=verbose) # Save result (results_file &lt;- paste0(results_path, fit_filters$pool, &#39;.RData&#39;)) saveRDS(results, file=results_file) } Multi-species pool 5 # Pygmy sperm whale (47), dwarf sperm whale (48), UNID Kogia (80), # Blainville&#39;s beaked whale (59), Cuvier&#39;s beaked whale (61), # UNID Mesoplodon (51), UNID beaked whale (49), Minke whale (71) if(TRUE){ # toggle # Detection function specifications fit_filters &lt;- list(spp = c(&#39;047&#39;, &#39;048&#39;, &#39;080&#39;, &#39;059&#39;, &#39;061&#39;, &#39;051&#39;, &#39;049&#39;, &#39;071&#39;), pool = &#39;Multi-species pool 5&#39;, cohort = &#39;all&#39;, truncation_distance = 4.5, years = years, regions = fit_regions, not_regions = fit_not_regions) # Density / abundance estimation plan estimates &lt;- c(estimator(spp = &#39;047&#39;, title = &quot;Pygmy sperm whale&quot;), estimator(spp = &#39;048&#39;, title = &quot;Dwarf sperm whale&quot;), estimator(spp = &#39;080&#39;, title = &quot;Unidentified Kogia&quot;), estimator(spp = &#39;059&#39;, title = &quot;Blainville&#39;s beaked whale&quot;), estimator(spp = &#39;061&#39;, title = &quot;Cuvier&#39;s beaked whale&quot;), estimator(spp = &#39;051&#39;, title = &quot;Unidentified Mesoplodon&quot;), estimator(spp = &#39;049&#39;, title = &quot;Unidentified beaked whale&quot;, alt_g0_spp = c(&#39;061&#39;,&#39;051&#39;), combine_g0 = TRUE), estimator(spp = &#39;071&#39;, title = &quot;Minke whale&quot;)) # Note Barlow2015 provides absolute estimates # for Cuviers, Kogia spp. and UNID Mesoplodon # Specify those absolute estimates here, drawn from Bradford et al. (2022): # Pygmy sperm estimates[[1]]$g0 &lt;- 0.005; estimates[[1]]$g0_cv &lt;- 0.15 estimates[[2]]$g0 &lt;- 0.004; estimates[[2]]$g0_cv &lt;- 0.15 # Dwarf sperm estimates[[3]]$g0 &lt;- 0.005; estimates[[3]]$g0_cv &lt;- 0.15 estimates[[4]]$g0 &lt;- 0.004; estimates[[4]]$g0_cv &lt;- 0.15 # UNID Kogia estimates[[5]]$g0 &lt;- 0.005; estimates[[5]]$g0_cv &lt;- 0.15 estimates[[6]]$g0 &lt;- 0.004; estimates[[6]]$g0_cv &lt;- 0.15 # Cuvier&#39;s estimates[[9]]$g0 &lt;- 0.13; estimates[[9]]$g0_cv &lt;- 0.20 estimates[[10]]$g0 &lt;- 0.11; estimates[[10]]$g0_cv &lt;- 0.21 # UNID Mesoplodon estimates[[11]]$g0 &lt;- 0.11; estimates[[11]]$g0_cv &lt;- 0.30 estimates[[12]]$g0 &lt;- 0.11; estimates[[12]]$g0_cv &lt;- 0.30 # Run analysis results &lt;- lta(cruz, Rg0, fit_filters, df_settings, estimates, use_g0 = TRUE, bootstraps = bootstraps, toplot=toplot, verbose=verbose) # Save result (results_file &lt;- paste0(results_path, fit_filters$pool, &#39;.RData&#39;)) saveRDS(results, file=results_file) } Multi-species pool 6 # Bryde&#39;s whale (72), Sei whale (73), Fin whale (74), Blue whale (75), # Sei/Bryde&#39;s (99), Fin/Sei/Bryde&#39;s (72, 73, 74, 99) # Bryde&#39;s, Sei&#39;s, and Sei/Bryde&#39;s all use same Rg0 (title = &quot;Sei/Bryde&#39;s&quot;) # Sei/Bryde&#39;s/Fin use an average of Fin and Sei/Bryde&#39;s. if(TRUE){ # toggle # Detection function specifications fit_filters &lt;- list(spp = c(&#39;072&#39;, &#39;073&#39;, &#39;074&#39;,&#39;075&#39;,&#39;099&#39;, &#39;199&#39;), pool = &#39;Multi-species pool 6&#39;, cohort = &#39;all&#39;, truncation_distance = 5.0, years = years, regions = fit_regions, not_regions = fit_not_regions) # Density / abundance estimation plan estimates &lt;- c(estimator(spp = &#39;072&#39;, title = &quot;Bryde&#39;s whale&quot;), estimator(spp = &#39;073&#39;, title = &quot;Sei whale&quot;), estimator(spp = &#39;074&#39;, title = &quot;Fin whale&quot;), estimator(spp = &#39;075&#39;, title = &quot;Blue whale&quot;), estimator(spp = &#39;099&#39;, title = &quot;Sei/Bryde&#39;s whale&quot;), estimator(spp = &#39;199&#39;, title = &quot;Sei/Bryde&#39;s/Fin whale&quot;, alt_g0_spp = c(&#39;072&#39;, &#39;073&#39;, &#39;099&#39;, &#39;074&#39;), combine_g0 = TRUE)) # Run analysis results &lt;- lta(cruz, Rg0, fit_filters, df_settings, estimates, use_g0 = TRUE, bootstraps = bootstraps, toplot=toplot, verbose=verbose) # Save result (results_file &lt;- paste0(results_path, fit_filters$pool, &#39;.RData&#39;)) saveRDS(results, file=results_file) } Humpback whale if(TRUE){ # toggle # Detection function specifications fit_filters &lt;- list(spp = c(&#39;076&#39;), pool = &#39;Humpback whale&#39;, cohort = &#39;all&#39;, truncation_distance = 5.5, years = years, regions = fit_regions, not_regions = fit_not_regions) # Density / abundance estimation plan estimates &lt;-c(estimator(spp = &#39;076&#39;, title = &quot;Humpback whale&quot;)) # Run analysis results &lt;- lta(cruz, Rg0, fit_filters, df_settings, estimates, use_g0 = TRUE, bootstraps = bootstraps, toplot=toplot, verbose=verbose) # Save result (results_file &lt;- paste0(results_path, fit_filters$pool, &#39;.RData&#39;)) saveRDS(results, file=results_file) } Unidentified rorquals # UNID rorquals (70) if(TRUE){ # toggle # Detection function specifications fit_filters &lt;- list(spp = c(&#39;070&#39;), pool = &#39;Unidentified rorqual&#39;, cohort = &#39;all&#39;, truncation_distance = 5.5, other_species = &#39;coerce&#39;, years = years, regions = fit_regions, not_regions = fit_not_regions) # Density / abundance estimation plan estimates &lt;- c(estimator(spp = &#39;070&#39;, title = &quot;Unidentified rorqual&quot;, alt_g0_spp = c(&#39;071&#39;,&#39;099&#39;,&#39;074&#39;,&#39;075&#39;), combine_g0 = TRUE)) # Run analysis results &lt;- lta(cruz, Rg0, fit_filters, df_settings, estimates, use_g0 = TRUE, bootstraps = bootstraps, toplot=toplot, verbose=verbose) # Save result (results_file &lt;- paste0(results_path, fit_filters$pool, &#39;.RData&#39;)) saveRDS(results, file=results_file) } Unidentified dolphins # UNID dolphin (177, 277, 377, 77) if(TRUE){ # toggle spp &lt;- c(&#39;177&#39;,&#39;277&#39;,&#39;377&#39;,&#39;077&#39;) pool_title &lt;- &#39;Unidentified dolphin&#39; # Detection function specifications fit_filters &lt;- list(spp = c(&#39;177&#39;,&#39;277&#39;,&#39;377&#39;,&#39;077&#39;), pool = pool_title, cohort = &#39;all&#39;, truncation_distance = 5.5, other_species = &#39;coerce&#39;, years = years, regions = fit_regions, not_regions = fit_not_regions) # Density / abundance estimation plan estimates &lt;- estimator(spp = spp, title = pool_title, alt_g0_spp = c(&#39;002&#39;,&#39;013&#39;,&#39;018&#39;,&#39;015&#39;, &#39;036&#39;, &#39;021&#39;), combine_g0 = TRUE) # Run analysis results &lt;- lta(cruz, Rg0, fit_filters, df_settings, estimates, use_g0 = TRUE, bootstraps = bootstraps, toplot=toplot, verbose=verbose) # Save result (results_file &lt;- paste0(results_path, fit_filters$pool, &#39;.RData&#39;)) saveRDS(results, file=results_file) } Unidentified cetaceans # UNID cetacean (78, 79, 98, 96) if(TRUE){ # toggle spp &lt;- c(&#39;078&#39;,&#39;079&#39;,&#39;098&#39;,&#39;096&#39;) pool_title &lt;- &#39;Unidentified cetacean&#39; # Detection function specifications fit_filters &lt;- list(spp = spp, pool = pool_title, cohort = &#39;all&#39;, truncation_distance = 5.5, other_species = &#39;coerce&#39;, years = years, regions = fit_regions, not_regions = fit_not_regions) # Density / abundance estimation plan estimates &lt;- estimator(spp = spp, title = pool_title, g0=1.0, g0_cv = 0.0) # Run analysis results &lt;- lta(cruz, Rg0, fit_filters, df_settings, estimates, use_g0 = TRUE, bootstraps = bootstraps, toplot=toplot, verbose=verbose) # Save result (results_file &lt;- paste0(results_path, fit_filters$pool, &#39;.RData&#39;)) saveRDS(results, file=results_file) } Bottlenose dolphin # Bottlenose dolphin (018) if(TRUE){ # toggle # Detection function specifications fit_filters &lt;- list(spp = c(&#39;015&#39;, &#39;018&#39;, &#39;021&#39;, &#39;032&#39;), pool = &#39;Bottlenose dolphin&#39;, cohort = &#39;bottlenose&#39;, truncation_distance = 5, years = years, regions = fit_regions, not_regions = fit_not_regions) # Density / abundance estimation plan scenarios &lt;- list( list(years = 2017, regions = &#39;WHICEAS&#39;, regions_remove = c(&#39;Bottlenose_KaNi&#39;, &#39;Bottlenose_OUFI&#39;, &#39;Bottlenose_BI&#39;), region_title = &#39;(WHICEAS)&#39;), list(years = 2020, regions = &#39;WHICEAS&#39;, regions_remove = c(&#39;Bottlenose_KaNi&#39;, &#39;Bottlenose_OUFI&#39;, &#39;Bottlenose_BI&#39;), region_title = &#39;(WHICEAS)&#39;)) estimator &lt;- lta_estimates(scenarios) estimates &lt;- estimator(spp = &#39;018&#39;, title = &#39;Bottlenose dolphin&#39;) # Run analysis results &lt;- lta(cruz, Rg0, fit_filters, df_settings, estimates, use_g0 = TRUE, bootstraps = bootstraps, toplot=toplot, verbose=verbose) # Save result (results_file &lt;- paste0(results_path, fit_filters$pool, &#39;.RData&#39;)) saveRDS(results, file=results_file) } Pantropical spotted dolphin # Pantropical spotted dolphin (002) if(TRUE){ # toggle # Detection function specifications fit_filters &lt;- list(spp = c(&#39;002&#39;), pool = &#39;Pantropical spotted dolphin&#39;, cohort = &#39;spotted&#39;, truncation_distance = 5, years = years, regions = fit_regions, not_regions = fit_not_regions) # Density / abundance estimation plan scenarios &lt;- list( list(years = 2017, regions = &#39;WHICEAS&#39;, regions_remove = c(&#39;Spotted_OU&#39;, &#39;Spotted_FI&#39;, &#39;Spotted_BI&#39;), region_title = &#39;(WHICEAS)&#39;), list(years = 2020, regions = &#39;WHICEAS&#39;, regions_remove = c(&#39;Spotted_OU&#39;, &#39;Spotted_FI&#39;, &#39;Spotted_BI&#39;), region_title = &#39;(WHICEAS)&#39;)) estimator &lt;- lta_estimates(scenarios) estimates &lt;- estimator(spp = &#39;002&#39;, title = &#39;Pantropical spotted dolphin&#39;) # Run analysis results &lt;- lta(cruz, Rg0, fit_filters, df_settings, estimates, use_g0 = TRUE, bootstraps = bootstraps, toplot=toplot, verbose=verbose) # Save result (results_file &lt;- paste0(results_path, fit_filters$pool, &#39;.RData&#39;)) saveRDS(results, file=results_file) } Results tables Collate results and generate report: ltas &lt;- lta_enlist(&#39;./whiceas/lta/&#39;) reporti &lt;- lta_report(ltas, cruz) Table 1. Sample sizes: total and detection functions. The lta_report() function above attempts to generate sample size tables based on the cruz object and ltas results (see $table1a and $table1b outputs of lta_report()), but this is difficult to generalize into an automatic function, especially when cohort-specific geostrata are involved. For this reason the sample size estimates are split into two tables: reporti$table1a presents sample sizes of total sightings and total sightings used in detection function fitting, and reporti$table1b presents sampel sizes used in density estimation. reporti$table1a   To expedite building up this sample size table, consider copying and pasting the table produced by lta_report()$table1b, then filling in the blanks with values from above: Table 1b. Sample sizes: density estimation. reporti$table1b Table 2. Detection functions for cetacean species and taxonomic categories. reporti$table2 Table 3. Estimates of line-transect parameters for cetacean species and taxonomic categories. reporti$table3 Table 4. Estimates of density (individuals per 1,000 km2) and abundance for cetacean species and taxonomic categories sighted while on systematic survey effort. reporti$table4 Table A1. Study areas. reporti$tableA1 Table A2. Effort totals and by Beaufort sea-state, in each survey year. reporti$tableA2 $`2017` # A tibble: 3 × 9 Species Stratum Effort B1 B2 B3 B4 B5 B6 &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 spotted WHICEAS 2787 0.0125 0.103 0.114 0.304 0.368 0.0987 2 bottlenose WHICEAS 2939 0.0119 0.0911 0.114 0.312 0.380 0.0910 3 all WHICEAS 3040 0.0115 0.0942 0.111 0.320 0.370 0.0934 $`2020` # A tibble: 3 × 9 Species Stratum Effort B1 B2 B3 B4 B5 B6 &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 spotted WHICEAS 4079 0.0147 0.0382 0.0710 0.274 0.401 0.201 2 bottlenose WHICEAS 4417 0.0174 0.0390 0.0715 0.281 0.407 0.185 3 all WHICEAS 4585 0.0170 0.0406 0.0695 0.281 0.407 0.184 "],["stratagallery.html", "17 Strata gallery Central North Pacific California Current ETP", " 17 Strata gallery This packages comes with several built-in datasets of geographic strata that are commonly used in NOAA/NMFS surveys. The functions strata_explore() and strata_select() were developed to help you explore those built-in options. Central North Pacific strata_explore(&#39;cnp&#39;) To acquire the filepath to one of these strata, pass the index (or indices) printed in the map titles above to the function strata_select(): strata &lt;- strata_select(selections = c(2,3), region = &#39;cnp&#39;) This function returns a named list that can be passed directly to the strata argument in load_settings(). strata %&gt;% names [1] &quot;OtherCNP&quot; &quot;MHI&quot; strata $OtherCNP Lon Lat 1 -131 40.00 2 -126 32.05 3 -120 25.00 4 -120 -5.00 5 -185 -5.00 6 -185 40.00 7 -131 40.00 $MHI Lon Lat 1 -156.00 22.00 2 -154.40 20.60 3 -153.50 19.20 4 -154.35 18.55 5 -155.20 17.75 6 -157.00 18.25 7 -157.50 19.20 8 -161.00 21.84 9 -160.00 23.00 10 -157.00 22.50 11 -156.00 22.00 California Current strata_explore(&#39;ccs&#39;) ETP You can do the same for the Eastern Tropical Pacific (ETP); there are about 70 polygons built-in to LTabundR for this region. We will just show a few of them here, using the start_at argument. strata_explore(&#39;etp&#39;, start_at = 64) "],["segmentizing.html", "18 Segmentizing Approach Base example Day vs Equal Length Contiguous vs. non-contiguous effort Segment remainder handling Typical settings", " 18 Segmentizing The package’s segmentize() function and its associated settings were designed to give researchers full control over how data are segmented, be it for design-based density analysis (which tend to use long segments of 100 km or more and allow for non-contiguous effort to be included in the same segment) or for habitat modeling (which tend to use short segments of 5 - 10 km and disallow non-contiguous effort to be pooled into the same segment). Approach Segments are built and stored separately for each cohort of species, since each cohort has specific settings for segmentizing. Within each cohort, the survey is first grouped into blocs of data that all share the same geographic/scientific scenario, i.e., all rows share the same Cruise number and geographic stratum. Since a survey may leave a stratum then return to it many days hence, it is normal for these blocs to contain non-contiguous data with large spatial gaps. These gaps will be addressed a few steps down. Those blocs are split further according to whether the effort scenario meets inclusion criteria for the analysis. These inclusion criteria are controlled by the cohort-specific settings such as distance_types, distance_modes, and distance_on_off. Rows of data that meet the inclusion criteria are relegated to their own data bloc, and given a new column, use, with the value TRUE. Data that do not meet the criteria are relegated to their own bloc as well (column use is FALSE). This means that, at the end of this process, we will have segments that will be used in the density/detection function analysis, and segments that will not. (The excluded segments are not deleted or transformed in any other way; they can still be used in summarizing detections, etc.) Next, the segmentize() function loops through each of these blocs of effort and parses its data into segments according to the segment_method. If segmentizing by \"day\", this is straightforward: all data occurring on a unique date are assigned to its own segment. Segmentizing by \"equallength\" (a target length for each segment) is a bit more complicated in terms of coding: segments are built up one line of data at a time; if the segment_target_km is reached or the segment_max_interval is exceeded, a new segment begins. At the end of this process, you have lists of data sorted into their segments, each with a unique seg_id, as well as a summary dataframe that provides the distance (km); time and coordinates for the beginning, middle, and end of the segment; and the weighted averages of sighting conditions and weather data contained in the segment. Setting up this demo The demonstration of segmentize() on in Processing chapter relies on the settings list that is attached as a slot in the cruz object. But you can override those settings with direct function inputs in segmentize(), which gives us a chance to explore segmentization options. First we load the demo data and carry out initial processing: # Load built-in settings example data(example_settings) settings &lt;- example_settings # Set path to DAS file das_file &lt;- &#39;data/surveys/HICEASwinter2020.das&#39; # First steps of formatting das &lt;- das_load(das_file, perform_checks = FALSE, print_glimpse = FALSE) cruz &lt;- process_strata(das, settings, verbose=FALSE) cruz &lt;- das_format(cruz) Base example Here is the segmentize() function parameterized with some basic settings: a target segment length of 30km, with the maximum allowable gap in effort within segments to be 6 hours. cruz_demo &lt;- segmentize(cruz, segment_method = &#39;equallength&#39;, segment_target_km = 30, segment_max_interval = 6, segment_remainder_handling = c(&#39;append&#39;,&#39;segment&#39;), distance_types = c(&#39;S&#39;,&#39;F&#39;,&#39;N&#39;), distance_modes = c(&#39;P&#39;,&#39;C&#39;), distance_on_off = c(TRUE), verbose=FALSE, to_plot = TRUE) Day vs Equal Length By day cruz_demo &lt;- segmentize(cruz, segment_method = &#39;day&#39;, verbose=FALSE, to_plot = TRUE) By target length of 100 km cruz_demo &lt;- segmentize(cruz, segment_method = &#39;equallength&#39;, segment_target_km = 100, segment_max_interval = 48, verbose=FALSE, to_plot = TRUE) Contiguous vs. non-contiguous effort The default example at the top allows for non-contiguous effort; a segment is allowed to contain effort separated by gaps as large as 24 hours (settings$segment_max_interval). To coerce segments to represent only contiguous effort, make that setting very small: cruz_demo &lt;- segmentize(cruz, segment_method = &#39;equallength&#39;, segment_target_km = 30, segment_max_interval = .1, verbose=FALSE, to_plot = TRUE) You can see that many contiguous periods of effort were much shorter than the target length of 30 km. This is why allowing for non-contiguous effort can be advantageous for target segment lengths larger than 5 - 10 km. Segment remainder handling In the top example, the setting for segment_remainder_handling, c('append','segment'), means that remainders less than half the target length will be randomly appended to another segment, while remainders more than half will be treated as their own segment (and will be placed randomly along the trackline). If you don’t want that level of complexity, you can simply assign a single setting: 'append' will append the remainder in all cases, regardless of remainder length relative to the target length. The same idea goes for 'segment'. Here is a simple comparison of those two approaches: \"append\" cruz_demo &lt;- segmentize(cruz, segment_method = &#39;equallength&#39;, segment_target_km = 10, segment_max_interval = 6, segment_remainder_handling = &#39;append&#39;, verbose=FALSE, to_plot = TRUE) \"segment\" cruz_demo &lt;- segmentize(cruz, segment_method = &#39;equallength&#39;, segment_target_km = 10, segment_max_interval = 6, segment_remainder_handling = &#39;segment&#39;, verbose=FALSE, to_plot = TRUE) \"disperse\" The other possible setting is 'disperse', which disperses the remainder evenly across all segments. To demonstrate, let’s use a target length of 10 km. cruz_demo &lt;- segmentize(cruz, segment_method = &#39;equallength&#39;, segment_target_km = 10, segment_max_interval = 6, segment_remainder_handling = &#39;disperse&#39;, verbose=FALSE, to_plot = TRUE) Note that most segments are longer than the target length, due to the impact of dispersing the remainder. If you wanted, you could combat this by making the target length slightly smaller: cruz_demo &lt;- segmentize(cruz, segment_method = &#39;equallength&#39;, segment_target_km = 9.5, segment_max_interval = 6, segment_remainder_handling = &#39;disperse&#39;, verbose=FALSE, to_plot = TRUE) But in general, the disperse option may be more appropriate for shorter segment lengths. Typical settings Design-based line transect analysis To replicate methods used in density estimation analyses, use large segment lengths (100 km or more) or simply segmentize by day. (See the examples above.) Remember that long segment lengths won’t work well unless you allow for non-contiguous effort. Habitat modeling To replicate the methods used in typical habitat modeling studies, use smaller segment lengths of contiguous effort. cruz_demo &lt;- segmentize(cruz, segment_method = &#39;equallength&#39;, segment_target_km = 5, segment_max_interval = .1, verbose=FALSE, to_plot = TRUE) "],["validation.html", "19 Validation Data processing", " 19 Validation As explained in the Overview, LTabundR was developed , among other reasons, to replace and build upon the FORTRAN software ABUND. To demonstrate that this purpose has been achieved, and to validate the results that are generated with LTabundR, this chapter is dedicated to comparing and contrasting the results of the two software packages. We have tried to develop LTabundR with the flexibility either to replicate ABUND results or to produce customizable results that could potentially vary from ABUND quite significantly (e.g., formatted for habitat modeling). However, even when we use LTabundR settings intended to replicate ABUND results, there are some intentional updates the processing routine that will lead to some small differences. This chapter requires the following packages: library(LTabundR) library(dplyr) library(tidyr) library(ggplot2) library(plotly) Data processing To validate the data processing functions within LTabundR, we can compare its output to that of ABUND9, written by Jay Barlow (NOAA Fisheries). First, we bring in the ABUND9 output files for the same DAS data: # Local paths to these files SIGHTS &lt;- read.csv(&#39;data/SIGHTS.csv&#39;) EFFORT &lt;- read.csv(&#39;data/EFFORT.csv&#39;) You may download these files here: SIGHTS.csv and EFFORT.csv. Sightings Pivot and format the ABUND SIGHTS data… abund &lt;- SIGHTS %&gt;% pivot_longer(cols = 31:101, names_to = &#39;species&#39;, values_to = &#39;best&#39;) %&gt;% filter(best &gt; 0) %&gt;% mutate(Region = gsub(&#39; &#39;,&#39;&#39;,Region)) %&gt;% mutate(DateTime = paste0(Yr,&#39;-&#39;,Mo,&#39;-&#39;,Da,&#39; &#39;,Hr,&#39;:&#39;,Min)) …then summarize counts of species within each cruise: abund_summ &lt;- abund %&gt;% group_by(cruise = CruzNo, species) %&gt;% summarize(ntot_abund = n(), nsys_abund = length(which(! Region %in% c(&#39;NONE&#39;, &#39;Off-Transect&#39;) &amp; EffortSeg &gt; 0))) %&gt;% mutate(species = gsub(&#39;SP&#39;,&#39;&#39;,species)) Then do the same for LTabundR output: load(&#39;whiceas_cruz.RData&#39;) (This cruz object was produced in the Data Processing chapter.) ltabundr &lt;- cruz$cohorts$all$sightings %&gt;% # Filter out species that ABUND ignored based on its INP file filter(!species %in% c(&#39;CU&#39;, &#39;PU&#39;)) ltabundr_summ &lt;- ltabundr %&gt;% filter(OnEffort == TRUE) %&gt;% group_by(cruise = Cruise, species) %&gt;% summarize(ntot_ltabundr = n(), nsys_ltabundr = length(which(included == TRUE &amp; EffType %in% c(&#39;S&#39;,&#39;F&#39;)))) Now join these two datasets by cruise and species code: mr &lt;- full_join(abund_summ, ltabundr_summ, by=c(&#39;cruise&#39;, &#39;species&#39;)) mr %&gt;% head # A tibble: 6 × 6 # Groups: cruise [1] cruise species ntot_abund nsys_abund ntot_ltabundr nsys_ltabundr &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; 1 901 002 3 2 3 2 2 901 013 4 4 4 4 3 901 015 2 1 2 1 4 901 018 2 1 2 1 5 901 031 1 1 1 1 6 901 032 2 0 2 0 Compare the total On-Effort sightings in both outputs: mr$ntot_abund %&gt;% sum(na.rm=TRUE) [1] 3223 mr$ntot_ltabundr %&gt;% sum(na.rm=TRUE) [1] 3227 Compare total sightings valid for use in density estimation (EffType \"S\" or \"F\" only, as well as other criteria such as Bft 0 - 6): mr$nsys_abund %&gt;% sum(na.rm=TRUE) [1] 2478 mr$nsys_ltabundr %&gt;% sum(na.rm=TRUE) [1] 2478 Let’s find the rows with discrepancies in sighting counts: bads &lt;- which(mr$nsys_abund != mr$nsys_ltabundr | mr$ntot_abund != mr$ntot_ltabundr | is.na(mr$ntot_abund) | is.na(mr$ntot_ltabundr) | is.na(mr$nsys_abund) | is.na(mr$nsys_ltabundr)) bads %&gt;% length [1] 7 Let’s look at those rows in the joined dataframe: mr[bads, ] # A tibble: 7 × 6 # Groups: cruise [5] cruise species ntot_abund nsys_abund ntot_ltabundr nsys_ltabundr &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; 1 1203 015 1 0 2 0 2 1203 049 1 1 2 1 3 1621 073 5 4 5 3 4 2001 051 2 2 1 1 5 2001 059 1 0 2 1 6 1165 047 NA NA 1 1 7 1631 003 NA NA 1 0 To investigate these 5 discrepancies, we will write a helper function that returns sightings details from both outputs for a given cruise-species: sight_compare &lt;- function(abund, ltabundr, cruise, spp){ message(&#39;ABUND:&#39;) abund %&gt;% filter(CruzNo == cruise, species == paste0(&#39;SP&#39;,spp)) %&gt;% select(5, 34, 26, 29, 33, 3) %&gt;% mutate(use_sit = EffortSeg != 0) %&gt;% select(-EffortSeg) %&gt;% arrange(desc(use_sit)) %&gt;% print abund %&gt;% filter(CruzNo == 1631) %&gt;% pull(species) %&gt;% table message(&#39;\\nLTabundR:&#39;) ltabundr %&gt;% filter(Cruise == cruise, species == spp) %&gt;% select(Cruise, DateTime, Bft, mixed, best, OnEffort, EffType, use, included) %&gt;% rename(use_sit = included, use_effort = use) %&gt;% arrange(desc(use_effort)) %&gt;% tibble %&gt;% print } Discrepancies Below we investigate each discrepancy identified above. Note that there are a few intentional design features that may produce differences in the sightings counts between ABUND9 and LTabundR. Here are a few: (1) In ABUND9, only sightings that occur while OnEffort == TRUE are returned; in contrast, LTabundR does not remove any sightings (it just flags them differently, using the included column variable). This will be evident in one of the discrepancies below, in which a sighting that occurred during a Beaufort Sea State of 7 is not returned by ABUND9, since the INP file for the ABUND routine specified that only sightings within Bft 0-6 should be used in analysis. Note, however, that one can readily filter LTabundR sightings to emulate ABUND9 output if needed. (2) LTabundR includes one additional criterion for inclusion in analysis: the sighting must occur at or forward of the beam (note that this restriction can be deactivated in load_cohort_settings(). (3) Since point-in-polygon calculations are very different in the two programs, it is possible that sightings occurring very near geostratum margins may be included/excluded differently. Cruise 1203, Species 015 In this case, LTabundR has a non-systematic sighting that ABUND has ignored. sight_compare(abund, ltabundr, 1203, &#39;015&#39;) ABUND: # A tibble: 1 × 6 CruzNo DateTime Beauf Mixed best use_sit &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;lgl&gt; 1 1203 2012-5-16 11:58 4 &quot; F&quot; 74.9 TRUE LTabundR: # A tibble: 2 × 9 Cruise DateTime Bft mixed best OnEffort EffType use_effort &lt;dbl&gt; &lt;dttm&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;chr&gt; &lt;lgl&gt; 1 1203 2012-05-13 13:19:44 5 TRUE 1 TRUE N TRUE 2 1203 2012-05-16 11:58:30 4 FALSE 74.9 TRUE N TRUE # ℹ 1 more variable: use_sit &lt;lgl&gt; Looking at the sighting details from LTabundR … (ltabundr %&gt;% filter(Cruise == 1203, species == &#39;015&#39;))[1,] Event DateTime Lat Lon OnEffort Cruise Mode SpPerc260 S 2012-05-13 13:19:44 11.96733 -161.1727 TRUE 1203 C OffsetGMT EffType ESWsides Course SpdKt Bft SwellHght WindSpdKt SpPerc260 -10 N 2 18 9.2 5 6 21 RainFog HorizSun VertSun Glare Vis ObsL Rec ObsR ObsInd EffortDot SpPerc260 5 12 12 FALSE 6 073 235 280 &lt;NA&gt; TRUE EventNum file_das line_num stratum_HI_EEZ SpPerc260 268 CenPac1986-2020_Final_alb_edited.das 493027 FALSE stratum_OtherCNP stratum_WHICEAS year month day yday km_valid km_int SpPerc260 TRUE FALSE 2012 5 13 134 TRUE 0 km_cum ship stratum use eff_bloc seg_id SightNo Subgroup SpPerc260 191053.4 OES OtherCNP TRUE 57-2 2330 069 &lt;NA&gt; SightNoDaily Obs ObsStd Bearing Reticle DistNm Cue Method Photos SpPerc260 20120513_20 235 TRUE 90 NA 0.1 3 1 Y Birds CalibSchool PhotosAerial Biopsy CourseSchool TurtleSp TurtleGs SpPerc260 Y N N N NA &lt;NA&gt; NA TurtleJFR TurtleAge TurtleCapt PinnipedSp PinnipedGs BoatType BoatGs SpPerc260 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; NA &lt;NA&gt; NA PerpDistKm species best low high prob mixed ss_tot lnsstot SpPerc260 0.1852 015 1 NaN NaN FALSE TRUE 6.956522 1.93968 ss_percent n_sp n_obs n_best n_low n_high calibr ss_valid mixed_max SpPerc260 NaN 2 1 1 1 1 TRUE FALSE FALSE spp_max included SpPerc260 &lt;NA&gt; TRUE According to ABUND, this sighting is not mixed-species, but LTabundR says it is. Looking at the raw DAS… das_file &lt;- &#39;data/surveys/CenPac1986-2020_Final_alb.das&#39; das &lt;- das_readtext(das_file) i &lt;- which(substr(das$das, 6, 18) == &#39;131944 051312&#39;) das$das[i] [1] &quot;268S.131944 051312 N11:58.04 W161:10.36 069 235 3 1 090 0.10 N N N&quot; [2] &quot;269A.131944 051312 N11:58.04 W161:10.36 069 Y Y 033 015 &quot; [3] &quot;269C.131944 051312 N11:58.04 W161:10.36 Overall estimate for full group- never saw all at once. -EMO&quot; [4] &quot;268G.131944 051312 N11:58.04 W161:10.36 069 A 235 1 090 0.10 &quot; [5] &quot;269A.131944 051312 N11:58.04 W161:10.36 069 Y Y 033 015&quot; [6] &quot;269C.131944 051312 N11:58.04 W161:10.36 begin PC protool, first sighting is subgroup &#39;A&#39;, acoustics already tracking &quot; [7] &quot;269C.131944 051312 N11:58.04 W161:10.36 photos taken during sighting indicate Steno (015) present. Not seen or estimated during sighting. -EMO&quot; We see that this was a sighting of false killer whales during which species 015 was picked up during photo-ID. In the absence of a percent composition estimate for this sighting, it was ignored by ABUND. Cruise 1203, Species 049 In this case, LTabundR has a non-systematic sighting that ABUND has ignored. sight_compare(abund, ltabundr, 1203, &#39;049&#39;) ABUND: # A tibble: 1 × 6 CruzNo DateTime Beauf Mixed best use_sit &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;lgl&gt; 1 1203 2012-5-3 14:21 6 &quot; F&quot; 1.16 TRUE LTabundR: # A tibble: 2 × 9 Cruise DateTime Bft mixed best OnEffort EffType use_effort &lt;dbl&gt; &lt;dttm&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;chr&gt; &lt;lgl&gt; 1 1203 2012-05-03 14:21:40 6 FALSE 1.16 TRUE S TRUE 2 1203 2012-05-07 10:54:09 7 FALSE 1 TRUE N FALSE # ℹ 1 more variable: use_sit &lt;lgl&gt; Looking at the sighting details from LTabundR … (ltabundr %&gt;% filter(Cruise == 1203, species == &#39;049&#39;))[2,] Event DateTime Lat Lon OnEffort Cruise Mode SpPerc1614 S 2012-05-07 10:54:09 5.960333 -162.1255 TRUE 1203 C OffsetGMT EffType ESWsides Course SpdKt Bft SwellHght WindSpdKt SpPerc1614 -10 N 2 269 8.8 7 9 30 RainFog HorizSun VertSun Glare Vis ObsL Rec ObsR ObsInd EffortDot SpPerc1614 5 5 1 FALSE 4.5 238 328 073 &lt;NA&gt; TRUE EventNum file_das line_num SpPerc1614 145 CenPac1986-2020_Final_alb_edited.das 489960 stratum_HI_EEZ stratum_OtherCNP stratum_WHICEAS year month day yday SpPerc1614 FALSE TRUE FALSE 2012 5 7 128 km_valid km_int km_cum ship stratum use eff_bloc seg_id SightNo SpPerc1614 TRUE 0 189739.1 OES OtherCNP FALSE 54-1 2309 052 Subgroup SightNoDaily Obs ObsStd Bearing Reticle DistNm Cue Method SpPerc1614 &lt;NA&gt; 20120507_9 238 TRUE 310 10 0.52 3 4 Photos Birds CalibSchool PhotosAerial Biopsy CourseSchool TurtleSp SpPerc1614 N N N N N NA &lt;NA&gt; TurtleGs TurtleJFR TurtleAge TurtleCapt PinnipedSp PinnipedGs SpPerc1614 NA &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; NA BoatType BoatGs PerpDistKm species best low high prob mixed ss_tot SpPerc1614 &lt;NA&gt; NA 0.7377314 049 1 1 1 FALSE FALSE 1 lnsstot ss_percent n_sp n_obs n_best n_low n_high calibr ss_valid SpPerc1614 0 1 1 1 1 1 1 TRUE TRUE mixed_max spp_max included SpPerc1614 TRUE 049 FALSE This is a sighting of a single Ziphiid whale. It appears to be within the geostratum: cruzi &lt;- filter_cruz(cruz, spp=&#39;049&#39;, years = 1988) map_cruz(cruzi) Loooking at the raw DAS data … i &lt;- which(substr(das$das, 6, 18) == &#39;105409 050712&#39;) das$das[(i[1] - 10):(i[1] + 3)] [1] &quot;135V.104143 050712 N05:57.61 W162:05.66 7 09 070 30.0&quot; [2] &quot;136N.104143 050712 N05:57.61 W162:05.66 272 09.3&quot; [3] &quot;137W.104143 050712 N05:57.61 W162:05.66 5 05 01 050 4.5&quot; [4] &quot;138*.104234 050712 N05:57.62 W162:05.79&quot; [5] &quot;139*.104434 050712 N05:57.62 W162:06.09&quot; [6] &quot;140*.104634 050712 N05:57.63 W162:06.39&quot; [7] &quot;141N.104713 050712 N05:57.63 W162:06.49 269 08.8&quot; [8] &quot;142*.104834 050712 N05:57.63 W162:06.69&quot; [9] &quot;143*.105034 050712 N05:57.63 W162:06.99&quot; [10] &quot;144*.105234 050712 N05:57.63 W162:07.29&quot; [11] &quot;145S.105409 050712 N05:57.62 W162:07.53 052 238 3 4 310 10.0 0.52 N N N&quot; [12] &quot;146A.105409 050712 N05:57.62 W162:07.53 052 N N 049 &quot; [13] &quot; 1 238 1 1 1 100&quot; [14] &quot;147*.105434 050712 N05:57.62 W162:07.59&quot; Note that the Beaufort sea state for this sighting is 7. When the ABUND routine was run, the INP file instructed it to disregard any sightings beyond Bft 6. This is why ABUND has no record of this sighting. Cruise 1165, Species 047 In this case, LTabundR has a systematic sighting of a pygmy sperm whale that ABUND has ignored. sight_compare(abund, ltabundr, 1165, &#39;047&#39;) ABUND: # A tibble: 0 × 6 # ℹ 6 variables: CruzNo &lt;int&gt;, DateTime &lt;chr&gt;, Beauf &lt;int&gt;, Mixed &lt;chr&gt;, # best &lt;dbl&gt;, use_sit &lt;lgl&gt; LTabundR: # A tibble: 1 × 9 Cruise DateTime Bft mixed best OnEffort EffType use_effort &lt;dbl&gt; &lt;dttm&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;chr&gt; &lt;lgl&gt; 1 1165 1988-07-30 19:28:00 0 FALSE 1.16 TRUE S TRUE # ℹ 1 more variable: use_sit &lt;lgl&gt; (ltabundr %&gt;% filter(Cruise == 1165, species == &#39;047&#39;))[1,] Event DateTime Lat Lon OnEffort Cruise Mode SpPerc1460 S 1988-07-30 19:28:00 26.3 -121.1167 TRUE 1165 C OffsetGMT EffType ESWsides Course SpdKt Bft SwellHght WindSpdKt SpPerc1460 NA S 2 163 10.5 0 NA NA RainFog HorizSun VertSun Glare Vis ObsL Rec ObsR ObsInd EffortDot SpPerc1460 1 NA NA NA NA 038 068 051 &lt;NA&gt; TRUE EventNum file_das line_num SpPerc1460 92 CenPac1986-2020_Final_alb_edited.das 50173 stratum_HI_EEZ stratum_OtherCNP stratum_WHICEAS year month day yday SpPerc1460 FALSE TRUE FALSE 1988 7 30 212 km_valid km_int km_cum ship stratum use eff_bloc seg_id SightNo SpPerc1460 TRUE 0 15232.08 MAC OtherCNP TRUE 51-0 2279 07 Subgroup SightNoDaily Obs ObsStd Bearing Reticle DistNm Cue Method SpPerc1460 &lt;NA&gt; 19880730_13 051 TRUE 45 3.38 0.9 3 4 Photos Birds CalibSchool PhotosAerial Biopsy CourseSchool TurtleSp SpPerc1460 &lt;NA&gt; N &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; NA &lt;NA&gt; TurtleGs TurtleJFR TurtleAge TurtleCapt PinnipedSp PinnipedGs SpPerc1460 NA &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; NA BoatType BoatGs PerpDistKm species best low high prob mixed SpPerc1460 &lt;NA&gt; NA 1.178606 047 1.15942 1 1.259921 FALSE FALSE ss_tot lnsstot ss_percent n_sp n_obs n_best n_low n_high calibr SpPerc1460 1.15942 0.1479201 1 1 3 3 3 3 TRUE ss_valid mixed_max spp_max included SpPerc1460 TRUE TRUE 047 TRUE To investigate this sighting, we can filter our cruz object and take a look at a map of this sighting: cruzi &lt;- filter_cruz(cruz, spp=&#39;047&#39;, years = 2012) map_cruz(cruzi) Using that map we see that this sighting occurred just inside of the OtherCNP geostratum. It is likely that the point-in-polygon subroutines inside ABUND9 decided that this sighting was out of the study area, and therefore excluded it. The subroutines used by LTabundR, which are based in the R package sf, should not be wrong in this case. Cruise 1621-073 In this case, both LTabundR and ABUND logged the same total number of sightings, but ABUND determined that that one was not valid for for density estimation, whereas LTabundR determined that two of them were not valid. sight_compare(abund, ltabundr, 1621, &#39;073&#39;) ABUND: # A tibble: 5 × 6 CruzNo DateTime Beauf Mixed best use_sit &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;lgl&gt; 1 1621 2002-11-8 14:19 3 &quot; F&quot; 3.48 TRUE 2 1621 2002-11-20 16:2 4 &quot; F&quot; 3.97 TRUE 3 1621 2002-11-22 10:27 4 &quot; F&quot; 4.96 TRUE 4 1621 2002-11-22 12:20 4 &quot; F&quot; 1.16 TRUE 5 1621 2002-11-29 13:48 5 &quot; F&quot; 2.32 TRUE LTabundR: # A tibble: 7 × 9 Cruise DateTime Bft mixed best OnEffort EffType use_effort &lt;dbl&gt; &lt;dttm&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;chr&gt; &lt;lgl&gt; 1 1621 2002-11-08 14:19:04 3 FALSE 3.48 TRUE S TRUE 2 1621 2002-11-20 16:02:58 4 FALSE 3.97 TRUE S TRUE 3 1621 2002-11-22 10:27:28 4 FALSE 4.96 TRUE S TRUE 4 1621 2002-11-22 12:20:33 4 FALSE 1.16 TRUE S TRUE 5 1621 2002-11-29 13:48:49 5 FALSE 2.32 TRUE N TRUE 6 1621 2002-11-20 12:59:22 4 FALSE 1.16 FALSE S FALSE 7 1621 2002-11-21 11:57:55 4 FALSE 1.16 FALSE S FALSE # ℹ 1 more variable: use_sit &lt;lgl&gt; The discrepancy is in the November 8, 2002 sighting at 14:19:04. Looking at the sighting details from LTabundR… (ltabundr %&gt;% filter(Cruise == 1621, species == &#39;073&#39;))[1,] Event DateTime Lat Lon OnEffort Cruise Mode SpPerc11806 S 2002-11-08 14:19:04 22.539 -171.7638 TRUE 1621 C OffsetGMT EffType ESWsides Course SpdKt Bft SwellHght WindSpdKt SpPerc11806 10 S 2 108 9.4 3 5 10 RainFog HorizSun VertSun Glare Vis ObsL Rec ObsR ObsInd EffortDot SpPerc11806 1 3 2 FALSE 6 200 073 208 &lt;NA&gt; TRUE EventNum file_das line_num SpPerc11806 129 CenPac1986-2020_Final_alb_edited.das 273857 stratum_HI_EEZ stratum_OtherCNP stratum_WHICEAS year month day yday SpPerc11806 TRUE TRUE FALSE 2002 11 8 312 km_valid km_int km_cum ship stratum use eff_bloc seg_id SightNo SpPerc11806 TRUE 0 91181.6 DSJ HI_EEZ TRUE 155-7 543 275 Subgroup SightNoDaily Obs ObsStd Bearing Reticle DistNm Cue Method SpPerc11806 &lt;NA&gt; 20021108_57 208 TRUE 93 0.5 2.75 6 4 Photos Birds CalibSchool PhotosAerial Biopsy CourseSchool TurtleSp SpPerc11806 Y N &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; NA &lt;NA&gt; TurtleGs TurtleJFR TurtleAge TurtleCapt PinnipedSp PinnipedGs SpPerc11806 NA &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; NA BoatType BoatGs PerpDistKm species best low high prob mixed SpPerc11806 &lt;NA&gt; NA 5.08602 073 3.478261 3 3 FALSE FALSE ss_tot lnsstot ss_percent n_sp n_obs n_best n_low n_high calibr SpPerc11806 3.478261 1.246532 1 1 6 6 6 6 TRUE ss_valid mixed_max spp_max included SpPerc11806 TRUE TRUE 073 FALSE LTabundR was correct to exclude this sighting because the bearing was past the beam (93 degrees). The bearing of 93 is also given in the raw DAS data… das_file &lt;- &#39;data/surveys/CenPac1986-2020_Final_alb.das&#39; das &lt;- das_readtext(das_file) i &lt;- which(substr(das$das, 6, 18) == &#39;141904 110802&#39;) das$das[i] [1] &quot;129S.141904 110802 N22:32.34 W171:45.83 275 208 6 4 093 0.5 2.75&quot; [2] &quot;130A.141904 110802 N22:32.34 W171:45.83 275 Y N 073&quot; … so it is not clear why ABUND did not exclude this sighting as invalid as well. It may be that ABUND was not expecting bearings above 90 degrees in the on-effort data. Cruise 1631, Species 003 In this case, there was a non-systematic sighting of species 003 that was found by LTabundR but not by ABUND. sight_compare(abund, ltabundr, 1631, &#39;003&#39;) ABUND: # A tibble: 0 × 6 # ℹ 6 variables: CruzNo &lt;int&gt;, DateTime &lt;chr&gt;, Beauf &lt;int&gt;, Mixed &lt;chr&gt;, # best &lt;dbl&gt;, use_sit &lt;lgl&gt; LTabundR: # A tibble: 1 × 9 Cruise DateTime Bft mixed best OnEffort EffType use_effort &lt;dbl&gt; &lt;dttm&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;chr&gt; &lt;lgl&gt; 1 1631 2006-09-02 07:35:14 5 TRUE 1 TRUE N TRUE # ℹ 1 more variable: use_sit &lt;lgl&gt; (ltabundr %&gt;% filter(Cruise == 1631, species == &#39;003&#39;)) Event DateTime Lat Lon OnEffort Cruise Mode SpPerc2279 S 2006-09-02 07:35:14 19.28883 -156.8227 TRUE 1631 P OffsetGMT EffType ESWsides Course SpdKt Bft SwellHght WindSpdKt SpPerc2279 10 N 2 155 10 5 4 18 RainFog HorizSun VertSun Glare Vis ObsL Rec ObsR ObsInd EffortDot SpPerc2279 1 10 2 FALSE 7 073 196 197 &lt;NA&gt; TRUE EventNum file_das line_num SpPerc2279 026 CenPac1986-2020_Final_alb_edited.das 405222 stratum_HI_EEZ stratum_OtherCNP stratum_WHICEAS year month day yday SpPerc2279 TRUE TRUE TRUE 2006 9 2 245 km_valid km_int km_cum ship stratum use eff_bloc seg_id SightNo SpPerc2279 TRUE 0 139591.4 Mc2 WHICEAS TRUE 217-1 1149 090 Subgroup SightNoDaily Obs ObsStd Bearing Reticle DistNm Cue Method SpPerc2279 &lt;NA&gt; 20060902_51 197 TRUE 59 0.3 4.12 2 4 Photos Birds CalibSchool PhotosAerial Biopsy CourseSchool TurtleSp SpPerc2279 N N &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; NA &lt;NA&gt; TurtleGs TurtleJFR TurtleAge TurtleCapt PinnipedSp PinnipedGs SpPerc2279 NA &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; NA BoatType BoatGs PerpDistKm species best low high prob mixed SpPerc2279 &lt;NA&gt; NA 6.540392 003 1 NaN NA FALSE TRUE ss_tot lnsstot ss_percent n_sp n_obs n_best n_low n_high calibr SpPerc2279 3.478261 1.246532 NaN 2 1 0 1 0 TRUE ss_valid mixed_max spp_max included SpPerc2279 FALSE FALSE &lt;NA&gt; TRUE The map indicates that this is not a geostratum boundary issue: cruzi &lt;- filter_cruz(cruz, spp=&#39;003&#39;, years = 2006) map_cruz(cruzi) Looking at the raw DAS data … das$das[405190:405220] [1] &quot; C 120000 090106 In transit to study area Sep 1. A.J. 10/18/06.&quot; [2] &quot;001B.061813 090206 N19:28.97 W156:55.51 1631 p 10 Y&quot; [3] &quot;002R.061813 090206 N19:28.97 W156:55.51 N&quot; [4] &quot;003P.061813 090206 N19:28.97 W156:55.51 197 007 277&quot; [5] &quot;004V.061813 090206 N19:28.97 W156:55.51 5 04 150 18.0&quot; [6] &quot;005N.061813 090206 N19:28.97 W156:55.51 154 10.4&quot; [7] &quot;006W.061813 090206 N19:28.97 W156:55.51 1 10 03 032 7.0&quot; [8] &quot;007C.061823 090206 N19:28.94 W156:55.50&quot; [9] &quot;008*.062019 090206 N19:28.63 W156:55.33&quot; [10] &quot;009*.063019 090206 N19:27.12 W156:54.53&quot; [11] &quot;010P.063950 090206 N19:25.68 W156:53.75 196 197 007&quot; [12] &quot;011C.063950 090206 N19:25.68 W156:53.75&quot; [13] &quot;012V.063950 090206 N19:25.68 W156:53.75 5 04 150 18.0&quot; [14] &quot;013N.063950 090206 N19:25.68 W156:53.75 151 10.1&quot; [15] &quot;014W.063950 090206 N19:25.68 W156:53.75 1 10 03 032 7.0&quot; [16] &quot;015*.064019 090206 N19:25.61 W156:53.71&quot; [17] &quot;016N.064253 090206 N19:25.22 W156:53.50 153 10.3&quot; [18] &quot;017*.065019 090206 N19:24.11 W156:52.90&quot; [19] &quot;018*.070019 090206 N19:22.62 W156:52.11&quot; [20] &quot;019*.071019 090206 N19:21.10 W156:51.30&quot; [21] &quot;020*.072019 090206 N19:19.59 W156:50.53&quot; [22] &quot;021P.072113 090206 N19:19.46 W156:50.45 073 196 197&quot; [23] &quot;022V.072113 090206 N19:19.46 W156:50.45 5 04 150 18.0&quot; [24] &quot;023N.072113 090206 N19:19.46 W156:50.45 155 10.0&quot; [25] &quot;024W.072113 090206 N19:19.46 W156:50.45 1 10 02 032 7.0&quot; [26] &quot;025*.073019 090206 N19:18.09 W156:49.75&quot; [27] &quot;026S.073514 090206 N19:17.33 W156:49.36 090 197 2 4 059 0.3 4.12 013&quot; [28] &quot;027A.073514 090206 N19:17.33 W156:49.36 090 N N 177 003&quot; [29] &quot; 1 197 3&quot; [30] &quot;028*.074019 090206 N19:16.53 W156:48.94&quot; [31] &quot;029V.074337 090206 N19:16.01 W156:48.67 4 04 150 12.0&quot; It appears that this was a multi-species sighting, but no species percentages were provided. In the absence of a percent composition estimate for this sighting, it was ignored by ABUND. Group sizes This plot compares the group size estimates returned by ABUND9 and LTabundR. The results should be identical: # Format ABUND abund &lt;- SIGHTS %&gt;% tidyr::pivot_longer(cols = 31:101, names_to = &#39;species&#39;, values_to = &#39;best&#39;) %&gt;% mutate(Region = gsub(&#39; &#39;,&#39;&#39;,Region)) %&gt;% filter(best &gt; 0, ! Region %in% c(&#39;NONE&#39;, &#39;Off-Transect&#39;), EffortSeg &gt; 0) %&gt;% select(Cruise = CruzNo, TotSS, LnTotSS, species, best) %&gt;% mutate(Software=&#39;ABUND9&#39;) # Format LTabundR ltabundr &lt;- cruz$cohorts$all$sightings %&gt;% filter(OnEffort == TRUE, included == TRUE, EffType %in% c(&#39;S&#39;, &#39;F&#39;)) %&gt;% select(Cruise, TotSS = ss_tot, LnTotSS = lnsstot, species, best) %&gt;% mutate(Software=&#39;LTabundR&#39;) # Combine the datasets ss &lt;- rbind(abund, ltabundr) # Plot the datasets ggplot(ss, aes(x=best, y=factor(Cruise), col=Software, pch=Software)) + geom_point(position=ggstance::position_dodgev(height=0.5), alpha=.6) + scale_x_continuous(trans=&#39;log&#39;, breaks=c(1,2, 5,10,25,50,100,500,1000,2500,5000)) + xlab(&#39;log Estimated School Size&#39;) + ylab(&#39;Cruise&#39;) + theme_light() Note two important differences in how the two programs calibrate group size: (1) If an observer is not included in the Group Size Calibration Coefficients .DAT file, ABUND applies a default coefficient (0.8625) to scale group size estimates; however, it applies this calibration to groups of *all** sizes, including solo animals or small groups of 2-3. In LTabundR, this is also the default, but users can choose to restrict calibrations for unknown observers to group size estimates of any size (see load_cohort_settings()). (2) Note that ABUND9 calibrates school sizes slightly differently than earlier versions of the software. The ABUND9 release notes mention a bug in previous versions that incorrectly calibrated school size. LTabundR corresponds perfectly with ABUND9 school size calibrations, but not with ABUND8 or earlier. Effort Perform basic formatting before doing any filtering , first for ABUND data… # Format ABUND abund &lt;- EFFORT %&gt;% mutate(Region = gsub(&#39; &#39;,&#39;&#39;,Region)) %&gt;% mutate(dt = paste0(Yr, stringr::str_pad(Mo, width=2, pad=&#39;0&#39;, side=&#39;left&#39;), stringr::str_pad(Da, width=2, pad=&#39;0&#39;, side=&#39;left&#39;))) %&gt;% rename(Cruise = CruzNo, Date = dt, km = length) %&gt;% mutate(effort = ifelse(Region == &#39;Off-Transect&#39;, &#39;N or F&#39;, &#39;S&#39;)) %&gt;% mutate(OnEffort = &#39;TRUE&#39;) %&gt;% mutate(software = &#39;ABUND 9&#39;) %&gt;% select(Region:Cruise, Date, effort, software) …then for LTabundR data. This helper function will be used below to format our cruz object: ltabundr_prep &lt;- function(cruz){ cruz$cohorts$all$das %&gt;% mutate(dt = gsub(&#39;-&#39;,&#39;&#39;,substr(DateTime, 1,10))) %&gt;% rename(Cruise = Cruise, Date = dt, km = km_int) %&gt;% mutate(effort = ifelse(EffType == &#39;S&#39;, &#39;S&#39;, &#39;N or F&#39;)) %&gt;% mutate(software = &#39;LTabundR&#39;) %&gt;% filter(OnEffort == TRUE) %&gt;% filter(!is.na(effort)==TRUE) %&gt;% group_by(software, Cruise, Date, effort) %&gt;% summarize(km = sum(km)) } Here is one more helper function to generate an interactive plot that compares systematic effort for each cruise in ABUND vs LTabundR: eff_plot &lt;- function(abund, ltabundr){ # Join datasets eff &lt;- rbind(abund %&gt;% dplyr::select(Cruise, Date, km, effort, software), ltabundr %&gt;% dplyr::select(Cruise, Date, km, effort, software)) %&gt;% filter(effort == &#39;S&#39;) %&gt;% group_by(Cruise) %&gt;% summarize(km_abund = sum(km[software == &#39;ABUND 9&#39;]) %&gt;% round, km_ltabundr = sum(km[software == &#39;LTabundR&#39;]) %&gt;% round) # Prepare plot p &lt;- ggplot(eff, aes(x = km_abund, y = km_ltabundr, col=factor(Cruise))) + geom_abline(slope=1, intercept=0, lty=3) + geom_point(alpha=.6) + ylab(&#39;LTabundR&#39;) + xlab(&#39;ABUND9&#39;) + scale_x_continuous(breaks = seq(0, 15000, by=2500)) + scale_y_continuous(breaks = seq(0, 15000, by=2500)) + labs(title=&#39;Effort per cruise&#39;, col=&#39;Cruise&#39;) + theme_light() print(p) } This helper function is a tabular version of the comparison plot: error_table &lt;- function(df){ dfi &lt;- df %&gt;% filter(effort == &#39;S&#39;) %&gt;% group_by(Cruise) %&gt;% summarize(year = lubridate::year(lubridate::ymd(Date))[1], km_abund = sum(km[software == &#39;ABUND 9&#39;]) %&gt;% round, km_ltabundr = sum(km[software == &#39;LTabundR&#39;]) %&gt;% round) %&gt;% mutate(diff = km_abund - km_ltabundr) %&gt;% mutate(prop = ((abs(diff) / km_abund)) %&gt;% round(5)) %&gt;% arrange(desc(prop)) %&gt;% as.data.frame() lm(km_ltabundr ~ km_abund, data=dfi) %&gt;% summary %&gt;% print return(dfi) } Now we format the LTabundR data and produce the comparison plot, which shows near-perfect correspondence: # Format LTabundR data for comparison ltabundr &lt;- ltabundr_prep(cruz) # Interactive plot eff_plot(abund, ltabundr) And here is the tabular version of that plot, sorted by the difference in KM, relative to total length of the Cruise (according to ABUND 9). The statistical summary of a linear regression is also returned: # Combine software data df &lt;- rbind(abund %&gt;% select(software, Cruise, Date, effort, km), ltabundr %&gt;% select(software, Cruise, Date, effort, km)) # Produce comparison table error_table(df) Call: lm(formula = km_ltabundr ~ km_abund, data = dfi) Residuals: Min 1Q Median 3Q Max -8.659 -6.692 -2.933 3.094 34.505 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 5.6871663 2.3871073 2.382 0.0229 * km_abund 1.0023296 0.0004711 2127.734 &lt;0.0000000000000002 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 9.337 on 34 degrees of freedom Multiple R-squared: 1, Adjusted R-squared: 1 F-statistic: 4.527e+06 on 1 and 34 DF, p-value: &lt; 0.00000000000000022 Cruise year km_abund km_ltabundr diff prop 1 1426 1991 11 28 -17 1.54545 2 1628 2005 2 1 1 0.50000 3 1508 1993 3 2 1 0.33333 4 1081 1987 50 60 -10 0.20000 5 1108 2011 2493 2539 -46 0.01845 6 1203 2012 1492 1507 -15 0.01005 7 1623 2003 4970 5011 -41 0.00825 8 1604 2016 1557 1568 -11 0.00706 9 1303 2013 3814 3838 -24 0.00629 10 2001 2020 4417 4442 -25 0.00566 11 901 2009 2004 2013 -9 0.00449 12 1631 2006 4040 4058 -18 0.00446 13 1370 1990 5425 5447 -22 0.00406 14 1616 2000 4960 4979 -19 0.00383 15 1165 1988 3433 3446 -13 0.00379 16 1706 2017 8965 8999 -34 0.00379 17 1004 2010 1123 1127 -4 0.00356 18 1705 2017 7345 7370 -25 0.00340 19 1080 1987 4367 4381 -14 0.00321 20 1629 2005 11072 11106 -34 0.00307 21 1610 1998 3163 3172 -9 0.00285 22 1642 2010 6347 6365 -18 0.00284 23 1614 1999 5883 5899 -16 0.00272 24 1164 1988 804 806 -2 0.00249 25 1607 1997 6669 6685 -16 0.00240 26 1622 2002 4599 4610 -11 0.00239 27 1641 2010 11721 11746 -25 0.00213 28 1621 2002 12436 12462 -26 0.00209 29 1611 1998 4055 4063 -8 0.00197 30 1268 1989 3750 3757 -7 0.00187 31 990 1986 3793 3800 -7 0.00185 32 989 1986 747 748 -1 0.00134 33 1369 1990 923 924 -1 0.00108 34 1001 2010 1134 1135 -1 0.00088 35 1617 2001 1 1 0 0.00000 36 1624 2003 763 763 0 0.00000 Note that there are a few intentional design features that may produce differences in effort calculation and segment divisions between the ABUND9 and LTabundR programs. Here are a few: (1) LTabundR works with DAS data that are loaded and formatted using swfscDAS:das_read() and das_process(). It is possible that these functions categorize events as On- or Off-Effort slightly differently than ABUND, or apply other differences that would be difficult for us to know or track. (2) After loading the data, LTabundR removes rows with invalid Cruise numbers, invalid times, and invalid coordinates. As far as we can tell, ABUND does not remove such missing data. This is a relatively minor point; in processing the 1986-2020 data (623,640 rows), 287 rows are missing Cruise info; 1,430 are missing valid times; and 556 are missing valid coordinates, for a total of 2,273 rows removed out of more than 625,000 (0.3% of rows). Many of these rows with missing data have the exact same coordinates and timestamps as complete rows nearby, since WinCruz can sometimes produce multiple lines at the same time when setting up metadata for the research day, which means that this row removal will rarely, if ever, effect total effort calculated. (3) In ABUND, custom functions are used to calculate whether DAS coordinates occur within geostrata are difficult to validate, and it is possible that they differ from the functions used in R for the same purpose. LTabundR uses functions within the well-established sf package to do these same calculations. (4) Both ABUND and LTabundR calculate the distance surveyed based on the sum of distances between adjacent rows in the DAS file. They do this differently, which may yield minor differences in total effort segment track lengths, but the default inputs for the load_survey_settings() function were selected to come as close to replicating the ABUND9 routine as possible. The ABUND9 routine (and therefore the LTabundR defaults) allow for large gaps (as much as 100 km) between subsequent rows within a single day of effort. The ABUND9 subroutine prints a warning message when the gap is greater than 30 km, but does not modify its estimate of distance traveled. This allows for the possibility that, in rare cases, estimates of distance surveyed will be spuriously large. (5) While ABUND uses a minimum length threshold to create segments, such that full-length segments are never less than that threshold and small remainder segments always occur at the end of a continuous period of effort, LTabundR uses an approach more similar to the effort-chopping functions in swfscDAS: it looks at continuous blocs of effort, determines how many full-length segments can be defined in each bloc, then randomly places the remainder within that bloc according to a set of user-defined settings (see load_survey_settings(). This process produces full-length segments whose distribution of exact lengths is centered about the target length, rather than always being greater than the target length. (6) To control the particularities of segmentizing, LTabundR uses settings such as segment_max_interval, which controls how discontinuous effort is allowed to be pooled into the same segment. These rules may produce slight differences in segment lengths. (7) Note that, since ABUND is a loop-based routine while LTabundR is modular, segments identified by the two program will never be exactly identical, and a 1:1 comparison of segments produced by the two programs is not possible. "],["spp_codes.html", "20 NOAA/NMFS species codes Table species_translator()", " 20 NOAA/NMFS species codes Table LTabundR provides the standard table of NOAA/NMFS species codes as a built-in dataset: This version of the species codes table was provided by Amanda Bradford (Pacific Islands Fisheries Science Center) in 2021. For an easier way to find a species of interest, look into the species_translator() function on the Miscellaneous functions page. species_translator() To streamline the management of species codes, scientific names, common names, etc., in the functions throughout this package, we have developed a “translator” function that returns the various identifiers for a species according to a variety of search terms. You can search by species code: # source(&#39;R/species_translator.R`) species_translator(id = &#39;035&#39;) %&gt;% t() 35 code &quot;035&quot; short_name &quot;LONG_PILOT&quot; scientific_name &quot;Globicephala melas&quot; common &quot;Long-finned pilot whale&quot; description &quot;Atlantic pilot whale; blackfish; pothead&quot; By the short code name: species_translator(id = &#39;LONG_PILOT&#39;) %&gt;% t() 35 code &quot;035&quot; short_name &quot;LONG_PILOT&quot; scientific_name &quot;Globicephala melas&quot; common &quot;Long-finned pilot whale&quot; description &quot;Atlantic pilot whale; blackfish; pothead&quot; By the scientific name: species_translator(id = &#39;Globicephala melas&#39;) %&gt;% t() 35 code &quot;035&quot; short_name &quot;LONG_PILOT&quot; scientific_name &quot;Globicephala melas&quot; common &quot;Long-finned pilot whale&quot; description &quot;Atlantic pilot whale; blackfish; pothead&quot; Or by one of the species’ common names: species_translator(id = &#39;Long-finned pilot whale&#39;) %&gt;% t() 35 code &quot;035&quot; short_name &quot;LONG_PILOT&quot; scientific_name &quot;Globicephala melas&quot; common &quot;Long-finned pilot whale&quot; description &quot;Atlantic pilot whale; blackfish; pothead&quot; The function will return any species for which there is a partial match: species_translator(id = &#39;megap&#39;) %&gt;% t() 76 code &quot;076&quot; short_name &quot;HUMPBACK_W&quot; scientific_name &quot;Megaptera novaeangliae&quot; common &quot;Humpback whale&quot; description &quot;&quot; 70 code &quot;070&quot; short_name &quot;UNID_RORQL&quot; scientific_name &quot;Balaenopterid sp&quot; common &quot;Unidentified rorqual (Balaenoptera; Megaptera)&quot; description &quot;&quot; species_translator(id = &#39;killer&#39;) code short_name scientific_name common 32 032 PYGMY_KLLR Feresa attenuata Pygmy killer whale 33 033 FALSE_KLLR Pseudorca crassidens False killer whale 37 037 KILLER_WHA Orcinus orca Killer whale 110 110 Orcinus orca Transient killer whale 111 111 Orcinus orca Resident killer whale 112 112 Orcinus orca Offshore killer whale 113 113 Orcinus orca Type A Antarctic killer whale 114 114 Orcinus orca Type B Antarctic killer whale 115 115 Orcinus orca Type C Antarctic killer whale description 32 slender blackfish 33 37 110 111 112 113 114 115 Note that if species_codes is NULL, as in the examples above, the list of codes used in ABUND9 will be used as a default. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
